{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-02T10:54:19.547551Z",
     "iopub.status.busy": "2025-12-02T10:54:19.547388Z",
     "iopub.status.idle": "2025-12-02T10:54:21.568512Z",
     "shell.execute_reply": "2025-12-02T10:54:21.567761Z",
     "shell.execute_reply.started": "2025-12-02T10:54:19.547535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T10:54:21.570355Z",
     "iopub.status.busy": "2025-12-02T10:54:21.569951Z",
     "iopub.status.idle": "2025-12-02T10:54:31.836363Z",
     "shell.execute_reply": "2025-12-02T10:54:31.835251Z",
     "shell.execute_reply.started": "2025-12-02T10:54:21.570336Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# install required libraries\n",
    "!pip install rdkit\n",
    "!pip install pingouin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T10:54:31.842689Z",
     "iopub.status.busy": "2025-12-02T10:54:31.842462Z",
     "iopub.status.idle": "2025-12-02T10:54:39.769396Z",
     "shell.execute_reply": "2025-12-02T10:54:39.768459Z",
     "shell.execute_reply.started": "2025-12-02T10:54:31.842668Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "from catboost import CatBoostRegressor\n",
    "from functools import lru_cache\n",
    "from joblib import Parallel, delayed\n",
    "from lightgbm import LGBMRegressor\n",
    "from rdkit import Chem, rdBase, RDLogger\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors, Crippen, Lipinski, Fragments, AllChem, rdFingerprintGenerator, RDKFingerprint\n",
    "from rdkit.Chem.AllChem import ComputeGasteigerCharges\n",
    "from rdkit.Chem.MACCSkeys import GenMACCSKeys\n",
    "from rdkit.Chem.EState import EState_VSA, AtomTypes as EAtomTypes\n",
    "from rdkit.Chem.rdchem import Mol\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from scipy.special import inv_boxcox\n",
    "from scipy.stats import boxcox, norm\n",
    "from sklearn.ensemble import ExtraTreesRegressor, HistGradientBoostingRegressor, RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from xgboost import XGBRegressor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T10:54:39.772394Z",
     "iopub.status.busy": "2025-12-02T10:54:39.771505Z",
     "iopub.status.idle": "2025-12-02T10:54:39.77815Z",
     "shell.execute_reply": "2025-12-02T10:54:39.777521Z",
     "shell.execute_reply.started": "2025-12-02T10:54:39.772368Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "rdBase.DisableLog('rdApp.*')\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T10:55:11.130339Z",
     "iopub.status.busy": "2025-12-02T10:55:11.129927Z",
     "iopub.status.idle": "2025-12-02T10:55:11.212754Z",
     "shell.execute_reply": "2025-12-02T10:55:11.211677Z",
     "shell.execute_reply.started": "2025-12-02T10:55:11.130315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train data\n",
    "df_train = pd.read_csv(\"/kaggle/input/melting-point/train.csv\", sep=\",\")[['SMILES', 'Tm']]\n",
    "print(df_train.shape)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T10:55:24.71193Z",
     "iopub.status.busy": "2025-12-02T10:55:24.711653Z",
     "iopub.status.idle": "2025-12-02T10:55:24.743762Z",
     "shell.execute_reply": "2025-12-02T10:55:24.742269Z",
     "shell.execute_reply.started": "2025-12-02T10:55:24.711911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test data\n",
    "df_test = pd.read_csv(\"/kaggle/input/melting-point/test.csv\", sep=\",\")[['id', 'SMILES']]\n",
    "print(df_test.shape)\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T10:55:36.613347Z",
     "iopub.status.busy": "2025-12-02T10:55:36.612142Z",
     "iopub.status.idle": "2025-12-02T10:55:36.633923Z",
     "shell.execute_reply": "2025-12-02T10:55:36.633049Z",
     "shell.execute_reply.started": "2025-12-02T10:55:36.613289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# sample submission\n",
    "sample_submission = pd.read_csv(\"/kaggle/input/melting-point/sample_submission.csv\", sep=\",\")\n",
    "print(sample_submission.shape)\n",
    "sample_submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading external Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T10:55:56.715817Z",
     "iopub.status.busy": "2025-12-02T10:55:56.714827Z",
     "iopub.status.idle": "2025-12-02T10:55:56.720889Z",
     "shell.execute_reply": "2025-12-02T10:55:56.719878Z",
     "shell.execute_reply.started": "2025-12-02T10:55:56.715778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def regression_metrics(y_val, pred_val):\n",
    "    \"\"\" show metrics for selected two continuous columns to be\n",
    "        compared with each other \n",
    "    \"\"\"   \n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return {\n",
    "        'R2': round(r2_score(y_val, pred_val), 3),\n",
    "        'MAE': round(mean_absolute_error(y_val, pred_val), 3),\n",
    "        'MAPE': round(mean_absolute_percentage_error(y_val, pred_val), 3),\n",
    "        'RMSE': round(root_mean_squared_error(y_val, pred_val), 3),\n",
    "        'sample_size': len(y_val),    \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T10:56:10.576884Z",
     "iopub.status.busy": "2025-12-02T10:56:10.576345Z",
     "iopub.status.idle": "2025-12-02T10:56:11.810672Z",
     "shell.execute_reply": "2025-12-02T10:56:11.809686Z",
     "shell.execute_reply.started": "2025-12-02T10:56:10.576865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load external data: smiles_melting_point\n",
    "df_smiles = pd.read_csv(\"/kaggle/input/external-melting-point-datasets/smiles_melting_point.csv\", on_bad_lines='skip' )\n",
    "df_smiles = df_smiles[[\"SMILES\", \"Melting Point {measured, converted}\"]]\n",
    "df_smiles = df_smiles.rename(columns={'Melting Point {measured, converted}': 'Tm', 'NAME': 'name'})\n",
    "df_smiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T10:56:17.251217Z",
     "iopub.status.busy": "2025-12-02T10:56:17.250948Z",
     "iopub.status.idle": "2025-12-02T10:56:17.344778Z",
     "shell.execute_reply": "2025-12-02T10:56:17.34389Z",
     "shell.execute_reply.started": "2025-12-02T10:56:17.251201Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# performance on training dataset\n",
    "df_inner = df_train[[\"SMILES\", \"Tm\"]].merge(df_smiles, how=\"inner\", on=\"SMILES\")\n",
    "df_inner.info()\n",
    "regression_metrics(df_inner.Tm_x, df_inner.Tm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T10:56:35.466841Z",
     "iopub.status.busy": "2025-12-02T10:56:35.466578Z",
     "iopub.status.idle": "2025-12-02T10:56:39.081158Z",
     "shell.execute_reply": "2025-12-02T10:56:39.080087Z",
     "shell.execute_reply.started": "2025-12-02T10:56:35.466828Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load external data: bradley\n",
    "df_bradley = pd.read_excel('/kaggle/input/external-melting-point-datasets/BradleyMeltingPointDataset.xlsx')\n",
    "df_bradleyplus = pd.read_excel('/kaggle/input/external-melting-point-datasets/BradleyDoublePlusGoodMeltingPointDataset.xlsx')\n",
    "df_bradley['Tm'] = df_bradley['mpC'].map(lambda x : x + 273.15)\n",
    "df_bradleyplus['Tm'] = df_bradleyplus['mpC'].map(lambda x: x + 273.15)\n",
    "df_bradley = df_bradley[['smiles', 'Tm']]\n",
    "df_bradleyplus = df_bradleyplus[['smiles', 'Tm']]\n",
    "df_bradley = pd.concat((df_bradley, df_bradleyplus), axis=0)\n",
    "df_bradley = df_bradley.rename(columns = {'smiles':'SMILES'})\n",
    "df_bradley.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T10:56:49.130599Z",
     "iopub.status.busy": "2025-12-02T10:56:49.129797Z",
     "iopub.status.idle": "2025-12-02T10:56:49.159971Z",
     "shell.execute_reply": "2025-12-02T10:56:49.158298Z",
     "shell.execute_reply.started": "2025-12-02T10:56:49.13057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# performance on training dataset\n",
    "df_inner = df_train[[\"SMILES\", \"Tm\"]].merge(df_bradley, how=\"inner\", on=\"SMILES\")\n",
    "df_inner.info()\n",
    "regression_metrics(df_inner.Tm_x, df_inner.Tm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T10:56:59.843801Z",
     "iopub.status.busy": "2025-12-02T10:56:59.843556Z",
     "iopub.status.idle": "2025-12-02T10:56:59.848332Z",
     "shell.execute_reply": "2025-12-02T10:56:59.847588Z",
     "shell.execute_reply.started": "2025-12-02T10:56:59.843788Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# have a unique chemical signature\n",
    "def canonicalize(smile):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        return Chem.MolToSmiles(mol, canonical=True)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T10:57:08.378735Z",
     "iopub.status.busy": "2025-12-02T10:57:08.377741Z",
     "iopub.status.idle": "2025-12-02T10:58:17.492544Z",
     "shell.execute_reply": "2025-12-02T10:58:17.491667Z",
     "shell.execute_reply.started": "2025-12-02T10:57:08.378716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# canonicalize all smiles: unique descriptor\n",
    "df_train['SMILES'] = df_train['SMILES'].apply(canonicalize)\n",
    "df_test['SMILES'] = df_test['SMILES'].apply(canonicalize)\n",
    "df_smiles['SMILES'] = df_smiles['SMILES'].apply(canonicalize)\n",
    "df_bradley['SMILES'] = df_bradley['SMILES'].apply(canonicalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T10:58:17.494054Z",
     "iopub.status.busy": "2025-12-02T10:58:17.493829Z",
     "iopub.status.idle": "2025-12-02T10:58:17.651279Z",
     "shell.execute_reply": "2025-12-02T10:58:17.650357Z",
     "shell.execute_reply.started": "2025-12-02T10:58:17.494035Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# add all together\n",
    "df_train = pd.concat([df_smiles, df_bradley, df_train], axis=0)\n",
    "df_train.drop_duplicates(subset='SMILES', keep='last', inplace=True)\n",
    "df_train.dropna(ignore_index=True, inplace=True)\n",
    "df_train.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T10:58:17.652149Z",
     "iopub.status.busy": "2025-12-02T10:58:17.651968Z",
     "iopub.status.idle": "2025-12-02T10:58:17.728525Z",
     "shell.execute_reply": "2025-12-02T10:58:17.727309Z",
     "shell.execute_reply.started": "2025-12-02T10:58:17.652131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# any overlapping test <-> train (after external data added)\n",
    "df_test = df_test.merge(df_train, how=\"left\", on=\"SMILES\")\n",
    "df_test[[\"SMILES\", \"Tm\"]].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T10:58:17.730172Z",
     "iopub.status.busy": "2025-12-02T10:58:17.729973Z",
     "iopub.status.idle": "2025-12-02T10:58:17.745706Z",
     "shell.execute_reply": "2025-12-02T10:58:17.744331Z",
     "shell.execute_reply.started": "2025-12-02T10:58:17.730156Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train.sample(20000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T10:59:38.302102Z",
     "iopub.status.busy": "2025-12-02T10:59:38.301838Z",
     "iopub.status.idle": "2025-12-02T10:59:38.354274Z",
     "shell.execute_reply": "2025-12-02T10:59:38.353171Z",
     "shell.execute_reply.started": "2025-12-02T10:59:38.302087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _safe(f, default=None):\n",
    "    def wrap(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except Exception:\n",
    "            return default\n",
    "    return wrap\n",
    "\n",
    "def _count_atoms(m, symbols):\n",
    "    s = set(symbols)\n",
    "    return sum(1 for a in m.GetAtoms() if a.GetSymbol() in s)\n",
    "\n",
    "def _largest_ring_size(m):\n",
    "    ri = m.GetRingInfo()\n",
    "    return max((len(r) for r in ri.AtomRings()), default=0)\n",
    "\n",
    "def count_smarts(m, patt):\n",
    "    return len(m.GetSubstructMatches(patt))\n",
    "\n",
    "def count_explicit_h(m):\n",
    "    mH = Chem.AddHs(m)\n",
    "    return sum(1 for a in mH.GetAtoms() if a.GetSymbol() == \"H\")\n",
    "\n",
    "def drop_constant_and_duplicate_columns(df):\n",
    "    # drop constant columns\n",
    "    nunique = df.nunique(dropna=False)\n",
    "    constant_cols = nunique[nunique <= 1].index.tolist()\n",
    "    df = df.drop(columns=constant_cols)\n",
    "    # drop duplicate columns\n",
    "    df = df.loc[:, ~df.T.duplicated(keep='first')]\n",
    "    return df\n",
    "\n",
    "def _pick(*cands):\n",
    "    \"\"\"Return first callable among names across modules.\"\"\"\n",
    "    for mod, name in cands:\n",
    "        fn = getattr(mod, name, None)\n",
    "        if callable(fn):\n",
    "            return fn\n",
    "    return None\n",
    "\n",
    "def gasteiger_stats(m):\n",
    "    m = Chem.AddHs(m)\n",
    "    ComputeGasteigerCharges(m)\n",
    "    vals = []\n",
    "    for a in m.GetAtoms():\n",
    "        v = a.GetDoubleProp('_GasteigerCharge') if a.HasProp('_GasteigerCharge') else 0.0\n",
    "        if pd.isna(v) or v == float('inf') or v == float('-inf'):\n",
    "            v = 0.0\n",
    "        vals.append(v)\n",
    "    arr = np.array(vals, dtype=float)\n",
    "    return {\n",
    "        \"Gasteiger_q_sum\": float(arr.sum()),\n",
    "        \"Gasteiger_q_abs_sum\": float(np.abs(arr).sum()),\n",
    "        \"Gasteiger_q_min\": float(arr.min(initial=0.0)),\n",
    "        \"Gasteiger_q_max\": float(arr.max(initial=0.0)),\n",
    "        \"Gasteiger_q_std\": float(arr.std(ddof=0)),\n",
    "    }\n",
    "\n",
    "MORGAN_BITS   = 512\n",
    "MORGAN_RADIUS = 2\n",
    "USE_MACCS     = True\n",
    "\n",
    "def _shape3d_worker(cansmi: str, maxIters: int = 0):\n",
    "    \"\"\"Compute fast 3D shape features for ONE canonical SMILES (embed with explicit Hs, compute shape without Hs).\"\"\"\n",
    "    try:\n",
    "        m = Chem.MolFromSmiles(cansmi)\n",
    "        if m is None:\n",
    "            return cansmi, {}\n",
    "\n",
    "        # --- Add explicit Hs for embedding/optimization (silences the “explicit Hs” spam) ---\n",
    "        mH = Chem.AddHs(m)\n",
    "\n",
    "        params = AllChem.ETKDGv3()\n",
    "        params.randomSeed = 123\n",
    "        params.useRandomCoords = True\n",
    "\n",
    "        with Chem.WrapLogs():\n",
    "            cid = AllChem.EmbedMolecule(mH, params)\n",
    "        if cid < 0:\n",
    "            with Chem.WrapLogs():\n",
    "                cid = AllChem.EmbedMolecule(mH, randomSeed=123)\n",
    "            if cid < 0:\n",
    "                return cansmi, {}\n",
    "\n",
    "        if maxIters and maxIters > 0:\n",
    "            try:\n",
    "                with Chem.WrapLogs():\n",
    "                    AllChem.UFFOptimizeMolecule(mH, confId=cid, maxIters=int(maxIters))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # --- Remove Hs before computing shape (keeps heavy-atom shape behavior) ---\n",
    "        # After RemoveHs, conformers are reindexed; use confId = 0.\n",
    "        m_noH = Chem.RemoveHs(mH)\n",
    "        confId = 0\n",
    "\n",
    "        out = {}\n",
    "        for nm, fn in [\n",
    "            (\"RadiusOfGyration\", rdMolDescriptors.CalcRadiusOfGyration),\n",
    "            (\"InertialShapeFactor\", rdMolDescriptors.CalcInertialShapeFactor),\n",
    "            (\"PMI1\", rdMolDescriptors.CalcPMI1),\n",
    "            (\"PMI2\", rdMolDescriptors.CalcPMI2),\n",
    "            (\"PMI3\", rdMolDescriptors.CalcPMI3),\n",
    "            (\"NPR1\", rdMolDescriptors.CalcNPR1),\n",
    "            (\"NPR2\", rdMolDescriptors.CalcNPR2),\n",
    "        ]:\n",
    "            try:\n",
    "                with Chem.WrapLogs():\n",
    "                    out[nm] = float(fn(m_noH, confId=confId))\n",
    "            except Exception:\n",
    "                out[nm] = 0.0\n",
    "\n",
    "        pmi1 = out.get(\"PMI1\", 0.0) or 0.0\n",
    "        pmi2 = out.get(\"PMI2\", 0.0) or 0.0\n",
    "        pmi3 = out.get(\"PMI3\", 0.0) or 0.0\n",
    "        out[\"PMI2_over_PMI1\"] = (pmi2 / pmi1) if pmi1 else 0.0\n",
    "        out[\"PMI3_over_PMI1\"] = (pmi3 / pmi1) if pmi1 else 0.0\n",
    "\n",
    "        return cansmi, out\n",
    "    except Exception:\n",
    "        return cansmi, {}\n",
    "\n",
    "def precompute_shape3d_cache(smiles_series, n_jobs=None, maxIters=0):\n",
    "    n_jobs = n_jobs or max(1, mp.cpu_count() - 1)\n",
    "    # canonicalize & unique\n",
    "    can = smiles_series.astype(str).apply(lambda s: Chem.MolToSmiles(Chem.MolFromSmiles(s), canonical=True)\n",
    "                                          if pd.notna(s) and Chem.MolFromSmiles(s) is not None else None)\n",
    "    uniq = sorted(x for x in set(can.tolist()) if x is not None)\n",
    "\n",
    "    if not uniq:\n",
    "        return {}\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs, backend=\"loky\", batch_size=64)(\n",
    "        delayed(_shape3d_worker)(s, maxIters) for s in uniq\n",
    "    )\n",
    "    return {k: v for k, v in results if k is not None}\n",
    "\n",
    "def _bond_order(b):\n",
    "    # return 1,2,3 or 1.5 for aromatic\n",
    "    if b.GetIsAromatic():\n",
    "        return 1.5\n",
    "    t = b.GetBondType()\n",
    "    if t == Chem.BondType.SINGLE: return 1\n",
    "    if t == Chem.BondType.DOUBLE: return 2\n",
    "    if t == Chem.BondType.TRIPLE: return 3\n",
    "    return 0\n",
    "\n",
    "def _ring_size_hist(m):\n",
    "    ri = m.GetRingInfo()\n",
    "    sizes = [len(r) for r in ri.AtomRings()]\n",
    "    out = {5:0, 6:0, 7:0, 8:0}\n",
    "    for s in sizes:\n",
    "        if s in out: out[s] += 1\n",
    "    return out, len(sizes)\n",
    "\n",
    "def _ring_systems_count(m):\n",
    "    # crude fused ring systems count via BFS over rings sharing atoms\n",
    "    ri = m.GetRingInfo()\n",
    "    rings = [set(r) for r in ri.AtomRings()]\n",
    "    if not rings: return 0\n",
    "    seen = set()\n",
    "    sys = 0\n",
    "    for i in range(len(rings)):\n",
    "        if i in seen: continue\n",
    "        sys += 1\n",
    "        stack = [i]\n",
    "        seen.add(i)\n",
    "        while stack:\n",
    "            j = stack.pop()\n",
    "            for k in range(len(rings)):\n",
    "                if k in seen: continue\n",
    "                if rings[j] & rings[k]:\n",
    "                    seen.add(k); stack.append(k)\n",
    "    return sys\n",
    "\n",
    "def _murcko_stats(m):\n",
    "    try:\n",
    "        scaf = MurckoScaffold.GetScaffoldForMol(m)\n",
    "        if scaf is None or scaf.GetNumAtoms() == 0:\n",
    "            return {\"MurckoAtoms\":0, \"MurckoRings\":0, \"MurckoRingSystems\":0, \"SideChainAtoms\":m.GetNumAtoms()}\n",
    "        msys = _ring_systems_count(scaf)\n",
    "        return {\n",
    "            \"MurckoAtoms\": scaf.GetNumAtoms(),\n",
    "            \"MurckoRings\": rdMolDescriptors.CalcNumRings(scaf),\n",
    "            \"MurckoRingSystems\": msys,\n",
    "            \"SideChainAtoms\": m.GetNumAtoms() - scaf.GetNumAtoms(),\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"MurckoAtoms\":0, \"MurckoRings\":0, \"MurckoRingSystems\":0, \"SideChainAtoms\":m.GetNumAtoms()}\n",
    "\n",
    "def _estate_stats(m):\n",
    "    try:\n",
    "        vals = EAtomTypes.EStateIndices(m)  # per-atom EState\n",
    "        if not vals: return {\"EState_sum\":0.0,\"EState_mean\":0.0,\"EState_max\":0.0,\"EState_min\":0.0,\"EState_std\":0.0}\n",
    "        import numpy as np\n",
    "        arr = np.asarray(vals, dtype=float)\n",
    "        return {\n",
    "            \"EState_sum\": float(arr.sum()),\n",
    "            \"EState_mean\": float(arr.mean()),\n",
    "            \"EState_max\": float(arr.max()),\n",
    "            \"EState_min\": float(arr.min()),\n",
    "            \"EState_std\": float(arr.std(ddof=0)),\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"EState_sum\":0.0,\"EState_mean\":0.0,\"EState_max\":0.0,\"EState_min\":0.0,\"EState_std\":0.0}\n",
    "\n",
    "def _smiles_morphology(smi: str):\n",
    "    if not smi: \n",
    "        return {\"SMI_len\":0,\"SMI_branches\":0,\"SMI_ringDigits\":0,\"SMI_stereoAt\":0,\"SMI_ezSlashes\":0}\n",
    "    return {\n",
    "        \"SMI_len\": len(smi),\n",
    "        \"SMI_branches\": smi.count(\"(\"),\n",
    "        \"SMI_ringDigits\": sum(ch.isdigit() for ch in smi),\n",
    "        \"SMI_stereoAt\": smi.count(\"@\"),\n",
    "        \"SMI_ezSlashes\": smi.count(\"/\") + smi.count(\"\\\\\"),\n",
    "    }\n",
    "\n",
    "def augment_extra_cheaps(row, m):\n",
    "    # ---- EState summaries ----\n",
    "    row.update(_estate_stats(m))\n",
    "\n",
    "    # ---- Bond order / saturation / conjugation ----\n",
    "    bonds = list(m.GetBonds())\n",
    "    nb = max(len(bonds), 1)\n",
    "    n_single = sum(1 for b in bonds if b.GetBondType() == Chem.BondType.SINGLE and not b.GetIsAromatic())\n",
    "    n_double = sum(1 for b in bonds if b.GetBondType() == Chem.BondType.DOUBLE)\n",
    "    n_triple = sum(1 for b in bonds if b.GetBondType() == Chem.BondType.TRIPLE)\n",
    "    n_arom   = sum(1 for b in bonds if b.GetIsAromatic())\n",
    "    row[\"FracSingle\"]  = n_single / nb\n",
    "    row[\"FracDouble\"]  = n_double / nb\n",
    "    row[\"FracTriple\"]  = n_triple / nb\n",
    "    row[\"FracAromatic\"]= n_arom   / nb\n",
    "    row[\"MeanBondOrder\"]= (sum(_bond_order(b) for b in bonds) / nb) if nb>0 else 0.0\n",
    "    # simple conjugation proxy: count alternating unsat/aromatic bonds\n",
    "    row[\"UnsatBondCount\"] = n_double + n_triple + n_arom\n",
    "\n",
    "    # ---- Ring size distribution + fused systems ----\n",
    "    hist, n_rings = _ring_size_hist(m)\n",
    "    row[\"Rings5\"] = hist[5]; row[\"Rings6\"] = hist[6]\n",
    "    row[\"Rings7\"] = hist[7]; row[\"Rings8\"] = hist[8]\n",
    "    row[\"RingSystems\"] = _ring_systems_count(m)\n",
    "    row[\"Rings56_frac\"] = (hist[5] + hist[6]) / (n_rings if n_rings>0 else 1)\n",
    "\n",
    "    # ---- Murcko scaffold stats ----\n",
    "    row.update(_murcko_stats(m))\n",
    "\n",
    "    # ---- Charge / zwitterion flags ----\n",
    "    tot_charge = sum(a.GetFormalCharge() for a in m.GetAtoms())\n",
    "    has_pos = any(a.GetFormalCharge() > 0 for a in m.GetAtoms())\n",
    "    has_neg = any(a.GetFormalCharge() < 0 for a in m.GetAtoms())\n",
    "    row[\"FormalCharge\"] = int(tot_charge)\n",
    "    row[\"IsZwitterion\"] = int(has_pos and has_neg)\n",
    "\n",
    "    # ---- SMILES morphology (cheap string features) ----\n",
    "    try:\n",
    "        smi = Chem.MolToSmiles(m, canonical=True)\n",
    "    except Exception:\n",
    "        smi = \"\"\n",
    "    row.update(_smiles_morphology(smi))\n",
    "\n",
    "    return row\n",
    "\n",
    "def reduce_memory_usage(df):\n",
    "    \"\"\" reduce memory used by the dataframe by converting into \n",
    "        more memory friendly data types\n",
    "    \"\"\"\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        # numerical\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        # categorical\n",
    "        elif col_type == object:\n",
    "            num_unique = df[col].nunique()\n",
    "            num_total = len(df[col])\n",
    "            if num_unique / num_total < 0.5:  # Heuristic: low cardinality\n",
    "                df[col] = df[col].astype('category')\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print('Memory usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def rdkit_feature_row(m, compute_3d=False, shape_cache=None):\n",
    "    row = {}\n",
    "\n",
    "    # ---- 1) Built-in descriptor list (2D)\n",
    "    for name, func in Descriptors._descList:\n",
    "        row[name] = _safe(func)(m)\n",
    "\n",
    "    # Common extras that aren’t always in _descList\n",
    "    row[\"MolLogP\"], row[\"MolMR\"] = _safe(Crippen.MolLogP)(m), _safe(Crippen.MolMR)(m)\n",
    "    row[\"NumHAcceptors\"], row[\"NumHDonors\"] = _safe(Lipinski.NumHAcceptors)(m), _safe(Lipinski.NumHDonors)(m)\n",
    "    row[\"NumRings\"] = _safe(rdMolDescriptors.CalcNumRings)(m)\n",
    "    row[\"NumAromaticRings\"] = _safe(rdMolDescriptors.CalcNumAromaticRings)(m)\n",
    "    row[\"NumAliphaticRings\"] = _safe(rdMolDescriptors.CalcNumAliphaticRings)(m)\n",
    "    row[\"NumSaturatedRings\"] = _safe(rdMolDescriptors.CalcNumSaturatedRings)(m)\n",
    "    row[\"NumBridgeheadAtoms\"] = _safe(rdMolDescriptors.CalcNumBridgeheadAtoms)(m)\n",
    "    row[\"NumSpiroAtoms\"]      = _safe(rdMolDescriptors.CalcNumSpiroAtoms)(m)\n",
    "    row[\"LargestRingSize\"] = _safe(_largest_ring_size)(m)\n",
    "    row[\"NumAmideBonds\"] = _safe(rdMolDescriptors.CalcNumAmideBonds)(m)\n",
    "    row[\"TPSA\"] = _safe(rdMolDescriptors.CalcTPSA)(m)\n",
    "    row[\"LabuteASA\"] = _safe(rdMolDescriptors.CalcLabuteASA)(m)\n",
    "\n",
    "    # Element counts\n",
    "    if m is None:\n",
    "        for el in [\"C\",\"H\",\"N\",\"O\",\"S\",\"F\",\"Cl\",\"Br\",\"I\",\"P\"]:\n",
    "            row[f\"Count_{el}\"] = 0\n",
    "    else:\n",
    "        for el in [\"C\",\"N\",\"O\",\"S\",\"F\",\"Cl\",\"Br\",\"I\",\"P\"]:\n",
    "            row[f\"Count_{el}\"] = _count_atoms(m, [el])\n",
    "        row[\"Count_H\"] = _safe(count_explicit_h)(m)\n",
    "\n",
    "    # ---- 2a) Fragment/group counts (built-in RDKit fragments only)\n",
    "    for attr in dir(Fragments):\n",
    "        if attr.startswith(\"fr_\"):\n",
    "            fn = getattr(Fragments, attr)\n",
    "            if callable(fn):\n",
    "                row[attr] = _safe(fn)(m)\n",
    "\n",
    "    # ---- 2b) Fingerprints: Morgan (binary) + MACCS (fast, high value)\n",
    "    try:\n",
    "        if m is not None:\n",
    "            # Morgan binary (no countSimulation → faster & leaner)\n",
    "            mgen = rdFingerprintGenerator.GetMorganGenerator(\n",
    "                radius=MORGAN_RADIUS, fpSize=MORGAN_BITS, countSimulation=False\n",
    "            )\n",
    "            mfp = mgen.GetFingerprint(m)\n",
    "            for i in range(MORGAN_BITS):\n",
    "                row[f\"Morgan_{i}\"] = int(mfp[i])\n",
    "        else:\n",
    "            for i in range(MORGAN_BITS):\n",
    "                row[f\"Morgan_{i}\"] = 0\n",
    "\n",
    "        if USE_MACCS:\n",
    "            if m is not None:\n",
    "                maccs = GenMACCSKeys(m)                     # 167 bits\n",
    "                for i in range(len(maccs)):\n",
    "                    row[f\"MACCS_{i}\"] = int(maccs[i])\n",
    "            else:\n",
    "                for i in range(167):\n",
    "                    row[f\"MACCS_{i}\"] = 0\n",
    "    except Exception:\n",
    "        # safety net: if FP gen fails for any reason, fill zeros and keep going\n",
    "        for i in range(MORGAN_BITS):\n",
    "            row.setdefault(f\"Morgan_{i}\", 0)\n",
    "        if USE_MACCS:\n",
    "            for i in range(167):\n",
    "                row.setdefault(f\"MACCS_{i}\", 0)\n",
    "\n",
    "    # ---- 3) VSA binnings / EState VSA (surface/charge environments)\n",
    "    for vsa_name, vsa_fn in [\n",
    "        (\"SlogP_VSA\", getattr(rdMolDescriptors, \"SlogP_VSA_\", None)),\n",
    "        (\"SMR_VSA\",   getattr(rdMolDescriptors, \"SMR_VSA_\", None)),\n",
    "        (\"EState_VSA\",getattr(rdMolDescriptors, \"EState_VSA_\", None)),\n",
    "    ]:\n",
    "        if vsa_fn is None:\n",
    "            continue\n",
    "        try:\n",
    "            bins = vsa_fn(m)\n",
    "            for i, val in enumerate(bins):\n",
    "                row[f\"{vsa_name}{i}\"] = val\n",
    "            row[f\"{vsa_name}_sum\"] = float(sum(bins))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ---- 4) Chi / Kappa / other indices (version-safe)\n",
    "    chi_kappa_specs = [\n",
    "        (\"Chi0\",   _pick((rdMolDescriptors,\"CalcChi0n\"))),\n",
    "        (\"Chi1\",   _pick((rdMolDescriptors,\"CalcChi1n\"))),\n",
    "        (\"Chi2\",   _pick((rdMolDescriptors,\"CalcChi2n\"))),\n",
    "        (\"Chi3\",   _pick((rdMolDescriptors,\"CalcChi3n\"))),\n",
    "        (\"Chi4\",   _pick((rdMolDescriptors,\"CalcChi4n\"))),\n",
    "        (\"Chi0v\",  _pick((rdMolDescriptors,\"CalcChi0v\"))),\n",
    "        (\"Chi1v\",  _pick((rdMolDescriptors,\"CalcChi1v\"))),\n",
    "        (\"Chi2v\",  _pick((rdMolDescriptors,\"CalcChi2v\"))),\n",
    "        (\"Chi3v\",  _pick((rdMolDescriptors,\"CalcChi3v\"))),\n",
    "        (\"Chi4v\",  _pick((rdMolDescriptors,\"CalcChi4v\"))),\n",
    "        (\"Kappa1\", _pick((rdMolDescriptors,\"CalcKappa1\"), (Descriptors,\"Kappa1\"))),\n",
    "        (\"Kappa2\", _pick((rdMolDescriptors,\"CalcKappa2\"), (Descriptors,\"Kappa2\"))),\n",
    "        (\"Kappa3\", _pick((rdMolDescriptors,\"CalcKappa3\"), (Descriptors,\"Kappa3\"))),\n",
    "        (\"BertzCT\",_pick((rdMolDescriptors,\"CalcBertzCT\"), (Descriptors,\"BertzCT\"))),\n",
    "        (\"HallKierAlpha\", _pick((rdMolDescriptors,\"CalcHallKierAlpha\"), (Descriptors,\"HallKierAlpha\"))),\n",
    "        (\"FractionCSP3\",  _pick((rdMolDescriptors,\"CalcFractionCSP3\"), (Descriptors,\"FractionCSP3\"))),\n",
    "    ]\n",
    "    for nm, fn in chi_kappa_specs:\n",
    "        if fn is not None:\n",
    "            row[nm] = _safe(fn)(m)\n",
    "\n",
    "    # ---- 4.5) Gasteiger charge features (polarity proxy)\n",
    "    row.update(_safe(gasteiger_stats, {})(m))\n",
    "\n",
    "     # ---- 5) 3D shape\n",
    "    if compute_3d and m is not None:\n",
    "        try:\n",
    "            cansmi = Chem.MolToSmiles(m, canonical=True)\n",
    "        except Exception:\n",
    "            cansmi = None\n",
    "        if shape_cache is not None and cansmi in shape_cache:\n",
    "            row.update(shape_cache[cansmi])\n",
    "        else:\n",
    "            # fallback: compute quickly (single conformer, no-H, no/min opt)\n",
    "            row = add_3d_features(row, m)  # your lean function\n",
    "\n",
    "    # ---- 6a) Handy interaction/ratio features (often correlate with MP)\n",
    "    hbd = row.get(\"NumHDonors\", 0) or 0\n",
    "    hba = row.get(\"NumHAcceptors\", 0) or 0\n",
    "    mw  = row.get(\"MolWt\", 0) or 1.0\n",
    "    hat = row.get(\"HeavyAtomCount\", 0) or 1.0\n",
    "    row[\"HBondCapacity\"] = hbd + hba\n",
    "    row[\"HBondDensity_perHeavyAtom\"] = (hbd + hba) / hat\n",
    "    row[\"RingDensity_perHeavyAtom\"] = (row.get(\"NumRings\",0) or 0) / hat\n",
    "    row[\"HalogenCount\"] = sum(row.get(k,0) for k in [\"Count_F\",\"Count_Cl\",\"Count_Br\",\"Count_I\"])\n",
    "    row[\"HeteroAtomFrac\"] = (row.get(\"Count_N\",0)+row.get(\"Count_O\",0)+row.get(\"Count_S\",0)+row.get(\"Count_P\",0)) / hat\n",
    "    row[\"AromRingFrac\"] = (row.get(\"NumAromaticRings\",0) or 0) / (row.get(\"NumRings\",1) or 1)\n",
    "\n",
    "    # ---- 6b) Extra cheap MP-focused interactions (very low cost, good value)\n",
    "    hbd = row.get(\"NumHDonors\", 0.0) or 0.0\n",
    "    hba = row.get(\"NumHAcceptors\", 0.0) or 0.0\n",
    "    mw  = row.get(\"MolWt\", 0.0) or 0.0\n",
    "    tpsa = row.get(\"TPSA\", 0.0) or 0.0\n",
    "    nrot = row.get(\"NumRotatableBonds\", 0.0) or 0.0\n",
    "    narm = row.get(\"NumAromaticRings\", 0.0) or 0.0\n",
    "    mollogp = row.get(\"MolLogP\", 0.0) or 0.0\n",
    "    bertz = row.get(\"BertzCT\", 0.0) or 0.0\n",
    "\n",
    "    row[\"HBond_Product\"]          = hbd * hba\n",
    "    row[\"LogP_div_TPSA\"]          = mollogp / (tpsa + 1.0)\n",
    "    row[\"LogP_x_TPSA\"]            = mollogp * tpsa\n",
    "    row[\"Flexibility_Score\"]      = nrot / (mw + 1.0)\n",
    "    row[\"MolWt_x_AromaticRings\"]  = mw * narm\n",
    "    row[\"Complexity_per_MW\"]      = bertz / (mw + 1.0)\n",
    "    row[\"Rigidity_Score\"]         = narm / (nrot + 1.0)\n",
    "    row = augment_extra_cheaps(row, m)\n",
    "    \n",
    "    return row\n",
    "\n",
    "def featurize_smiles(df, compute_3d=True, maxIters_3d=0):\n",
    "    # molecular chemical\n",
    "    mols = df[\"SMILES\"].apply(lambda s: Chem.MolFromSmiles(s) if pd.notna(s) else None)\n",
    "\n",
    "    # optional: precompute lean 3D cache in parallel (unique SMILES)\n",
    "    shape_cache = None\n",
    "    if compute_3d:\n",
    "        shape_cache = precompute_shape3d_cache(df[\"SMILES\"], n_jobs=-1, maxIters=maxIters_3d)\n",
    "\n",
    "    # row-wise features (sequential is fine; 3D is now cached)\n",
    "    feats = []\n",
    "    for m in mols:\n",
    "        feats.append(rdkit_feature_row(m, compute_3d=compute_3d, shape_cache=shape_cache))\n",
    "    feats = pd.DataFrame(feats)\n",
    "\n",
    "    # drop meaningless ones\n",
    "    feats = drop_constant_and_duplicate_columns(feats)\n",
    "    df = pd.concat([df.reset_index(drop=True), feats.reset_index(drop=True)], axis=1)\n",
    "    df = reduce_memory_usage(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T11:00:01.335387Z",
     "iopub.status.busy": "2025-12-02T11:00:01.335021Z",
     "iopub.status.idle": "2025-12-02T11:05:22.241298Z",
     "shell.execute_reply": "2025-12-02T11:05:22.239423Z",
     "shell.execute_reply.started": "2025-12-02T11:00:01.33537Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train = featurize_smiles(df_train)\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T11:05:22.244182Z",
     "iopub.status.busy": "2025-12-02T11:05:22.243867Z",
     "iopub.status.idle": "2025-12-02T11:05:28.365774Z",
     "shell.execute_reply": "2025-12-02T11:05:28.364503Z",
     "shell.execute_reply.started": "2025-12-02T11:05:22.244158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_test = featurize_smiles(df_test, True)\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T11:05:28.366961Z",
     "iopub.status.busy": "2025-12-02T11:05:28.366768Z",
     "iopub.status.idle": "2025-12-02T11:05:28.449221Z",
     "shell.execute_reply": "2025-12-02T11:05:28.447795Z",
     "shell.execute_reply.started": "2025-12-02T11:05:28.366947Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "extra_features_training = set(df_train.columns) - set(df_test.columns) - set([\"Tm\"])\n",
    "extra_features_test = set(df_test.columns) - set(df_train.columns)\n",
    "print(len(extra_features_training), len(extra_features_test))\n",
    "df_train.drop(columns=extra_features_training, inplace=True)\n",
    "df_test.drop(columns=extra_features_test, inplace=True)\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T11:05:28.451615Z",
     "iopub.status.busy": "2025-12-02T11:05:28.451367Z",
     "iopub.status.idle": "2025-12-02T11:05:28.467187Z",
     "shell.execute_reply": "2025-12-02T11:05:28.466313Z",
     "shell.execute_reply.started": "2025-12-02T11:05:28.451599Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def distribution_vs_gaussian(df, target):\n",
    "    \"\"\" show the distribution of target column vs gaussian\n",
    "    \"\"\" \n",
    "    _, axes = plt.subplots(1, 2, figsize=(20, 7))\n",
    "    sns.histplot(df[target], kde=True, stat=\"density\", ax=axes[0])\n",
    "    mu, std = norm.fit(df[target])\n",
    "    xmin, xmax = axes[0].get_xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = norm.pdf(x, mu, std)\n",
    "    axes[0].plot(x, p, 'k', linewidth=2)\n",
    "    axes[0].set(title='Distribution of target vs Gaussian')\n",
    "    pg.qqplot(df[target], dist='norm', ax=axes[1]) \n",
    "    axes[1].set(title='QQ-Plot of Target')\n",
    "\n",
    "def convert_object_to_category(df):\n",
    "    \"\"\" return a copy of the data with category types instead of objects \n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    categorical_cols = df.select_dtypes(exclude=[\"number\", \"bool\"])\n",
    "    for col in categorical_cols:\n",
    "        df_copy[col] = df_copy[col].astype('category')\n",
    "    return df_copy\n",
    "\n",
    "def calculate_vs_gaussian_r2(series):\n",
    "    \"\"\" calculate the r-squared between the target and a normal distribution.\n",
    "    \"\"\"\n",
    "    series = series.dropna().sort_values()\n",
    "    # generate theoretical normal quantiles\n",
    "    mean, std = series.mean(), series.std()\n",
    "    theoretical_quantiles = norm.ppf(np.linspace(0.01, 0.99, len(series)), loc=mean, scale=std)\n",
    "    return round(r2_score(series, theoretical_quantiles), 4)\n",
    "\n",
    "def target_transformation(df, target, method=\"log\"):\n",
    "    \"\"\" do transformation on the target variable. methods can be as follows:\n",
    "         + log:     logarithmic transformation\n",
    "         + sqrt:    square root\n",
    "         + cbrt:    cubic root\n",
    "         + box:     box-cox transformation\n",
    "         + yeo:     yeo-johnson transformation\n",
    "    \"\"\"   \n",
    "    new_target = target + f\"_{method}\"\n",
    "    if method == \"log\":\n",
    "        df[new_target] = np.log1p(df[target])\n",
    "        return df\n",
    "    elif method == \"sqrt\":\n",
    "        df[new_target] = np.sqrt(df[target])\n",
    "        return df\n",
    "    elif method == \"cbrt\":\n",
    "        df[new_target] = np.cbrt(df[target])\n",
    "        return df\n",
    "    elif method == \"box\":\n",
    "        df[new_target], box_lam = boxcox(df[target] + 1)\n",
    "        return df, box_lam\n",
    "    elif method == \"yeo\":\n",
    "        yeo_transformer = PowerTransformer(method='yeo-johnson')\n",
    "        df[new_target] = yeo_transformer.fit_transform(df[target].values.reshape(-1, 1)).flatten()\n",
    "        return df, yeo_transformer\n",
    "\n",
    "def inverse_transformation(series, method, box_lam=None, yeo_transformer=None):\n",
    "    \"\"\" do inverse transformation on the given series. methods can be as follows:\n",
    "         + log:     logarithmic transformation\n",
    "         + sqrt:    square root\n",
    "         + cbrt:    cubic root\n",
    "         + box:     box-cox transformation\n",
    "         + yeo:     yeo-johnson transformation\n",
    "    \"\"\"   \n",
    "    if method == \"norm\":\n",
    "        return series\n",
    "    elif method == \"log\":\n",
    "        return np.expm1(series)\n",
    "    elif method == \"sqrt\":\n",
    "        return np.square(series)\n",
    "    elif method == \"cbrt\":\n",
    "        return np.power(series, 3)\n",
    "    elif method == \"box\":\n",
    "        return inv_boxcox(series, box_lam) - 1\n",
    "    elif method == \"yeo\":\n",
    "        return yeo_transformer.inverse_transform(series.reshape(-1, 1)).flatten()\n",
    "\n",
    "def check_best_target_transformation(df, target):\n",
    "    \"\"\" check best possible target transformation method for closest-to-gaussian\n",
    "        + r2: check how close the transformed target to normal (gaussian) distribution by checking R2-score\n",
    "    \"\"\"\n",
    "    norm = df[target].copy()\n",
    "    log = np.log1p(df[target])\n",
    "    sqrt = np.sqrt(df[target])\n",
    "    cbrt = np.cbrt(df[target])\n",
    "    box, box_lam = boxcox(df[target] + 1)\n",
    "    yeo_transformer = PowerTransformer(method='yeo-johnson')\n",
    "    yeo = yeo_transformer.fit_transform(df[target].values.reshape(-1, 1)).flatten()\n",
    "    # dict of different approaches\n",
    "    series_dict = {'norm': norm, 'log': log, 'sqrt': sqrt, 'cbrt': cbrt, 'box': box, 'yeo': yeo}\n",
    "    # calculate r-squared for each\n",
    "    r2_scores = {method: calculate_vs_gaussian_r2(pd.Series(data)) for method, data in series_dict.items()}\n",
    "    print(r2_scores)\n",
    "    # find best methodology\n",
    "    best_method = max(r2_scores, key=r2_scores.get)\n",
    "    best_r2 = r2_scores[best_method]\n",
    "    print(f\"The best transformation is '{best_method}' with an R-squared of {best_r2:.4f}\")\n",
    "    return box_lam, yeo_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T11:05:28.468517Z",
     "iopub.status.busy": "2025-12-02T11:05:28.468224Z",
     "iopub.status.idle": "2025-12-02T11:05:29.397037Z",
     "shell.execute_reply": "2025-12-02T11:05:29.396316Z",
     "shell.execute_reply.started": "2025-12-02T11:05:28.468497Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "distribution_vs_gaussian(df_train, \"Tm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T11:06:22.37627Z",
     "iopub.status.busy": "2025-12-02T11:06:22.375851Z",
     "iopub.status.idle": "2025-12-02T11:06:22.585545Z",
     "shell.execute_reply": "2025-12-02T11:06:22.584482Z",
     "shell.execute_reply.started": "2025-12-02T11:06:22.376186Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# check best target transformation\n",
    "box_lam, yeo_transformer = check_best_target_transformation(df_train, \"Tm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filling null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T11:06:32.624684Z",
     "iopub.status.busy": "2025-12-02T11:06:32.624285Z",
     "iopub.status.idle": "2025-12-02T11:06:33.510082Z",
     "shell.execute_reply": "2025-12-02T11:06:33.509308Z",
     "shell.execute_reply.started": "2025-12-02T11:06:32.624669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "numeric_cols = list(set(df_test.select_dtypes(include=[\"number\"]).columns) - set([\"Tm\"]))\n",
    "df_train[numeric_cols] = df_train[numeric_cols].fillna(df_train[numeric_cols].mean()).fillna(0)\n",
    "df_test[numeric_cols] = df_test[numeric_cols].fillna(df_train[numeric_cols].mean()).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T11:06:36.560993Z",
     "iopub.status.busy": "2025-12-02T11:06:36.560674Z",
     "iopub.status.idle": "2025-12-02T11:06:36.567646Z",
     "shell.execute_reply": "2025-12-02T11:06:36.566957Z",
     "shell.execute_reply.started": "2025-12-02T11:06:36.560976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_x_and_y(df, exclude, target):\n",
    "    \"\"\" split target from the data \n",
    "    \"\"\"\n",
    "    X = df.drop(columns=exclude + [target]).copy()\n",
    "    y = df[target].copy()\n",
    "    return X, y\n",
    "\n",
    "def get_feature_importance(df, exclude, target, model, no_features_in_plot=30):\n",
    "    \"\"\" get feature importance by LGBMClassifier model\n",
    "    \"\"\"\n",
    "    # split data into X and y\n",
    "    X, y = split_x_and_y(df, exclude, target)\n",
    "    # fit the model\n",
    "    model.fit(X, y)\n",
    "    # get importances\n",
    "    importances = model.feature_importances_\n",
    "    importances = np.round(importances / sum(importances), 4)\n",
    "    sorted_idx = importances.argsort()[::-1]\n",
    "    df_importance = pd.DataFrame(data={\"feature\": X.columns[sorted_idx].astype(str), \"importance\": importances[sorted_idx]})\n",
    "    # summarize feature importance\n",
    "    plt.figure(figsize=(15, no_features_in_plot/4))\n",
    "    sns.barplot(x='importance', y='feature', data=df_importance[:no_features_in_plot], orient=\"h\", palette=\"Set3\")\n",
    "    plt.show()\n",
    "    return df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T11:06:38.253103Z",
     "iopub.status.busy": "2025-12-02T11:06:38.252016Z",
     "iopub.status.idle": "2025-12-02T11:06:38.260778Z",
     "shell.execute_reply": "2025-12-02T11:06:38.259702Z",
     "shell.execute_reply.started": "2025-12-02T11:06:38.253081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_models = {\n",
    "    \"ada\": AdaBoostRegressor(random_state=42),\n",
    "    \"xgbr\": XGBRegressor(random_state=42),\n",
    "    \"lgbm\": LGBMRegressor(verbose=-1, random_state=42),\n",
    "    \"catb\": CatBoostRegressor(silent=True, random_state=42),\n",
    "    \"rfr\": RandomForestRegressor(random_state=42),\n",
    "    \"extra\": ExtraTreesRegressor(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T11:06:46.195329Z",
     "iopub.status.busy": "2025-12-02T11:06:46.194424Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "importances = {}\n",
    "for model_n, model_i in base_models.items():\n",
    "    importances[model_n] = get_feature_importance(df_train, [\"SMILES\"], \"Tm\", model_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# top n features\n",
    "def top_n_features(n, best=True):\n",
    "    if best:\n",
    "        top_n = set()\n",
    "        for _, df_i in importances.items():\n",
    "            top_n = top_n.union(set(df_i[:n].feature))  \n",
    "        bottom_n = set(df_i.feature) - top_n  \n",
    "    else:\n",
    "        bottom_n = set()\n",
    "        for _, df_i in importances.items():\n",
    "            bottom_n = bottom_n.union(set(df_i[-n:].feature))  \n",
    "        top_n = set(df_i.feature) - bottom_n\n",
    "    return sorted(list(top_n)), sorted(list(bottom_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# try top n features\n",
    "def score_in_test(X, y, model):\n",
    "    # train / val split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42, shuffle=True)\n",
    "    # model prediction\n",
    "    model.fit(X_train, y_train)\n",
    "    pred_val = model.predict(X_val)\n",
    "    # get scores\n",
    "    return regression_metrics(y_val, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "top_n, bottom_n = top_n_features(73, False)\n",
    "print(len(top_n), len(bottom_n))\n",
    "# X / y split\n",
    "X, y = split_x_and_y(df_train, [\"SMILES\"] + bottom_n, \"Tm\")\n",
    "# cv predictions\n",
    "pred_cv = cross_val_predict(base_models[\"lgbm\"], X, y, cv=5) \n",
    "# score\n",
    "regression_metrics(y, pred_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def objective_lgbm(trial):\n",
    "    # set hyperparameter space\n",
    "    params = {        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 2000, step=250),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 3e-1, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256), \n",
    "        \"max_depth\":trial.suggest_int(\"max_depth\", 3, 16),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 1, 256),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 10),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0, 10),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 8),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0)\n",
    "    }    \n",
    "    \n",
    "    # set model\n",
    "    model = LGBMRegressor(random_state=42, verbose=-1, objective='regression_l1', metric='mae', **params)    \n",
    "    # get score\n",
    "    return score_in_test(X, y, model)[\"MAE\"] \n",
    "\n",
    "study_lgbm = optuna.create_study(direction='minimize')\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=5) \n",
    "# NOTE: this is just for showcasing! You guys can (or maybe should!) increase the n_trials first, and can also optimize using different hyperparameters!\n",
    "study_lgbm.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "study_lgbm.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tuned_model = LGBMRegressor(random_state=42, verbose=-1, objective='regression_l1', metric='mae', **study_lgbm.best_params)    \n",
    "score_in_test(X, y, tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tuned_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# get predictions\n",
    "preds = tuned_model.predict(df_test[X.columns])\n",
    "df_test[\"prediction\"] = preds.copy()\n",
    "df_test[\"id\"] = sample_submission.id.copy()\n",
    "df_test[[\"id\", \"SMILES\", \"Tm\", \"prediction\"]].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's save submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# submission\n",
    "print(all(df_test.id == sample_submission.id))\n",
    "df_test.Tm = df_test.Tm.fillna(df_test.prediction)\n",
    "df_test[[\"id\", \"Tm\"]].to_csv('df_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLZ UPVOTE the NOTEBOOK if you really liked it 🔥👍👍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13473948,
     "sourceId": 113155,
     "sourceType": "competition"
    },
    {
     "datasetId": 8675461,
     "sourceId": 13646947,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
