{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "919a2673",
   "metadata": {},
   "source": [
    "# Model Training Comparison\n",
    "Train multiple regressors on melting point features and compare metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11c9390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_predict,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# Models\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3acaa176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_bitstring_column(df: pd.DataFrame, column: str, prefix: str) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Expand a bitstring column into individual 0/1 columns.\"\"\"\n",
    "    if column not in df.columns:\n",
    "        return df, []\n",
    "    bit_lengths = df[column].dropna().map(len)\n",
    "    if bit_lengths.empty:\n",
    "        return df.drop(columns=[column]), []\n",
    "    bit_len = int(bit_lengths.max())\n",
    "    filled = df[column].fillna(\"0\" * bit_len)\n",
    "    bits_df = filled.apply(lambda s: pd.Series([int(ch) for ch in s[:bit_len]])).astype(np.int8)\n",
    "    bits_df.columns = [f\"{prefix}_{i}\" for i in bits_df.columns]\n",
    "    df = df.drop(columns=[column])\n",
    "    df = pd.concat([df, bits_df], axis=1)\n",
    "    return df, list(bits_df.columns)\n",
    "\n",
    "\n",
    "def expand_tuple_column(df: pd.DataFrame, column: str, prefix: str, expected: Optional[int] = None) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Expand a tuple-like column (e.g., PMI1/2/3) into numeric columns.\"\"\"\n",
    "    if column not in df.columns:\n",
    "        return df, []\n",
    "    def to_seq(val: object) -> Sequence:\n",
    "        if isinstance(val, str):\n",
    "            try:\n",
    "                parsed = json.loads(val)\n",
    "                if isinstance(parsed, list):\n",
    "                    return parsed\n",
    "            except Exception:\n",
    "                return []\n",
    "        if isinstance(val, (list, tuple)):\n",
    "            return val\n",
    "        return []\n",
    "    seq_series = df[column].apply(to_seq)\n",
    "    width = expected or int(seq_series.map(len).max() or 0)\n",
    "    cols: List[str] = []\n",
    "    if width:\n",
    "        expanded = pd.DataFrame(seq_series.apply(lambda seq: list(seq)[:width]).tolist(), index=df.index)\n",
    "        expanded = expanded.add_prefix(f\"{prefix}_\").astype(np.float32)\n",
    "        cols = list(expanded.columns)\n",
    "        df = pd.concat([df.drop(columns=[column]), expanded], axis=1)\n",
    "    else:\n",
    "        df = df.drop(columns=[column])\n",
    "    return df, cols\n",
    "\n",
    "\n",
    "def load_and_prepare_features(data_path: Path) -> Tuple[pd.DataFrame, pd.Series, Optional[pd.Series]]:\n",
    "    train_data = pd.read_csv(data_path)\n",
    "    TARGET_COLUMN = \"Tm\"\n",
    "    if TARGET_COLUMN not in train_data.columns:\n",
    "        raise KeyError(f\"Missing target column '{TARGET_COLUMN}'\")\n",
    "\n",
    "    smiles_col = \"canonical_smiles\" if \"canonical_smiles\" in train_data.columns else None\n",
    "    smiles: Optional[pd.Series] = None\n",
    "\n",
    "    drop_raw = [\"id\", \"gasteiger_charges\"]\n",
    "    df = train_data.drop(columns=[c for c in drop_raw if c in train_data.columns])\n",
    "    if smiles_col:\n",
    "        df = df.rename(columns={smiles_col: \"__smiles__\"})\n",
    "\n",
    "    df, morgan_cols = expand_bitstring_column(df, \"morgan_fingerprint_bits\", \"morgan\")\n",
    "    df, maccs_cols = expand_bitstring_column(df, \"maccs_keys_bits\", \"maccs\")\n",
    "    df, pmi_cols = expand_tuple_column(df, \"principal_moments_3d\", \"pmi\", expected=3)\n",
    "\n",
    "    target = df[TARGET_COLUMN].astype(np.float32)\n",
    "    features = df.drop(columns=[TARGET_COLUMN])\n",
    "    if \"__smiles__\" in features.columns:\n",
    "        smiles = features.pop(\"__smiles__\")\n",
    "\n",
    "    features = features.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    all_nan_cols = features.columns[features.isna().all()]\n",
    "    const_cols = [c for c in features.columns if features[c].nunique(dropna=False) <= 1]\n",
    "    drop_cols = sorted(set(all_nan_cols.tolist() + const_cols))\n",
    "    features = features.drop(columns=drop_cols)\n",
    "    median_values = features.median(numeric_only=True)\n",
    "    features = features.fillna(median_values).astype(np.float32)\n",
    "\n",
    "    print(\n",
    "        f\"Prepared features: {features.shape[0]} rows, {features.shape[1]} cols (\"\n",
    "        f\"+{len(morgan_cols)} morgan, +{len(maccs_cols)} maccs, +{len(pmi_cols)} pmi; \"\n",
    "        f\"dropped {len(all_nan_cols)} all-NaN cols, {len(const_cols)} constant cols)\"\n",
    "    )\n",
    "    return features, target, smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "309d17fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared features: 2660 rows, 2185 cols (+2048 morgan, +167 maccs, +0 pmi; dropped 2 all-NaN cols, 134 constant cols)\n",
      "Holdout: 2128 train rows, 532 valid rows, 2185 features\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(\"result/data/melting_point_features.csv\")\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"Data file not found at {data_path.resolve()}\")\n",
    "X, y, smiles = load_and_prepare_features(data_path)\n",
    "\n",
    "# Keep a quick holdout for sanity checks alongside CV\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "print(f\"Holdout: {X_train.shape[0]} train rows, {X_valid.shape[0]} valid rows, {X_train.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe32f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TARGET_TRANSFORM = True\n",
    "TARGET_TRANSFORMER = PowerTransformer(method=\"yeo-johnson\", standardize=True)\n",
    "\n",
    "def maybe_wrap(model):\n",
    "    if not USE_TARGET_TRANSFORM:\n",
    "        return model\n",
    "    return TransformedTargetRegressor(regressor=model, transformer=TARGET_TRANSFORMER)\n",
    "\n",
    "\n",
    "def generate_scaffold(smiles: str) -> str:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return \"parse_failed\"\n",
    "    try:\n",
    "        return MurckoScaffold.MurckoScaffoldSmiles(mol=mol) or \"no_scaffold\"\n",
    "    except Exception:\n",
    "        return \"no_scaffold\"\n",
    "\n",
    "\n",
    "def make_scaffold_folds(smiles: Optional[pd.Series], n_splits: int = 5, seed: int = 42) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    if smiles is None:\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "        return [(train_idx, val_idx) for train_idx, val_idx in kf.split(X)]\n",
    "    rng = check_random_state(seed)\n",
    "    smiles_seq = smiles.reset_index(drop=True)\n",
    "    scaffold_to_indices: Dict[str, List[int]] = {}\n",
    "    for idx, smi in enumerate(smiles_seq):\n",
    "        scaffold = generate_scaffold(smi)\n",
    "        scaffold_to_indices.setdefault(scaffold, []).append(idx)\n",
    "    # Largest scaffolds first, then round-robin assign to folds\n",
    "    sorted_scaffolds = sorted(scaffold_to_indices.items(), key=lambda kv: len(kv[1]), reverse=True)\n",
    "    folds: List[List[int]] = [[] for _ in range(n_splits)]\n",
    "    for i, (_, idxs) in enumerate(sorted_scaffolds):\n",
    "        target_fold = i % n_splits\n",
    "        rng.shuffle(idxs)\n",
    "        folds[target_fold].extend(idxs)\n",
    "    split_indices: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "    all_indices = np.arange(len(smiles_seq))\n",
    "    for fold in folds:\n",
    "        val_idx = np.array(sorted(fold))\n",
    "        train_idx = np.setdiff1d(all_indices, val_idx)\n",
    "        split_indices.append((train_idx, val_idx))\n",
    "    return split_indices\n",
    "\n",
    "\n",
    "def evaluate_holdout(name, model, X_train, y_train, X_valid, y_valid):\n",
    "    mdl = clone(model)\n",
    "    mdl.fit(X_train, y_train)\n",
    "    y_pred = mdl.predict(X_valid)\n",
    "    mae = mean_absolute_error(y_valid, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
    "    r2 = r2_score(y_valid, y_pred)\n",
    "    return {\"model\": name, \"mae_holdout\": float(mae), \"rmse_holdout\": float(rmse), \"r2_holdout\": float(r2)}\n",
    "\n",
    "\n",
    "def evaluate_cv(name, model, X, y, folds: List[Tuple[np.ndarray, np.ndarray]]):\n",
    "    preds = np.zeros(len(y), dtype=float)\n",
    "    for train_idx, val_idx in folds:\n",
    "        mdl = clone(model)\n",
    "        mdl.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "        preds[val_idx] = mdl.predict(X.iloc[val_idx])\n",
    "    mae = mean_absolute_error(y, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y, preds))\n",
    "    r2 = r2_score(y, preds)\n",
    "    return {\"model\": name, \"mae_cv\": float(mae), \"rmse_cv\": float(rmse), \"r2_cv\": float(r2)}\n",
    "\n",
    "\n",
    "def tune_lightgbm(X, y, folds: List[Tuple[np.ndarray, np.ndarray]]):\n",
    "    base = LGBMRegressor(\n",
    "        objective=\"regression_l1\",\n",
    "        random_state=42,\n",
    "        n_estimators=1600,\n",
    "        learning_rate=0.03,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        verbose=-1,\n",
    "    )\n",
    "    param_dist = {\n",
    "        \"num_leaves\": [63, 95, 127, 191],\n",
    "        \"max_depth\": [-1, 10, 14, 18, 22],\n",
    "        \"min_child_samples\": [10, 20, 40, 80],\n",
    "        \"subsample\": [0.7, 0.8, 0.9, 1.0],\n",
    "        \"colsample_bytree\": [0.6, 0.75, 0.9, 1.0],\n",
    "        \"reg_alpha\": [0.0, 0.05, 0.1, 0.2],\n",
    "        \"reg_lambda\": [0.0, 0.1, 0.5, 1.0],\n",
    "    }\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=24,\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=folds,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "    )\n",
    "    search.fit(X, y)\n",
    "    print(f\"Best LightGBM params: {search.best_params_}\")\n",
    "    return search.best_estimator_, search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa479cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ScaffoldKFold with 5 folds\n"
     ]
    }
   ],
   "source": [
    "folds = make_scaffold_folds(smiles, n_splits=5, seed=42)\n",
    "cv_label = \"ScaffoldKFold\" if smiles is not None else \"KFold\"\n",
    "print(f\"Using {cv_label} with {len(folds)} folds\")\n",
    "\n",
    "tuned_lgbm, best_params = tune_lightgbm(X, y, folds)\n",
    "models = [\n",
    "    (\"lightgbm_tuned\", maybe_wrap(tuned_lgbm)),\n",
    "    (\"random_forest\", maybe_wrap(RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1, max_depth=None))),\n",
    "    (\"extra_trees\", maybe_wrap(ExtraTreesRegressor(n_estimators=600, random_state=42, n_jobs=-1, max_depth=None))),\n",
    "    (\"grad_boost\", maybe_wrap(GradientBoostingRegressor(random_state=42, n_estimators=800, learning_rate=0.05, max_depth=3))),\n",
    "    (\"hist_grad_boost\", maybe_wrap(HistGradientBoostingRegressor(random_state=42, max_depth=12, learning_rate=0.05, max_iter=900))),\n",
    "    (\"lightgbm_baseline\", maybe_wrap(LGBMRegressor(objective=\"regression\", random_state=42, n_estimators=1200, learning_rate=0.03, num_leaves=128, subsample=0.85, colsample_bytree=0.85, min_child_samples=20, reg_lambda=0.5, reg_alpha=0.1, verbose=-1))),\n",
    " ]\n",
    "\n",
    "results = []\n",
    "for name, mdl in models:\n",
    "    print(f\"Training {name}...\")\n",
    "    holdout_metrics = evaluate_holdout(name, mdl, X_train, y_train, X_valid, y_valid)\n",
    "    cv_metrics = evaluate_cv(name, mdl, X, y, folds)\n",
    "    merged = {**holdout_metrics, **cv_metrics}\n",
    "    if name == \"lightgbm_tuned\":\n",
    "        merged[\"best_params\"] = best_params\n",
    "    results.append(merged)\n",
    "\n",
    "results_df = pd.DataFrame(results).set_index(\"model\")\n",
    "display(results_df[[c for c in results_df.columns if c.startswith(\"mae\") or c.startswith(\"rmse\") or c.startswith(\"r2\")]])\n",
    "best = results_df.sort_values(\"mae_cv\").head(3)\n",
    "print(\"Best (by CV MAE):\")\n",
    "display(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f6a4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metrics to result/data/model_comparison_metrics.json and result/data/model_comparison_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Save metrics\n",
    "output_dir = Path(\"result/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "metrics_json = output_dir / \"model_comparison_metrics.json\"\n",
    "metrics_csv = output_dir / \"model_comparison_metrics.csv\"\n",
    "metrics_json.write_text(json.dumps(results, indent=2))\n",
    "results_df.to_csv(metrics_csv)\n",
    "print(f\"Saved metrics to {metrics_json} and {metrics_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
