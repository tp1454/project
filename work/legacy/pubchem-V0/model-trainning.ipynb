{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2810b664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package install check failed for ('scikit-learn', 'scikit-learn'): name 'ensure_package' is not defined\n",
      "Package install check failed for ('lightgbm', 'lightgbm'): name 'ensure_package' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Install baseline modeling deps if missing (best-effort)\n",
    "for pkg in [\n",
    "    ('scikit-learn', 'scikit-learn'),\n",
    "    ('lightgbm', 'lightgbm'),\n",
    "]:\n",
    "    try:\n",
    "        ensure_package(pkg[0], pkg[1])\n",
    "    except Exception as exc:\n",
    "        print(f\"Package install check failed for {pkg}: {exc}\")\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "#stratifed kfold \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0dc2f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, Iterable, Optional, Tuple\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from rdkit import Chem, rdBase, RDLogger\n",
    "from rdkit.Chem import AllChem, Crippen, Descriptors, Fragments, Lipinski, rdMolDescriptors, rdFingerprintGenerator\n",
    "from rdkit.Chem.MACCSkeys import GenMACCSKeys\n",
    "from rdkit.Chem.EState import AtomTypes as EAtomTypes\n",
    "\n",
    "try:\n",
    "    from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "except Exception:\n",
    "    MurckoScaffold = None\n",
    "\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "rdBase.DisableLog('rdApp.*')\n",
    "\n",
    "DATA_DIR = Path('../../main-data')\n",
    "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
    "PUBCHEM_PATH = Path('result/data/melting_point_features.csv')\n",
    "TEST_PATH = DATA_DIR / 'test.csv'  # optional\n",
    "OUTPUT_DIR = Path('result/data')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MORGAN_BITS = 512\n",
    "MORGAN_RADIUS = 2\n",
    "USE_MACCS = True\n",
    "COMPUTE_3D = True\n",
    "MAX_ITERS_3D = 0  # 0 = no optimization, >0 enables a short UFF optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ef6311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_feat = pd.read_csv(PUBCHEM_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cad4506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows used: 10520\n",
      "Numeric features: 937\n",
      "Target stats: -5126.0 273.41996007604564 6332.0\n"
     ]
    }
   ],
   "source": [
    "# Build X/y from the featurized training frame\n",
    "assert 'Tm' in df_train_feat.columns, \"Expected target column 'Tm' in df_train_feat\"\n",
    "assert 'SMILES' in df_train_feat.columns, \"Expected 'SMILES' column in df_train_feat\"\n",
    "\n",
    "work_df = df_train_feat.copy()\n",
    "work_df = work_df.dropna(subset=['Tm']).reset_index(drop=True)\n",
    "\n",
    "# Use numeric features only (exclude identifiers/strings)\n",
    "feature_cols = [c for c in work_df.columns if c not in ('Tm', 'SMILES')]\n",
    "X = work_df[feature_cols].select_dtypes(include=['number'])\n",
    "y = work_df['Tm'].astype(float)\n",
    "\n",
    "print('Rows used:', len(work_df))\n",
    "print('Numeric features:', X.shape[1])\n",
    "print('Target stats:', float(y.min()), float(y.mean()), float(y.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590b4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQR bounds for Tm: [-250.000, 750.000] (Q1=97.700, Q3=406.400, IQR=308.700)\n",
      "Removed outliers: 640/10520 (6.08%)\n",
      "After outlier removal -> Rows: 9880 Numeric features: 937\n"
     ]
    }
   ],
   "source": [
    "# Outlier removal \n",
    "Tm = work_df['Tm'].astype(float)\n",
    " \n",
    "lower = -250\n",
    "upper = 750\n",
    "\n",
    "mask = (Tm >= lower) & (Tm <= upper)\n",
    "removed = int((~mask).sum())\n",
    "total = int(len(work_df))\n",
    "pct_removed = 100.0 * removed / total if total else 0.0\n",
    "\n",
    "print(f\"IQR bounds for Tm: [{lower:.3f}, {upper:.3f}]\")\n",
    "print(f\"Removed outliers: {removed}/{total} ({pct_removed:.2f}%)\")\n",
    "\n",
    "# Apply filter\n",
    "work_df = work_df.loc[mask].reset_index(drop=True)\n",
    "feature_cols = [c for c in work_df.columns if c not in ('Tm', 'SMILES')]\n",
    "X = work_df[feature_cols].select_dtypes(include=['number'])\n",
    "y = work_df['Tm'].astype(float)\n",
    "#y_transformed = np.sign(y) * np.log1p(np.abs(y)) add log transform\n",
    "\n",
    "print('After outlier removal -> Rows:', len(work_df), 'Numeric features:', X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd01ed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count per feature:\n",
      "MaxAbsEStateIndex    0\n",
      "MaxEStateIndex       0\n",
      "MinAbsEStateIndex    0\n",
      "MinEStateIndex       0\n",
      "qed                  0\n",
      "                    ..\n",
      "SMI_len              0\n",
      "SMI_branches         0\n",
      "SMI_ringDigits       0\n",
      "SMI_stereoAt         0\n",
      "SMI_ezSlashes        0\n",
      "Length: 937, dtype: int64\n",
      "\n",
      "Inf count per feature:\n",
      "MaxAbsEStateIndex    0\n",
      "MaxEStateIndex       0\n",
      "MinAbsEStateIndex    0\n",
      "MinEStateIndex       0\n",
      "qed                  0\n",
      "                    ..\n",
      "SMI_len              0\n",
      "SMI_branches         0\n",
      "SMI_ringDigits       0\n",
      "SMI_stereoAt         0\n",
      "SMI_ezSlashes        0\n",
      "Length: 937, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for inf and NaN values\n",
    "print(\"NaN count per feature:\")\n",
    "print(X.isna().sum())\n",
    "print(\"\\nInf count per feature:\")\n",
    "print(np.isinf(X).sum())\n",
    "\n",
    "# Replace inf with NaN, then impute\n",
    "X = X.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac661f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <function _log_callback at 0x7c1ae0605120>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tp_ubuntu/project/.venv/lib/python3.12/site-packages/lightgbm/basic.py\", line 287, in _log_callback\n",
      "    def _log_callback(msg: bytes) -> None:\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-choosing col-wise multi-threading, the overhead of testing was 3.201664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 31486\n",
      "[LightGBM] [Info] Number of data points in the train set: 7904, number of used features: 868\n",
      "[LightGBM] [Info] Start training from score 280.399994\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_imp = imputer.fit_transform(X_train)\n",
    "X_valid_imp = imputer.transform(X_valid)\n",
    "\n",
    "model = lgb.LGBMRegressor(\n",
    "    objective='quantile',\n",
    "    alpha=0.5,\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    min_child_samples=100,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train_imp, y_train,\n",
    "    eval_set=[(X_valid_imp, y_valid)],\n",
    "    eval_metric='l1',\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6674de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_valid = model.predict(X_valid_imp, num_iteration=model.best_iteration_)\n",
    "mae = mean_absolute_error(y_valid, pred_valid)\n",
    "rmse = np.sqrt(mean_squared_error(y_valid, pred_valid))\n",
    "r2 = r2_score(y_valid, pred_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ccb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metrics to result/data/baseline_lgbm_metrics.json\n",
      "Saved predictions to result/data/baseline_lgbm_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Save baseline outputs\n",
    "baseline_metrics = {\n",
    "    'mae': mae,\n",
    "    'rmse': rmse,\n",
    "    'r2': r2,\n",
    "    'n_rows': int(len(work_df)),\n",
    "    'n_features': int(X.shape[1]),\n",
    "    'best_iteration': int(getattr(model, 'best_iteration_', 0) or 0),\n",
    "}\n",
    "import json\n",
    "metrics_path = OUTPUT_DIR / 'baseline_lgbm_metrics.json'\n",
    "with open(metrics_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(baseline_metrics, f, indent=2)\n",
    "print('Saved metrics to', metrics_path)\n",
    "\n",
    "pred_path = OUTPUT_DIR / 'baseline_lgbm_predictions.csv'\n",
    "pred_df = pd.DataFrame({\n",
    "    'y_true': y_valid.reset_index(drop=True),\n",
    "    'y_pred': pred_valid,\n",
    "})\n",
    "pred_df.to_csv(pred_path, index=False)\n",
    "print('Saved predictions to', pred_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
