{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba85359e",
   "metadata": {},
   "source": [
    "# IMPORT DATA AND REMOVE OUTLIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a384c133",
   "metadata": {},
   "source": [
    "## Import Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64a22807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df_original = pd.read_csv('result/data/melting_point_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad834c6e",
   "metadata": {},
   "source": [
    "## REMOVE OUTLIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d500dc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Báº®T Äáº¦U Xá»¬ LÃ & CHIA Dá»® LIá»†U ---\n",
      "1. Dá»¯ liá»‡u sau khi lá»c: 8766 dÃ²ng (ÄÃ£ loáº¡i 1754 nhiá»…u)\n",
      "------------------------------\n",
      "âœ… HOÃ€N Táº¤T! Dá»¯ liá»‡u Ä‘Ã£ sáº¡ch vÃ  Ä‘Æ°á»£c chia:\n",
      "   - X_train: (7012, 937) | y_train (Log): (7012,)\n",
      "   - X_test : (1754, 937)  | y_test  (Log): (1754,)\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Báº®T Äáº¦U Xá»¬ LÃ & CHIA Dá»® LIá»†U ---\")\n",
    "\n",
    "df_clean = df_original.dropna(subset=['Tm'])\n",
    "df_clean = df_clean[(df_clean['Tm'] > 0) & (df_clean['Tm'] < 4000)].copy()\n",
    "\n",
    "print(f\"1. Dá»¯ liá»‡u sau khi lá»c: {len(df_clean)} dÃ²ng (ÄÃ£ loáº¡i {len(df_original) - len(df_clean)} nhiá»…u)\")\n",
    "\n",
    "y = df_clean['Tm']\n",
    "X = df_clean.drop(columns=['Tm', 'SMILES'], errors='ignore').select_dtypes(include=[np.number])\n",
    "\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(X)\n",
    "X_final = pd.DataFrame(imputer.transform(X), columns=X.columns)\n",
    "\n",
    "joblib.dump(imputer, 'result/imputer.pkl')\n",
    "\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, \n",
    "    y_log, \n",
    "    test_size=0.2, \n",
    "    random_state=2601\n",
    ")\n",
    "\n",
    "y_test_real = np.expm1(y_test)\n",
    "joblib.dump(imputer, 'imputer.pkl')\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"âœ… HOÃ€N Táº¤T! Dá»¯ liá»‡u Ä‘Ã£ sáº¡ch vÃ  Ä‘Æ°á»£c chia:\")\n",
    "print(f\"   - X_train: {X_train.shape} | y_train (Log): {y_train.shape}\")\n",
    "print(f\"   - X_test : {X_test.shape}  | y_test  (Log): {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32825ac6",
   "metadata": {},
   "source": [
    "# MODEL TRAINING: FEATURES SELECTION USING RFECV, GA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e176a307",
   "metadata": {},
   "source": [
    "## RFECV Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "915a736d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. CHáº Y RFECV ---\n",
      "Fitting estimator with 937 features.\n",
      "Fitting estimator with 917 features.\n",
      "Fitting estimator with 897 features.\n",
      "Fitting estimator with 877 features.\n",
      "Fitting estimator with 857 features.\n",
      "Fitting estimator with 837 features.\n",
      "Fitting estimator with 817 features.\n",
      "Fitting estimator with 797 features.\n",
      "Fitting estimator with 777 features.\n",
      "Fitting estimator with 757 features.\n",
      "Fitting estimator with 737 features.\n",
      "Fitting estimator with 717 features.\n",
      "Fitting estimator with 697 features.\n",
      "Fitting estimator with 677 features.\n",
      "Fitting estimator with 657 features.\n",
      "Fitting estimator with 637 features.\n",
      "Fitting estimator with 617 features.\n",
      "Fitting estimator with 597 features.\n",
      "Fitting estimator with 577 features.\n",
      "Fitting estimator with 557 features.\n",
      "â±ï¸ Time Run: 543.84 s\n",
      "âœ… RFECV Selected: 537 features\n",
      "\n",
      "--- 3. ÄÃNH GIÃ ---\n",
      "\n",
      "========================================\n",
      "ðŸ“Š Káº¾T QUáº¢ RFECV (REAL SCALE)\n",
      "Features : 537\n",
      "MAE      : 83.3528 K\n",
      "RMSE     : 193.6333 K\n",
      "R2       : 0.5936\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['result/features_list_rfecv.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- 2. CHáº Y RFECV ---\")\n",
    "start = time.time()\n",
    "\n",
    "model_rfe = LGBMRegressor(\n",
    "    objective='regression',\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    random_state=2601,\n",
    "    n_jobs=-1, verbose=-1\n",
    ")\n",
    "\n",
    "rfe = RFECV(\n",
    "    estimator=model_rfe,\n",
    "    min_features_to_select=50,\n",
    "    step=20,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "selected_rfe = X_train.columns[rfe.support_]\n",
    "print(f\"â±ï¸ Time Run: {time.time() - start:.2f} s\")\n",
    "print(f\"âœ… RFECV Selected: {len(selected_rfe)} features\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 3. ÄÃNH GIÃ ---\")\n",
    "\n",
    "eval_model = LGBMRegressor(\n",
    "    n_estimators=3000, learning_rate=0.01, num_leaves=50, max_depth=-1,\n",
    "    subsample=0.8, colsample_bytree=0.7, random_state=2601, n_jobs=-1, verbose=-1\n",
    ")\n",
    "\n",
    "eval_model.fit(X_train[selected_rfe], y_train)\n",
    "\n",
    "y_pred_log = eval_model.predict(X_test[selected_rfe])\n",
    "y_pred_real = np.expm1(y_pred_log)\n",
    "y_test_real = np.expm1(y_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test_real, y_pred_real)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_real, y_pred_real))\n",
    "r2 = r2_score(y_test_real, y_pred_real)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"ðŸ“Š Káº¾T QUáº¢ RFECV (REAL SCALE)\")\n",
    "print(f\"Features : {len(selected_rfe)}\")\n",
    "print(f\"MAE      : {mae:.4f} K\")\n",
    "print(f\"RMSE     : {rmse:.4f} K\")\n",
    "print(f\"R2       : {r2:.4f}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "joblib.dump(eval_model, 'result/melting_point_model_rfecv.pkl')\n",
    "joblib.dump(list(selected_rfe), 'result/features_list_rfecv.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcada578",
   "metadata": {},
   "source": [
    "## GENETIC ALGORITHM (GA) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1334ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_genetic import GAFeatureSelectionCV\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- 2. CHáº Y GENETIC ALGORITHM (GA) ---\")\n",
    "start = time.time()\n",
    "\n",
    "model_ga = LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31, \n",
    "    ax_depth=-1,\n",
    "    random_state=2601,\n",
    "    n_jobs=1, verbose=-1\n",
    ")\n",
    "\n",
    "ga = GAFeatureSelectionCV(\n",
    "    estimator=model_ga,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    population_size=50,\n",
    "    generations=15,\n",
    "    mutation_probability=0.1,\n",
    "    crossover_probability=0.8,\n",
    "    keep_top_k=2,\n",
    "    elitism=True,\n",
    "    n_jobs=-1, verbose=True\n",
    ")\n",
    "\n",
    "ga.fit(X_train, y_train)\n",
    "\n",
    "selected_ga = X_train.columns[ga.support_]\n",
    "\n",
    "print(f\"Time Run: {time.time() - start:.2f} s\")\n",
    "print(f\"\\nâœ… GA Chosen {len(selected_ga)} features\")\n",
    "joblib.dump(list(selected_ga), 'result/ga_features.pkl')\n",
    "\n",
    "print(\"\\n--- 3. ÄÃNH GIÃ (EVALUATION) ---\")\n",
    "eval_model = LGBMRegressor(\n",
    "    n_estimators=3000, learning_rate=0.01, num_leaves=50, max_depth=-1,\n",
    "    subsample=0.8, colsample_bytree=0.7, random_state=2601, n_jobs=-1, verbose=-1\n",
    ")\n",
    "\n",
    "eval_model.fit(X_train[selected_ga], y_train)\n",
    "\n",
    "y_pred_log = eval_model.predict(X_test[selected_ga])\n",
    "\n",
    "y_pred_real = np.expm1(y_pred_log)\n",
    "\n",
    "y_test_real = np.expm1(y_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test_real, y_pred_real)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_real, y_pred_real))\n",
    "r2 = r2_score(y_test_real, y_pred_real)\n",
    "\n",
    "print(\"\\n--- RESULT GA (REAL SCALE) ---\")\n",
    "print(f\"Features: {len(selected_ga)}\")\n",
    "print(f\"MAE     : {mae:.4f} K\")\n",
    "print(f\"RMSE    : {rmse:.4f} K\")\n",
    "print(f\"R2      : {r2:.4f}\")\n",
    "\n",
    "joblib.dump(model_ga, 'result/melting_point_model_ga.pkl')\n",
    "joblib.dump(list(selected_ga), 'result/features_list_ga.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586459f5",
   "metadata": {},
   "source": [
    "# UNION AND INTERSECTION 2 MODEL TRAINNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba85fe",
   "metadata": {},
   "source": [
    "## FEATURES UNION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "rfe_features = joblib.load('result/rfe_features.pkl')\n",
    "ga_features = joblib.load('result/ga_features.pkl')\n",
    "\n",
    "common_features_uni = set(rfe_features) | set(ga_features)\n",
    "\n",
    "print(f\"\\nðŸ’Ž Tá»•ng features sau khi há»£p (Union): {len(common_features_uni)}\")\n",
    "print(list(common_features_uni))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y_log, test_size=0.2, random_state=2601\n",
    ")\n",
    "\n",
    "features_uni = list(common_features_uni)\n",
    "valid_features_uni = [f for f in features_uni if f in X_final.columns]\n",
    "\n",
    "manual_params = {\n",
    "    'n_estimators': 8000,\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 40,\n",
    "    'max_depth': 15,\n",
    "    'min_child_samples': 20,\n",
    "    \n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.6,\n",
    "    \n",
    "    'reg_alpha': 0.2,\n",
    "    'reg_lambda': 0.2,\n",
    "    \n",
    "    'random_state': 2601,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "model_uni = LGBMRegressor(**manual_params)\n",
    "model_uni.fit(X_train[valid_features_uni], y_train)\n",
    "\n",
    "joblib.dump(model_uni, 'result/melting_point_model_uni.pkl')\n",
    "joblib.dump(valid_features_uni, 'result/features_list_uni.pkl')\n",
    "\n",
    "print(\"ðŸ’¾ ÄÃ£ lÆ°u model vÃ  features Ä‘Æ°á»£c Union thÃ nh cÃ´ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2311e6a5",
   "metadata": {},
   "source": [
    "## Score Union Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7549c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "model = joblib.load('result/melting_point_model_uni.pkl')\n",
    "features = joblib.load('result/features_list_uni.pkl')\n",
    "\n",
    "print(f\"ÄÃ£ load model. Äang Ä‘Ã¡nh giÃ¡ trÃªn táº­p Test cÃ³ ({len(X_test)} máº«u)...\")\n",
    "\n",
    "y_pred_raw = model.predict(X_test[features])\n",
    "\n",
    "if y_test.max() > 15:\n",
    "    y_test_real_eval = y_test\n",
    "    print(\"PhÃ¡t hiá»‡n y_test lÃ  thang Ä‘o Thá»±c (giá»¯ nguyÃªn).\")\n",
    "else:\n",
    "    y_test_real_eval = np.expm1(y_test)\n",
    "    print(\"PhÃ¡t hiá»‡n y_test lÃ  thang Ä‘o Log (Ä‘Ã£ chuyá»ƒn vá» Thá»±c).\")\n",
    "\n",
    "if y_pred_raw.max() > 15:\n",
    "    y_pred_real = y_pred_raw\n",
    "else:\n",
    "    y_pred_real = np.expm1(y_pred_raw)\n",
    "\n",
    "mae = mean_absolute_error(y_test_real_eval, y_pred_real)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_real_eval, y_pred_real))\n",
    "r2 = r2_score(y_test_real_eval, y_pred_real)\n",
    "\n",
    "print(\"\\n--- RESULT UNION ---\")\n",
    "print(f\"MAE : {mae:.4f} K\")\n",
    "print(f\"RMSE: {rmse:.4f} K\")\n",
    "print(f\"R2  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67801f9",
   "metadata": {},
   "source": [
    "## FEATURES INTERSECTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550942b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "rfe_features = joblib.load('result/rfe_features.pkl')\n",
    "ga_features = joblib.load('result/ga_features.pkl')\n",
    "\n",
    "common_features_int = set(rfe_features) & set(ga_features)\n",
    "\n",
    "print(f\"\\nðŸ’Ž Tá»•ng features sau khi giao (Intersection): {len(common_features_int)}\")\n",
    "print(list(common_features_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0e2cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "best_features_int = list(common_features_int)\n",
    "valid_features_int = [f for f in best_features_int if f in X_final.columns]\n",
    "\n",
    "manual_params = {\n",
    "    'n_estimators': 8000,\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 40,\n",
    "    'max_depth': 15,\n",
    "    'min_child_samples': 20,\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'reg_alpha': 0.2,\n",
    "    'reg_lambda': 0.2,\n",
    "    'random_state': 2601,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "model_int = LGBMRegressor(**manual_params)\n",
    "\n",
    "model_int.fit(X_train[valid_features_int], y_train)\n",
    "\n",
    "joblib.dump(model_int, 'result/melting_point_model_int.pkl')\n",
    "joblib.dump(valid_features_int, 'result/features_list_int.pkl')\n",
    "\n",
    "print(f\"ðŸ’¾ ÄÃ£ lÆ°u Intersection ({len(valid_features_int)} features) thÃ nh cÃ´ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae651ea9",
   "metadata": {},
   "source": [
    "## Score Intersection Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda522f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "model_int = joblib.load('result/melting_point_model_int.pkl')\n",
    "features_int = joblib.load('result/features_list_int.pkl')\n",
    "\n",
    "print(f\"ÄÃ£ load Model Intersection. Äang Ä‘Ã¡nh giÃ¡ trÃªn táº­p Test cÃ³ ({len(X_test)} máº«u)...\")\n",
    "\n",
    "y_pred_raw = model_int.predict(X_test[features_int])\n",
    "\n",
    "if y_test.max() > 15:\n",
    "    y_test_real_eval = y_test\n",
    "    print(\"PhÃ¡t hiá»‡n y_test lÃ  thang Ä‘o Thá»±c.\")\n",
    "else:\n",
    "    y_test_real_eval = np.expm1(y_test)\n",
    "    print(\"PhÃ¡t hiá»‡n y_test lÃ  thang Ä‘o Log (Ä‘Ã£ chuyá»ƒn vá» Thá»±c).\")\n",
    "\n",
    "if y_pred_raw.max() > 15:\n",
    "    y_pred_real = y_pred_raw\n",
    "else:\n",
    "    y_pred_real = np.expm1(y_pred_raw)\n",
    "\n",
    "\n",
    "mae = mean_absolute_error(y_test_real_eval, y_pred_real)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_real_eval, y_pred_real))\n",
    "r2 = r2_score(y_test_real_eval, y_pred_real)\n",
    "\n",
    "print(\"\\n--- Káº¾T QUáº¢ INTERSECTION ---\")\n",
    "print(f\"MAE : {mae:.4f} K\")\n",
    "print(f\"RMSE: {rmse:.4f} K\")\n",
    "print(f\"R2  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b8109b",
   "metadata": {},
   "source": [
    "# Score each Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9640c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_metrics(name, feature_list):\n",
    "    valid_feats = [f for f in feature_list if f in X_train.columns]\n",
    "\n",
    "    if not valid_feats: \n",
    "        return {\"Method\": name, \"Features\": 0, \"R2\": 0, \"MAE\": 0, \"RMSE\": 0}\n",
    "\n",
    "    model = LGBMRegressor(\n",
    "        n_jobs=-1, verbose=-1, random_state=2601,\n",
    "        n_estimators=8000, learning_rate=0.01,\n",
    "        num_leaves=45, max_depth=-1,\n",
    "        subsample=0.8, objective = 'regression_l1', metric= 'mae',\n",
    "        colsample_bytree=0.7, reg_alpha=0.1, reg_lambda=0.1\n",
    "    )\n",
    "\n",
    "    callbacks = [lgb.early_stopping(stopping_rounds=1000, verbose=False)]\n",
    "\n",
    "    model.fit(X_train[valid_feats], y_train, \n",
    "              eval_set=[(X_test[valid_feats], y_test)],\n",
    "              eval_metric='rmse', callbacks=callbacks)\n",
    "\n",
    "    y_pred_real = np.expm1(model.predict(X_test[valid_feats], num_iteration=model.best_iteration_))\n",
    "    \n",
    "    if y_test.max() > 15:\n",
    "         y_test_real = y_test\n",
    "    else:\n",
    "         y_test_real = np.expm1(y_test)\n",
    "\n",
    "    return {\n",
    "        \"Method\": name, \n",
    "        \"Features\": len(valid_feats),\n",
    "        \"R2\": r2_score(y_test_real, y_pred_real),\n",
    "        \"MAE\": mean_absolute_error(y_test_real, y_pred_real),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_test_real, y_pred_real))\n",
    "    }\n",
    "\n",
    "feats_all = list(X_train.columns)\n",
    "\n",
    "try: \n",
    "    feats_rfe = joblib.load('result/rfe_features.pkl')\n",
    "except: \n",
    "    feats_rfe = []\n",
    "    print(\"KhÃ´ng tÃ¬m tháº¥y file RFE features\")\n",
    "\n",
    "try: \n",
    "    feats_ga = joblib.load('result/ga_features.pkl')\n",
    "except: \n",
    "    feats_ga = []\n",
    "    print(\"KhÃ´ng tÃ¬m tháº¥y file GA features\")\n",
    "\n",
    "feats_uni = list(set(feats_rfe) | set(feats_ga))\n",
    "feats_int = list(set(feats_rfe) & set(feats_ga))\n",
    "\n",
    "results = []\n",
    "results.append(get_metrics(\"Original\", feats_all))\n",
    "\n",
    "if feats_rfe: results.append(get_metrics(\"RFE\", feats_rfe))\n",
    "if feats_ga:  results.append(get_metrics(\"GA\", feats_ga))\n",
    "if feats_uni: results.append(get_metrics(\"Union (RFE|GA)\", feats_uni))\n",
    "if feats_int: results.append(get_metrics(\"Intersect (RFE&GA)\", feats_int))\n",
    "\n",
    "df_res = pd.DataFrame(results)\n",
    "if not df_res.empty:\n",
    "    base = df_res.iloc[0]\n",
    "    df_res['Diff_R2'] = df_res['R2'] - base['R2']\n",
    "    df_res['Diff_MAE'] = df_res['MAE'] - base['MAE']\n",
    "    df_res['Diff_RMSE'] = df_res['RMSE'] - base['RMSE']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*85)\n",
    "    print(\"             Báº¢NG SO SÃNH HIá»†U QUáº¢ CÃC PHÆ¯Æ NG PHÃP CHá»ŒN FEATURES\")\n",
    "    print(\"=\"*85)\n",
    "    print(df_res.round(4).to_string(index=False))\n",
    "    print(\"=\"*85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4d57d2",
   "metadata": {},
   "source": [
    "# ELBOW PLOT TRAIN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709c9b7",
   "metadata": {},
   "source": [
    "## Elbow GA Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098e2a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "best_params = {\n",
    "    'learning_rate': 0.05, \n",
    "    'n_estimators': 3000,\n",
    "    'num_leaves': 100,\n",
    "    'max_depth': 12,\n",
    "    'min_child_samples': 10,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'reg_alpha': 0.0,\n",
    "    'reg_lambda': 0.0,\n",
    "    'random_state': 2601,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "valid_ga_feats = [f for f in list(selected_ga) if f in X_train.columns]\n",
    "\n",
    "print(\"Äang xáº¿p háº¡ng features...\")\n",
    "ranker = LGBMRegressor(**best_params)\n",
    "ranker.fit(X_train[valid_ga_feats], y_train)\n",
    "\n",
    "imp_df = pd.DataFrame({\n",
    "    'Feature': valid_ga_feats,\n",
    "    'Importance': ranker.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "sorted_feats = imp_df['Feature'].tolist()\n",
    "\n",
    "steps = list(range(len(sorted_feats), 100, -20)) + list(range(100, 0, -5))\n",
    "results = []\n",
    "\n",
    "print(f\"\\nBáº¯t Ä‘áº§u vÃ²ng láº·p cáº¯t giáº£m features ({len(steps)} vÃ²ng)...\")\n",
    "\n",
    "for k in steps:\n",
    "    current_feats = sorted_feats[:k]\n",
    "    \n",
    "    model = LGBMRegressor(**best_params)\n",
    "    model.fit(X_train[current_feats], y_train)\n",
    "    \n",
    "    y_pred_log = model.predict(X_test[current_feats])\n",
    "    \n",
    "    y_pred_real = np.expm1(y_pred_log)\n",
    "    y_test_real = np.expm1(y_test)\n",
    "\n",
    "    r2 = r2_score(y_test_real, y_pred_real)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_real, y_pred_real))\n",
    "    mae = mean_absolute_error(y_test_real, y_pred_real)\n",
    "\n",
    "    print(f\"   -> DÃ¹ng {k:3d} features: R2 = {r2:.4f} | MAE = {mae:.2f} | RMSE = {rmse:.2f}\")\n",
    "    results.append({'Num_Features': k, 'R2': r2, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values(by='Num_Features')\n",
    "df_leaderboard = df_results.sort_values(by='R2', ascending=False).reset_index(drop=True)\n",
    "csv_filename = 'result/feature_selection_results.csv'\n",
    "df_results.to_csv(csv_filename, index=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_results['Num_Features'], df_results['R2'], marker='o', linewidth=2, color='blue')\n",
    "\n",
    "best_row = df_results.loc[df_results['R2'].idxmax()]\n",
    "plt.scatter(best_row['Num_Features'], best_row['R2'], color='red', s=150, zorder=5)\n",
    "plt.annotate(f\"Äá»‰nh: {best_row['R2']:.4f}\\n({int(best_row['Num_Features'])} feats)\", \n",
    "             (best_row['Num_Features'], best_row['R2']), \n",
    "             xytext=(best_row['Num_Features']+20, best_row['R2']-0.01),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "plt.title('Biá»ƒu Ä‘á»“ Elbow: Hiá»‡u quáº£ khi giáº£m dáº§n sá»‘ lÆ°á»£ng Features', fontsize=14)\n",
    "plt.xlabel('Sá»‘ lÆ°á»£ng Features', fontsize=12)\n",
    "plt.ylabel('Äá»™ chÃ­nh xÃ¡c (R2)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.gca().invert_xaxis()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBáº¢NG Xáº¾P Háº NG (SCORE GIáº¢M Dáº¦N):\")\n",
    "print(df_leaderboard[['R2', 'Num_Features', 'MAE', 'RMSE']].head(10))\n",
    "\n",
    "print(\"\\nBáº¢NG THEO THá»¨ Tá»° FEATURE (ÃT -> NHIá»€U):\")\n",
    "print(df_results[['Num_Features', 'R2', 'MAE', 'RMSE']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13a8c2",
   "metadata": {},
   "source": [
    "## Elbow RFECV Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ae099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- START RFE WITH LEADERBOARD ---\")\n",
    "start = time.time()\n",
    "\n",
    "model = LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=50,\n",
    "    max_depth=-1,\n",
    "    random_state=2601,\n",
    "    n_jobs=1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=model,\n",
    "    step=20,\n",
    "    cv=3,\n",
    "    scoring='r2', \n",
    "    min_features_to_select=50,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "r2_scores = rfecv.cv_results_['mean_test_score']\n",
    "n_scores = len(r2_scores)\n",
    "\n",
    "feature_counts = [50 + i * 20 for i in range(n_scores)]\n",
    "\n",
    "if len(feature_counts) > len(r2_scores):\n",
    "    feature_counts = feature_counts[:len(r2_scores)]\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "    'Num_Features': feature_counts,\n",
    "    'Score_R2': r2_scores\n",
    "})\n",
    "\n",
    "df_leaderboard = df_results.sort_values(by='Score_R2', ascending=False).reset_index(drop=True)\n",
    "\n",
    "selected_rfecv = X_train.columns[rfecv.support_]\n",
    "print(f\"\\nTime Run: {time.time() - start:.2f} s\")\n",
    "print(f\"Best Number of Features: {rfecv.n_features_}\")\n",
    "print(f\"Best CV R2 Score: {df_leaderboard.iloc[0]['Score_R2']:.4f}\")\n",
    "\n",
    "print(\"\\nLEADERBOARD (DESCENDING SCORE):\")\n",
    "print(df_leaderboard.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nPROGRESS (BY FEATURE COUNT):\")\n",
    "print(df_results.head(10).to_string(index=False))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_results['Num_Features'], df_results['Score_R2'], marker='o', color='green', linewidth=2)\n",
    "\n",
    "best_row = df_leaderboard.iloc[0]\n",
    "plt.scatter(best_row['Num_Features'], best_row['Score_R2'], color='red', s=150, zorder=5)\n",
    "plt.annotate(f\"Best: {best_row['Score_R2']:.4f}\\n({int(best_row['Num_Features'])} feats)\", \n",
    "             (best_row['Num_Features'], best_row['Score_R2']), \n",
    "             xytext=(best_row['Num_Features']+20, best_row['Score_R2']-0.005),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "plt.title('RFECV Performance Curve', fontsize=14)\n",
    "plt.xlabel('Number of Features Selected', fontsize=12)\n",
    "plt.ylabel('Cross Validation Score (R2)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSelected Features List:\")\n",
    "print(list(selected_rfecv))\n",
    "\n",
    "y_pred_log = rfecv.predict(X_test)\n",
    "y_pred_real = np.expm1(y_pred_log)\n",
    "y_test_real = np.expm1(y_test)\n",
    "\n",
    "test_r2 = r2_score(y_test_real, y_pred_real)\n",
    "test_mae = mean_absolute_error(y_test_real, y_pred_real)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_real, y_pred_real))\n",
    "\n",
    "print(\"\\n--- FINAL TEST EVALUATION (Best Features) ---\")\n",
    "print(f\"R2 Score : {test_r2:.4f}\")\n",
    "print(f\"MAE      : {test_mae:.4f} K\")\n",
    "print(f\"RMSE     : {test_rmse:.4f} K\")\n",
    "\n",
    "df_results.to_csv('result/rfecv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5728a31e",
   "metadata": {},
   "source": [
    "# Compare before and after using partial correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e18594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ULTRA_HIGH_CORR = 0.998\n",
    "\n",
    "print(\"Äang tÃ­nh toÃ¡n ma tráº­n tÆ°Æ¡ng quan...\")\n",
    "corr_matrix = X_train.corr().abs()\n",
    "\n",
    "ranker_temp = LGBMRegressor(n_estimators=100, verbose=-1, random_state=2601)\n",
    "ranker_temp.fit(X_train, y_train)\n",
    "importances = pd.Series(ranker_temp.feature_importances_, index=X_train.columns)\n",
    "\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = []\n",
    "\n",
    "for column in upper.columns:\n",
    "    correlated_cols = upper.index[upper[column] > ULTRA_HIGH_CORR].tolist()\n",
    "    \n",
    "    if correlated_cols:\n",
    "        for other_col in correlated_cols:\n",
    "            if other_col in to_drop: continue\n",
    "            \n",
    "            imp_col = importances.get(column, 0)\n",
    "            imp_other = importances.get(other_col, 0)\n",
    "            \n",
    "            if imp_col < imp_other:\n",
    "                to_drop.append(column)\n",
    "                break \n",
    "            else:\n",
    "                to_drop.append(other_col)\n",
    "\n",
    "to_drop = list(set(to_drop))\n",
    "print(f\"ÄÃ£ tÃ¬m tháº¥y {len(to_drop)} features trÃ¹ng láº·p (Corr > {ULTRA_HIGH_CORR})\")\n",
    "\n",
    "if len(to_drop) > 0:\n",
    "    feats_filtered = [f for f in X_train.columns if f not in to_drop]\n",
    "    \n",
    "    print(f\"Äang cháº¡y Ä‘Ã¡nh giÃ¡ láº¡i vá»›i {len(feats_filtered)} features...\")\n",
    "    res_filtered = get_metrics(f\"Filtered (Corr > {ULTRA_HIGH_CORR})\", feats_filtered)\n",
    "    \n",
    "    new_r2 = res_filtered['R2']\n",
    "    \n",
    "    old_r2 = df_res.loc[df_res['Method'] == 'Original', 'R2'].values[0]\n",
    "    \n",
    "    print(f\"\\nâœ… Káº¿t quáº£ R2 CÅ© (Original): {old_r2:.4f}\")\n",
    "    print(f\"âœ… Káº¿t quáº£ R2 Má»›i (Filtered): {new_r2:.4f}\")\n",
    "    \n",
    "    methods = ['Original', 'Filtered']\n",
    "    scores = [old_r2, new_r2]\n",
    "    colors = ['gray', 'green' if new_r2 >= old_r2 else 'red']\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    bars = plt.bar(methods, scores, color=colors, width=0.5)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{height:.4f}',\n",
    "                 ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "    plt.title(f'So sÃ¡nh R2: Giá»¯ nguyÃªn vs Lá»c TÆ°Æ¡ng quan (> {ULTRA_HIGH_CORR})', fontsize=14)\n",
    "    plt.ylabel('R2 Score (Real Scale)', fontsize=12)\n",
    "    plt.ylim(0, max(scores) + 0.1)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "    new_row = pd.DataFrame([res_filtered])\n",
    "    df_res = pd.concat([df_res, new_row], ignore_index=True)\n",
    "\n",
    "else:\n",
    "    print(\"âœ… KhÃ´ng cÃ³ features nÃ o quÃ¡ giá»‘ng nhau Ä‘á»ƒ xÃ³a.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a58d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('result/data/melting_point_features.csv')\n",
    "df = df.dropna(subset=['Tm'])\n",
    "\n",
    "df = df[(df['Tm'] > 0) & (df['Tm'] < 1000)].copy()\n",
    "\n",
    "print(f\"Data size after removing outliers (>1000K): {len(df)}\")\n",
    "\n",
    "y = df['Tm']\n",
    "X = df.drop(columns=['Tm']).select_dtypes(include=[np.number])\n",
    "\n",
    "y = y.reset_index(drop=True)\n",
    "X = X.reset_index(drop=True)\n",
    "\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_final = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_log, test_size=0.2, random_state=2601)\n",
    "\n",
    "corr_matrix = X_train.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.999)]\n",
    "\n",
    "if to_drop:\n",
    "    X_train = X_train.drop(columns=to_drop)\n",
    "    X_test = X_test.drop(columns=to_drop)\n",
    "    print(f\"Dropped {len(to_drop)} high correlation features\")\n",
    "\n",
    "voting_model = LGBMRegressor(\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=2000,\n",
    "    num_leaves=50,\n",
    "    max_depth=-1,\n",
    "    random_state=2601,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "voting_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_log = voting_model.predict(X_test)\n",
    "y_pred_real = np.expm1(y_pred_log)\n",
    "y_test_real = np.expm1(y_test)\n",
    "\n",
    "new_r2 = r2_score(y_test_real, y_pred_real)\n",
    "mae_real = mean_absolute_error(y_test_real, y_pred_real)\n",
    "rmse_real = np.sqrt(mean_squared_error(y_test_real, y_pred_real))\n",
    "\n",
    "print(\"\\n--- RESULT ---\")\n",
    "print(f\"R2 Score : {new_r2:.4f}\")\n",
    "print(f\"MAE      : {mae_real:.4f} K\")\n",
    "print(f\"RMSE     : {rmse_real:.4f} K\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(y_test_real, y_pred_real, alpha=0.4, color='blue', s=15, label='Data')\n",
    "\n",
    "max_val = max(y_test_real.max(), y_pred_real.max())\n",
    "min_val = min(y_test_real.min(), y_pred_real.min())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Fit')\n",
    "\n",
    "plt.title(f'Evaluation (Tm < 1000K)\\nR2 = {new_r2:.4f} | MAE = {mae_real:.2f} K', fontsize=14)\n",
    "plt.xlabel('Actual Tm (K)', fontsize=12)\n",
    "plt.ylabel('Predicted Tm (K)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b8e1ac",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
