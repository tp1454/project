{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a998bf0d",
   "metadata": {},
   "source": [
    "# BÁO CÁO KỸ THUẬT: TỐI ƯU HÓA MÔ HÌNH DỰ ĐOÁN NHIỆT ĐỘ NÓNG CHẢY\n",
    "\n",
    "**(Technical Report: Melting Point Prediction using 2D Descriptors & Advanced Feature Selection)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ed3d49",
   "metadata": {},
   "source": [
    "## 1. INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdef5ff",
   "metadata": {},
   "source": [
    "### 1.1. Data & Approach\n",
    "\n",
    "Trong dự án này, mình tập trung giải quyết bài toán dự đoán nhiệt độ nóng chảy (Tm) sử dụng các **đặc trưng cấu trúc 2D (2D Molecular Descriptors)**.\n",
    "\n",
    "* **Tại sao chọn 2D thay vì 3D?**\n",
    "Trong nghiên cứu QSPR về nhiệt độ nóng chảy, việc lựa chọn không gian đặc trưng (Descriptor Space) đóng vai trò quyết định. Mình ưu tiên sử dụng các Đặc trưng 2D (2D Descriptors) thay vì 3D dựa trên các cơ sở học thuật sau:\n",
    "\n",
    "Mặc dù  là tính chất vật lý phụ thuộc vào cấu trúc tinh thể 3D, nhưng việc sử dụng descriptors 3D gặp phải vấn đề lớn về sự linh hoạt cấu trúc (conformational flexibility) và độ nhiễu cao. Các nghiên cứu của **Tetko et al. (2001)** và **Hughes et al. (2008)** đã chứng minh rằng các mô hình dựa trên 2D Descriptors thường đạt độ chính xác tương đương hoặc tốt hơn 3D nhờ tính ổn định (robustness) và khả năng tổng quát hóa cao hơn.\n",
    "Do đó, nghiên cứu này tối ưu hóa triệt để thông tin từ dữ liệu 2D thông qua các kỹ thuật chọn lọc đặc trưng tiên tiến, thay vì phức tạp hóa vấn đề bằng dữ liệu 3D.\n",
    "\n",
    "Tính bất biến cấu hình (Conformational Invariance): Các đặc trưng 3D phụ thuộc lớn vào cấu trúc không gian của phân tử. Tuy nhiên, một phân tử có thể tồn tại ở hàng ngàn cấu hình (conformers) khác nhau và việc xác định chính xác cấu hình \"kết tinh\" (crystal packing conformation) là cực kỳ tốn kém và dễ sai số. Theo *Karthikeyan et al. (2005)*, việc sử dụng các đặc trưng 2D (như chỉ số topo, fingerprints) mang lại độ ổn định cao hơn và tránh được nhiễu do việc tối ưu hóa hình học 3D không chính xác.\n",
    "\n",
    "Hiệu suất tương đương: Các nghiên cứu tổng quan của *Dearden (2003)* chỉ ra rằng mặc dù nhiệt độ nóng chảy là một tính chất phụ thuộc vào mạng tinh thể (3D), nhưng các mô hình dựa trên đặc trưng 2D vẫn đạt được độ chính xác ngang bằng hoặc thậm chí tốt hơn do loại bỏ được nhiễu, đặc biệt là trong các bài toán sàng lọc quy mô lớn (High-throughput screening).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5250ba1f",
   "metadata": {},
   "source": [
    "### 1.2. Quy trình thực hiện\n",
    "\n",
    "Quy trình bao gồm 3 bước chính: (1) Tiền xử lý dữ liệu mạnh tay để loại bỏ nhiễu vô cơ, (2) So sánh các phương pháp và chọn ra phương pháp tối ưu nhất, và (3) Tinh chỉnh mô hình LightGBM với hàm mục tiêu L1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998155f9",
   "metadata": {},
   "source": [
    "## 2. PHÂN TÍCH CHI TIẾT QUY TRÌNH THỰC NGHIỆM (EDA CODE ANALYSIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9d5fb8",
   "metadata": {},
   "source": [
    "### 2.1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d25bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df_original = pd.read_csv('result/data/melting_point_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56af7ae0",
   "metadata": {},
   "source": [
    "1. **Nguồn dữ liệu (`pd.read_csv`)**:\n",
    "* **File:** `melting_point_features.csv`\n",
    "* **Vị trí:** Nằm trong thư mục `result/data/`.\n",
    "* **Ý nghĩa:** Đây là \"nguyên liệu đầu vào\" quan trọng nhất. File này chứa bảng dữ liệu gồm các đặc trưng hóa học (features) của các chất, dùng để dạy mô hình học cách dự đoán nhiệt độ nóng chảy.\n",
    "\n",
    "\n",
    "2. **Công cụ chủ lực (`LGBMRegressor`)**:\n",
    "* Code đã nạp sẵn thuật toán **LightGBM**. Đây là \"bộ não\" chính sẽ được sử dụng để xây dựng mô hình dự đoán sau này\n",
    "\n",
    "\n",
    "3. **Công cụ xử lý**:\n",
    "* `SimpleImputer`: Để tự động điền các ô dữ liệu bị khuyết (trống).\n",
    "* `train_test_split`: Để chia dữ liệu thành 2 phần: Học (Train) và Thi (Test)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eec637",
   "metadata": {},
   "source": [
    "### 2.2. PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b04d26a",
   "metadata": {},
   "source": [
    "#### 2.2.1. Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9632f10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['Tm'] > 0) & (df['Tm'] < 4000)]\n",
    "\n",
    "y = df_clean['Tm']\n",
    "X = df_clean.drop(columns=['Tm']).select_dtypes(include=[np.number])\n",
    "\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_final = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, \n",
    "    y_log, \n",
    "    test_size=0.2, \n",
    "    random_state=2601\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99057ed",
   "metadata": {},
   "source": [
    "1. Quy trình xử lý (Algorithmic Pipeline)\n",
    "\n",
    "**- Target Filtering:** Loại bỏ `NaN`  Lọc miền giá trị hợp lệ $(0 < T_m < 4000)$.\n",
    "\n",
    "**- Feature Preprocessing:**\n",
    "* Selection: Chỉ chọn dữ liệu số (`numerical`).\n",
    "* Sanitization: `Inf`-> `NaN`.\n",
    "* Imputation: Điền khuyết bằng **Trung vị (Median)**.\n",
    "\n",
    "\n",
    "**- Target Transformation:** $y_{new} = \\ln(1 + y)$ (Log-transformation để giảm độ lệch phân phối).\n",
    "\n",
    "**- Data Splitting:** Tỷ lệ **80/20** (Train/Test) với `random_state=2601`.\n",
    "\n",
    "2. Thống kê dữ liệu (Data Statistics)\n",
    "\n",
    "* **Input:** ~10,520 mẫu (ước tính từ số lượng bị loại).\n",
    "* **Cleaned:** **8,766 mẫu** (đã loại bỏ 1,754 mẫu nhiễu/outliers).\n",
    "* **Dimensionality:** **937 đặc trưng** (features).\n",
    "* **Shape:**\n",
    "* Train Set: $7012 \\times 937$\n",
    "* Test Set: $1754 \\times 937$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17761f3a",
   "metadata": {},
   "source": [
    "#### 2.2.2. Compare before and after using partial correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c96a8",
   "metadata": {},
   "source": [
    "Trong các bộ dữ liệu hóa học (QSPR/QSAR), hiện tượng đa cộng tuyến (multicollinearity) thường xuyên xảy ra khi nhiều descriptors cùng mô tả một đặc tính cấu trúc tương tự nhau (ví dụ: MolWt và HeavyAtomMolWt). Điều này gây lãng phí tài nguyên tính toán và có thể làm nhiễu mô hình.\n",
    "\n",
    "* ***Mình đã áp dụng quy trình lọc thông minh hai bước:***\n",
    "    - Phát hiện cặp đôi hoàn hảo: Tính toán ma trận tương quan Pearson cho toàn bộ 937 đặc trưng và xác định các cặp có hệ số tương quan cực cao ($|r| > 0.998$).\n",
    "    - Trọng tài thông minh (Smart Arbitration): Thay vì xóa ngẫu nhiên 1 trong 2, mình sử dụng mô hình LightGBM (\"ranker_temp\") để chấm điểm tầm quan trọng (Feature Importance) của từng feature. Trong mỗi cặp trùng lặp, feature có điểm thấp hơn sẽ bị loại bỏ, giữ lại feature mang nhiều thông tin dự báo hơn.\n",
    "\n",
    "* ***Kết quả thực nghiệm:***\n",
    "    - Phát hiện: Thuật toán đã tìm ra và loại bỏ 8 đặc trưng bị trùng lặp thông tin gần như tuyệt đối (Corr > 0.998).\n",
    "    - Tác động đến hiệu suất:\n",
    "        Mô hình Gốc (Original): $R^2 = 0.6563$.\n",
    "        Mô hình sau lọc (Filtered): $R^2 = 0.6612$.\n",
    "\n",
    "        * **Cải thiện**: Độ chính xác tăng thêm ~0.005. Mặc dù con số tuyệt đối nhỏ, nhưng việc tăng $R^2$ đồng thời với việc giảm số lượng biến số là một tín hiệu rất tích cực.\n",
    "\n",
    "* ***Ý nghĩa khoa học:***\n",
    "\n",
    "    Kết quả này khẳng định rằng \"ít hơn là nhiều hơn\" (Less is More). Việc loại bỏ các biến số dư thừa không chỉ giúp mô hình nhẹ hơn mà còn giúp nó tập trung học các quy luật thực sự thay vì bị phân tâm bởi các tín hiệu lặp lại. Đây là bước chuẩn bị nền tảng quan trọng giúp các thuật toán chọn lọc phía sau (như GA hay RFECV) hoạt động hiệu quả hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85899449",
   "metadata": {},
   "source": [
    "#### 2.2.3. Scatter Plot: Phân tích Kết quả Thực nghiệm Biểu Đồ Phân Tán\n",
    "\n",
    "**1. Kết quả Định lượng:**\n",
    "Sau các bước làm sạch và chuẩn hóa dữ liệu, mô hình LightGBM khởi tạo trên toàn bộ tập đặc trưng (chưa qua chọn lọc chuyên sâu) ghi nhận kết quả:\n",
    "\n",
    "* Độ chính xác: Hệ số xác định $R^2 \\approx 0.6194$, giải thích được khoảng 62% biến thiên của dữ liệu.\n",
    "\n",
    "* Sai số: Sai số tuyệt đối trung bình (MAE) đạt 64.7 K, mức chấp nhận được cho mô hình tham chiếu ban đầu.\n",
    "\n",
    "**2. Đánh giá Nguyên nhân Biến động:**\n",
    "So với thử nghiệm trên tập dữ liệu toàn phần (bao gồm cả chất vô cơ/muối), các chỉ số này phản ánh chính xác hiệu năng trên nhóm hợp chất hữu cơ:\n",
    "\n",
    "* **Tính ổn định:** Dữ liệu tập trung mật độ cao quanh đường chéo lý tưởng ($y=x$) trong dải nhiệt độ phổ biến ($300-600K$), chứng tỏ mô hình không bị các lỗi hệ thống (systematic bias) nghiêm trọng.\n",
    "* **Giới hạn cấu trúc:** Tại vùng nhiệt độ cao ($>600K$), xuất hiện hiện tượng phương sai không đồng nhất (Heteroscedasticity) - đám mây dữ liệu loe rộng, phản ánh khó khăn của các descriptors 2D trong việc mô tả các tương tác liên phân tử phức tạp ở nhiệt độ cao.\n",
    "\n",
    "**Kết luận:** Đây là mức hiệu năng cơ sở (Honest Baseline) tin cậy cho phạm vi nghiên cứu ($0 < T_m < 1000K$). Kết quả này xác nhận dữ liệu đã đủ tiêu chuẩn để tiến hành các bước tối ưu hóa chọn lọc đặc trưng (như RFE và GA) nhằm nâng cao độ chính xác vượt ngưỡng 62%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bfeaca",
   "metadata": {},
   "source": [
    "### 2.3. MODEL TRAINING: FEATURES SELECTION USING RFECV, GA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708c84ea",
   "metadata": {},
   "source": [
    "Trong bối cảnh bài toán Hóa học tính toán (Chemoinformatics) với không gian đặc trưng ban đầu lớn (937 descriptors), việc áp dụng các kỹ thuật giảm chiều dữ liệu là bắt buộc để giải quyết vấn đề Curse of Dimensionality và giảm thiểu hiện tượng quá khớp (Overfitting). Mình sử dụng tiếp cận lai (Hybrid approach) kết hợp hai thuật toán: RFECV (đại diện cho phương pháp Wrapper - Greedy) và Genetic Algorithm (đại diện cho phương pháp Stochastic/Heuristic)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0672de",
   "metadata": {},
   "source": [
    "#### 1. **RECURSIVE FEATURE ELIMINATION WITH CROSS-VALIDATION (RFECV):**\n",
    "\n",
    "1.1. Nguồn gốc và Định nghĩa\n",
    "\n",
    "**RFECV (Loại bỏ đặc trưng đệ quy có kiểm định chéo)** Để giải quyết vấn đề số chiều lớn (High-dimensionality), mình áp dụng thuật toán Loại bỏ đặc trưng đệ quy tích hợp kiểm định chéo (RFECV). Đây là phương pháp được *Guyon et al. (2002)* đề xuất và được chứng minh là một trong những kỹ thuật mạnh mẽ nhất để loại bỏ các biến dư thừa (redundant features) mà không làm mất thông tin quan trọng.\n",
    "\n",
    "Khác với các phương pháp lọc đơn giản (Filter methods), RFECV tương tác trực tiếp với mô hình (Wrapper method) để đánh giá tập hợp biến. Nghiên cứu của *Granitto et al. (2006)* trong lĩnh vực hóa trắc lượng (Chemometrics) cũng khẳng định rằng RFECV giúp giảm thiểu hiện tượng quá khớp (overfitting) và tăng khả năng tổng quát hóa của mô hình tốt hơn so với việc chỉ sử dụng Genetic Algorithm đơn lẻ.\n",
    "\n",
    "1.2. Cơ chế hoạt động (Mechanism)\n",
    "\n",
    "RFECV hoạt động dựa trên nguyên lý **loại bỏ lùi (Backward Elimination)** kết hợp với đánh giá chéo (Cross-Validation) để tìm số lượng đặc trưng tối ưu (). Quy trình cụ thể như sau:\n",
    "\n",
    "1. **Huấn luyện khởi tạo:** Mô hình (trong trường hợp này là LightGBM) được huấn luyện trên toàn bộ tập đặc trưng ban đầu ().\n",
    "2. **Xếp hạng (Ranking):** Thuật toán trích xuất tầm quan trọng của từng đặc trưng (feature importance) thông qua thuộc tính `feature_importances_` (dựa trên Gain hoặc Split trong cây quyết định).\n",
    "3. **Loại bỏ (Pruning):** Các đặc trưng có trọng số thấp nhất sẽ bị loại bỏ khỏi tập dữ liệu.\n",
    "4. **Lặp lại (Iteration):** Quá trình 1-3 được lặp lại cho đến khi tập đặc trưng rỗng.\n",
    "5. **Kiểm định chéo (Cross-Validation):** Tại mỗi bước lặp, thuật toán ghi nhận điểm số (Scoring metric, ví dụ: RMSE hoặc R2) trên tập validation. Số lượng đặc trưng tối ưu () được xác định tại điểm mà sai số mô hình là thấp nhất.\n",
    "\n",
    "1.3. Tại sao sử dụng RFECV?\n",
    "\n",
    "* **Tính ổn định:** Việc tích hợp Cross-Validation (CV) giúp RFECV không bị chệch (bias) bởi một tập dữ liệu cụ thể, đảm bảo tập đặc trưng được chọn có khả năng tổng quát hóa cao.\n",
    "* **Giải quyết Đa cộng tuyến:** RFECV có khả năng loại bỏ các đặc trưng dư thừa (redundant) có độ tương quan cao, giữ lại đặc trưng đại diện tốt nhất cho mô hình.\n",
    "\n",
    "* *Mục tiêu:* Loại bỏ các features 2D bị cộng tuyến (collinear) hoặc nhiễu.\n",
    "* *Kết quả:* Giữ lại 537 features có tính đại diện cao nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af66722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rfe = LGBMRegressor(\n",
    "    objective='regression',\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    random_state=2601,\n",
    "    n_jobs=-1, verbose=-1\n",
    ")\n",
    "\n",
    "rfe = RFECV(\n",
    "    estimator=model_rfe,\n",
    "    min_features_to_select=50,\n",
    "    step=20,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "eval_model = LGBMRegressor(\n",
    "    n_estimators=3000, learning_rate=0.01, num_leaves=50, max_depth=-1,\n",
    "    subsample=0.8, colsample_bytree=0.7, random_state=2601, n_jobs=-1, verbose=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40e70fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Time Run: 334.77 s\n",
    "RFECV Selected: 537 features\n",
    "========================================\n",
    "KẾT QUẢ RFECV (REAL SCALE)\n",
    "Features : 537\n",
    "MAE      : 83.3528 K\n",
    "RMSE     : 193.6333 K\n",
    "R2       : 0.5936\n",
    "========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57274f6c",
   "metadata": {},
   "source": [
    "***1. Thiết lập Thuật toán và Cắt tỉa (Algorithm Configuration):***\n",
    "\n",
    "Đoạn mã thiết lập một quy trình sàng lọc đặc trưng hai giai đoạn, tách biệt giữa mô hình dùng để chọn lọc (Selector) và mô hình dùng để đánh giá (Evaluator).\n",
    "\n",
    "* **Base Estimator:**\n",
    "Sử dụng `LGBMRegressor` với cấu hình thiên về tốc độ (`n_estimators=500`, `learning_rate=0.05`). Mục đích của mô hình này không phải để dự đoán chính xác nhất, mà là để xếp hạng độ quan trọng của các đặc trưng (feature importance) một cách nhanh chóng.\n",
    "* **Chiến lược RFECV:**\n",
    "* **Step Size (`step=20`):** Thay vì loại bỏ từng đặc trưng đơn lẻ, thuật toán loại bỏ 20 đặc trưng yếu nhất sau mỗi vòng lặp. Đây là chiến lược **\"Aggressive Pruning\" (Cắt tỉa mạnh)**, giúp giảm thiểu đáng kể chi phí tính toán trên không gian đặc trưng lớn (937 chiều) mà ít ảnh hưởng đến độ hội tụ của tập đặc trưng tối ưu.\n",
    "* **Cross-Validation (`cv=3`):** Sử dụng kiểm định chéo 3-fold để đảm bảo tính ổn định của tập đặc trưng được chọn, tránh hiện tượng quá khớp (overfitting) vào một tập dữ liệu cụ thể.\n",
    "* **Scoring Metric (`neg_mean_absolute_error`):** Tối ưu hóa trực tiếp trên sai số tuyệt đối (MAE), phù hợp với mục tiêu giảm thiểu độ lệch nhiệt độ (Kelvin).\n",
    "\n",
    "***2. Hiệu suất Giảm chiều Dữ liệu (Dimensionality Reduction Efficiency):***\n",
    "\n",
    "Kết quả đầu ra phản ánh hiệu quả của quá trình giảm chiều:\n",
    "\n",
    "* **Thời gian thực thi:** 334.77 giây (~5.5 phút). Đây là thời gian xử lý tối ưu cho việc huấn luyện lặp lại trên gần 1000 đặc trưng, nhờ vào việc tận dụng đa luồng (`n_jobs=-1`) và bước nhảy lớn (`step=20`).\n",
    "* **Tỷ lệ nén đặc trưng:**\n",
    "* Số lượng ban đầu: **937**\n",
    "* Số lượng sau chọn lọc: **537**\n",
    "* **Kết luận:** Thuật toán đã loại bỏ **400 đặc trưng** (tương đương ~42.7% không gian dữ liệu). Điều này chỉ ra rằng một phần lớn các descriptors ban đầu là nhiễu (noise) hoặc dư thừa (redundant).\n",
    "\n",
    "***3. Đánh giá Hiệu năng Sơ bộ (Preliminary Performance Evaluation):***\n",
    "\n",
    "Sau khi chọn lọc, một mô hình đánh giá độc lập được xây dựng với cấu hình phức tạp hơn (`n_estimators=3000`, `num_leaves=50`) để kiểm tra chất lượng tập đặc trưng mới:\n",
    "\n",
    "* **Metrics (Real Scale):**\n",
    "* : Mô hình giải thích được khoảng 59% phương sai của dữ liệu.\n",
    "* : Sai số trung bình tuyệt đối nằm trong khoảng chấp nhận được cho bước sàng lọc thô.\n",
    "* : Giá trị này còn cao, cho thấy mô hình ở giai đoạn này vẫn chịu ảnh hưởng bởi các giá trị ngoại lai (do hàm mục tiêu mặc định là L2 Regression).\n",
    "\n",
    "**Tổng kết:** Bước RFECV này đóng vai trò là \"bộ lọc thô\" (Coarse Filter), thành công trong việc giảm gần một nửa số chiều dữ liệu mà vẫn duy trì được độ chính xác cơ bản  ($R^2 \\approx 0.6$), tạo tiền đề cho các bước tinh chỉnh sâu hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9334f0d7",
   "metadata": {},
   "source": [
    "#### 2. **Genetic Algorithm (GA):**\n",
    "\n",
    "2.1. Nguồn gốc và Định nghĩa\n",
    "\n",
    "**Genetic Algorithm (GA)** là một kỹ thuật tối ưu hóa ngẫu nhiên (stochastic optimization) dựa trên nguyên lý chọn lọc tự nhiên và di truyền học của Darwin, được **John Holland** giới thiệu lần đầu vào năm 1975 trong tác phẩm *\"Adaptation in Natural and Artificial Systems\"*. Trong Hóa học, GA được xem là phương pháp tiêu chuẩn để giải quyết các bài toán QSAR/QSPR phức tạp (**Leardi, 2001**).\n",
    "\n",
    "2.2. Cơ chế hoạt động (Mechanism)\n",
    "\n",
    "GA coi mỗi tập con đặc trưng là một \"cá thể\" (individual) và tập hợp các cá thể là một \"quần thể\" (population). Quá trình tiến hóa diễn ra qua các bước:\n",
    "\n",
    "1. **Khởi tạo (Initialization):** Tạo ngẫu nhiên một quần thể các chuỗi nhị phân (binary strings/chromosomes), trong đó bit 1 đại diện cho việc chọn đặc trưng, bit 0 là không chọn.\n",
    "2. **Đánh giá (Fitness Evaluation):** Mỗi cá thể được đánh giá bằng một hàm mục tiêu (Fitness function) – ở đây là giá trị RMSE của mô hình LightGBM khi sử dụng tập đặc trưng tương ứng.\n",
    "3. **Chọn lọc (Selection):** Các cá thể có độ thích nghi tốt (RMSE thấp) được giữ lại để làm cha mẹ cho thế hệ sau (ví dụ: phương pháp Tournament Selection).\n",
    "4. **Lai ghép (Crossover):** Trao đổi thông tin di truyền giữa các cặp cha mẹ để tạo ra cá thể con mới, nhằm kết hợp các đặc trưng tốt từ cả hai.\n",
    "5. **Đột biến (Mutation):** Đảo ngẫu nhiên một số bit (0 thành 1 hoặc ngược lại) với xác suất thấp. Bước này cực kỳ quan trọng để duy trì sự đa dạng di truyền, giúp thuật toán thoát khỏi các điểm tối ưu cục bộ (Local Optima).\n",
    "\n",
    "2.3. Tại sao sử dụng GA?\n",
    "\n",
    "* **Tìm kiếm toàn cục (Global Search):** Khác với RFECV (vốn có tính chất tham lam - greedy), GA có khả năng duyệt không gian tìm kiếm rộng lớn và tìm ra nghiệm tối ưu toàn cục.\n",
    "* **Phát hiện tương tác phi tuyến:** GA cực kỳ hiệu quả trong việc tìm ra các tổ hợp đặc trưng (synergistic features) mà khi đứng riêng lẻ thì không quan trọng, nhưng khi kết hợp lại thì cho hiệu quả dự đoán cao.\n",
    "\n",
    "* *Mục tiêu:* Tìm kiếm các tổ hợp features phi tuyến tính.\n",
    "* *Kết quả:* Giữ lại 473 features, phát hiện được các tương tác ngầm giữa các nhóm chức hóa học mà RFE bỏ sót."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb45d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ga = LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31, \n",
    "    ax_depth=-1,\n",
    "    random_state=2601,\n",
    "    n_jobs=1, verbose=-1\n",
    ")\n",
    "\n",
    "ga = GAFeatureSelectionCV(\n",
    "    estimator=model_ga,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    population_size=50,\n",
    "    generations=15,\n",
    "    mutation_probability=0.1,\n",
    "    crossover_probability=0.8,\n",
    "    keep_top_k=2,\n",
    "    elitism=True,\n",
    "    n_jobs=-1, verbose=True\n",
    ")\n",
    "\n",
    "eval_model = LGBMRegressor(\n",
    "    n_estimators=3000, learning_rate=0.01, num_leaves=50, max_depth=-1,\n",
    "    subsample=0.8, colsample_bytree=0.7, random_state=2601, n_jobs=-1, verbose=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa57abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "GA Chosen 473 features\n",
    "--- RESULT GA (REAL SCALE) ---\n",
    "Features: 473\n",
    "MAE     : 83.3353 K\n",
    "RMSE    : 193.0514 K\n",
    "R2      : 0.5960"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e131ffd8",
   "metadata": {},
   "source": [
    "***1. Thiết lập Thuật toán và Cơ chế Di truyền (Algorithm Configuration):***\n",
    "\n",
    "Khác với RFECV (tham lam & đơn hướng), đoạn mã này thiết lập một quy trình tìm kiếm ngẫu nhiên có định hướng (Stochastic Search) để khám phá không gian đặc trưng.\n",
    "\n",
    "* **Cấu hình Quần thể (Population Dynamics):**\n",
    "* `population_size=50`: Khởi tạo 50 \"cá thể\" (mỗi cá thể là một tổ hợp đặc trưng khác nhau) trong mỗi thế hệ.\n",
    "* `generations=15`: Quá trình tiến hóa diễn ra qua 15 bước. Đây là con số khiêm tốn (để tiết kiệm thời gian) nhưng đủ để quan sát sự hội tụ.\n",
    "\n",
    "\n",
    "* **Toán tử Di truyền (Genetic Operators):**\n",
    "* `crossover_probability=0.8`: Xác suất lai ghép cao (80%) khuyến khích việc trao đổi thông tin giữa các tập đặc trưng tốt, giúp tìm ra các tổ hợp \"cộng hưởng\".\n",
    "* `mutation_probability=0.1`: Xác suất đột biến thấp (10%) giúp duy trì sự đa dạng di truyền, tránh rơi vào bẫy tối ưu cục bộ (Local Minima).\n",
    "* `elitism=True`: Chiến thuật \"Tinh hoa\", đảm bảo các cá thể tốt nhất của thế hệ trước luôn được bảo tồn sang thế hệ sau (không bị mất đi do lai ghép ngẫu nhiên).\n",
    "\n",
    "\n",
    "* **Hàm Mục tiêu (Fitness Function):**\n",
    "* Tối ưu hóa trên `neg_mean_absolute_error`. Lưu ý rằng giá trị Fitness trong log (ví dụ `-0.305`) là trên thang đo Logarit (do `y_train` đã log-transform).\n",
    "\n",
    "\n",
    "\n",
    "***2. Phân tích Quá trình Hội tụ (Convergence Analysis):***\n",
    "\n",
    "Bảng log kết quả (`gen 0` đến `gen 15`) cho thấy bức tranh rõ nét về sự tiến hóa:\n",
    "\n",
    "* **Sự cải thiện Fitness:**\n",
    "* Gen 0 (Max Fitness): **-0.3106**\n",
    "* Gen 15 (Max Fitness): **-0.3058**\n",
    "* **Nhận xét:** Fitness tăng đều đặn qua từng thế hệ. Mặc dù mức tăng tuyệt đối nhỏ (trên thang log), nhưng nó cho thấy thuật toán đang \"học\" cách chọn features tốt hơn sau mỗi vòng lặp.\n",
    "\n",
    "\n",
    "* **Độ lệch chuẩn (Fitness Std):** Giảm từ `0.0016` xuống `0.0006`. Điều này chỉ ra rằng quần thể đang **hội tụ** (converging) về một giải pháp tối ưu, các cá thể ngày càng giống nhau và tốt đều hơn.\n",
    "\n",
    "***3. Hiệu suất Tính toán & Kết quả (Computational Efficiency & Performance):***\n",
    "\n",
    "* **Thời gian thực thi:** **1326.69 giây (~22 phút)**.\n",
    "* **So sánh:** Chậm hơn gấp 4 lần so với RFECV (5.5 phút). Điều này dễ hiểu vì GA phải huấn luyện hàng nghìn mô hình (50 cá thể x 15 thế hệ x 3 CV = ~2250 lần fit).\n",
    "\n",
    "\n",
    "* **Tỷ lệ nén đặc trưng:**\n",
    "* Số lượng chọn: **473 features** (ít hơn RFECV - 537 features).\n",
    "* **Kết luận:** GA tìm ra một tập hợp gọn nhẹ hơn nhưng hiệu quả hơn.\n",
    "\n",
    "\n",
    "* **Đánh giá Hiệu năng (Real Scale):**\n",
    "* $R^2 = 0.5960$: Nhỉnh hơn RFECV ($0.5936$).\n",
    "* $RMSE \\approx 193.05 K$: Thấp hơn RFECV ($193.63 K$).\n",
    "\n",
    "\n",
    "\n",
    "**Tổng kết:** Mặc dù tốn kém thời gian tính toán hơn nhiều, GA đã chứng minh được giá trị của mình bằng cách tìm ra một tập đặc trưng **nhỏ hơn (473 vs 537)** nhưng lại cho độ chính xác **cao hơn**. Điều này gợi ý rằng GA đã thành công trong việc tìm ra các tương tác phi tuyến giữa các features mà phương pháp tham lam như RFECV đã bỏ sót."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8a9514",
   "metadata": {},
   "source": [
    "\n",
    "#### 3. **SO SÁNH: RFECV vs. GA**\n",
    "\n",
    "Dưới đây là bảng so sánh học thuật sự khác biệt cốt lõi giữa hai phương pháp này, giải thích lý do tại sao chiến lược kết hợp (Intersection) lại hiệu quả.\n",
    "\n",
    "| Tiêu chí so sánh | RFECV (Recursive Feature Elimination) | Genetic Algorithm (GA) |\n",
    "| --- | --- | --- |\n",
    "| **Chiến lược tìm kiếm** | **Deterministic & Greedy** (Định hãm & Tham lam). Tại mỗi bước, nó luôn đưa ra quyết định tốt nhất cục bộ (loại bỏ cái tệ nhất). | **Stochastic & Evolutionary** (Ngẫu nhiên & Tiến hóa). Dựa trên xác suất để tìm kiếm không gian giải pháp rộng hơn. |\n",
    "| **Hướng tiếp cận** | **Backward Elimination** (Loại bỏ lùi). Bắt đầu với tất cả đặc trưng và cắt giảm dần. | **Random/Heuristic Search**. Có thể bắt đầu ngẫu nhiên và tiến hóa theo cả hai hướng (thêm vào hoặc bớt đi). |\n",
    "| **Khả năng tối ưu** | Dễ bị mắc kẹt tại **Tối ưu cục bộ (Local Optima)**. Nếu bước đầu loại bỏ nhầm một đặc trưng quan trọng (do nhiễu), nó không thể khôi phục lại. | Có khả năng thoát khỏi tối ưu cục bộ nhờ cơ chế **Đột biến (Mutation)**, hướng tới **Tối ưu toàn cục (Global Optima)**. |\n",
    "| **Tương tác biến** | Tốt trong việc xác định các đặc trưng có tác động đơn lẻ mạnh (Main effects). | Xuất sắc trong việc phát hiện các **tương tác phức tạp** giữa các nhóm đặc trưng (Interaction effects). |\n",
    "| **Chi phí tính toán** | Thấp hơn, hội tụ nhanh hơn. | Rất cao, tốn kém tài nguyên tính toán do phải huấn luyện hàng nghìn mô hình qua các thế hệ. |\n",
    "\n",
    "\n",
    "Trong nghiên cứu này, mình không chọn riêng lẻ một phương pháp mà sử dụng 2 phương pháp **Union (Hợp nhất)** và **Intersection (Giao thoa)**:\n",
    "\n",
    "Việc này tận dụng điểm mạnh của cả hai:\n",
    "\n",
    "1. **RFECV** đảm bảo các đặc trưng được chọn có đóng góp thực sự về mặt thống kê (loại bỏ nhiễu).\n",
    "2. **GA** đảm bảo các đặc trưng được chọn có sự tương tác tốt với nhau để mô hình hóa các mối quan hệ phi tuyến trong dữ liệu hóa học."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca5aff7",
   "metadata": {},
   "source": [
    "### 2.4. UNION AND INTERSECTION 2 MODEL TRAINNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d068cea",
   "metadata": {},
   "source": [
    "**Giới thiệu sơ bộ về 2 phương pháp (Union và Intersection)**\n",
    "\n",
    "Để khai thác tối đa sức mạnh của cả RFECV (thống kê) và GA (tiến hóa), mình không chọn riêng lẻ một thuật toán mà áp dụng kỹ thuật Ensemble Feature Selection thông qua hai chiến lược:\n",
    "\n",
    "**1. Phương pháp UNION (HỢP - $\\cup$)**\n",
    "\n",
    "Cơ chế: Kết hợp tất cả các đặc trưng được ít nhất một thuật toán (RFECV hoặc GA) lựa chọn.\n",
    "\n",
    "$$S_{Union} = S_{RFECV} \\cup S_{GA}$$\n",
    "\n",
    "Mục tiêu: Tối đa hóa thông tin đầu vào, đảm bảo không bỏ sót bất kỳ đặc trưng tiềm năng nào mà một trong hai thuật toán có thể đã bỏ qua.\n",
    "\n",
    "Đặc điểm: Tập đặc trưng lớn (727 features), giàu thông tin nhưng có rủi ro nhiễu cao.\n",
    "\n",
    "**2. Phương pháp INTERSECTION (GIAO - $\\cap$)**\n",
    "\n",
    "Cơ chế: Chỉ giữ lại những đặc trưng được CẢ HAI thuật toán đồng thuận lựa chọn.\n",
    "\n",
    "$$S_{Intersect} = S_{RFECV} \\cap S_{GA}$$\n",
    "\n",
    "Mục tiêu: Lọc nhiễu triệt để, chỉ giữ lại các đặc trưng \"cốt lõi\" (Robust Features) có độ tin cậy cao nhất về mặt thống kê và tương tác hóa học.\n",
    "\n",
    "Đặc điểm: Tập đặc trưng gọn nhẹ (283 features), giảm thiểu Overfitting và tăng tốc độ mô hình."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed61660",
   "metadata": {},
   "source": [
    "#### 2.4.1. Features Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298655a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_features_uni = set(rfe_features) | set(ga_features)\n",
    "\n",
    "Tổng features sau khi hợp (Union): 727"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08be436",
   "metadata": {},
   "source": [
    "Đọc danh sách features từ 2 file rfe_features.pkl (kết quả của RFE) và ga_features.pkl (kết quả của GA).\n",
    "\n",
    "Toán tử Hợp (|): `Dòng set(rfe_features) | set(ga_features)` sẽ lấy tất cả các features xuất hiện ở ít nhất 1 trong 2 danh sách.\n",
    "\n",
    "Kết quả: Biến `common_features_uni` sẽ chứa một danh sách features (727 mẫu), bao gồm cả những cái mà RFE thấy quan trọng và những cái mà GA thấy quan trọng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f61c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_uni = list(common_features_uni)\n",
    "valid_features_uni = [f for f in features_uni if f in X_final.columns]\n",
    "\n",
    "manual_params = {\n",
    "    'n_estimators': 8000,\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 40,\n",
    "    'max_depth': 15,\n",
    "    'min_child_samples': 20,\n",
    "    \n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.6,\n",
    "    \n",
    "    'reg_alpha': 0.2,\n",
    "    'reg_lambda': 0.2,\n",
    "    \n",
    "    'random_state': 2601,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "model_uni = LGBMRegressor(**manual_params)\n",
    "model_uni.fit(X_train[valid_features_uni], y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2763bd03",
   "metadata": {},
   "source": [
    "#### **HUẤN LUYỆN MÔ HÌNH VỚI UNION**\n",
    "\n",
    "1. Mục tiêu (Objective)\n",
    "\n",
    "Xây dựng một mô hình LightGBM sử dụng tập đặc trưng rộng lớn (khoảng 727 features) thu được từ phép hợp của RFECV và GA. Mục đích là để kiểm thử xem việc giữ lại nhiều thông tin có giúp mô hình dự đoán tốt hơn không.\n",
    "\n",
    "2. Các bước thực hiện chính\n",
    "\n",
    "**Chuẩn bị dữ liệu đặc trưng:**\n",
    "\n",
    "* Tập hợp đặc trưng: Sử dụng danh sách `common_features_uni` (kết quả hợp nhất từ RFECV và GA).\n",
    "\n",
    "* Kiểm tra hợp lệ: Lọc lại danh sách `valid_features_uni` kiểm tra và chỉ giữ lại các features trong danh sách Union mà thực sự tồn tại trong DataFrame `X_final`. Đây là bước kiểm tra an toàn (safety check), tránh lỗi kỹ thuật.\n",
    "\n",
    "\n",
    "**Thiết lập tham số thủ công (`manual_params`):**\n",
    "* Thay vì để mặc định, code thiết lập một bộ tham số \"thận trọng\" để kiểm soát độ phức tạp của tập đặc trưng lớn (hơn 700 features).\n",
    "\n",
    "\n",
    "**Huấn luyện (`fit`):**\n",
    "* `model_uni.fit(...)`: Mô hình học mối quan hệ giữa các features Union và nhiệt độ nóng chảy.\n",
    "\n",
    "\n",
    "**Lưu trữ (`joblib.dump`):**\n",
    "* Xuất mô hình đã học (`melting_point_model_uni.pkl`) và danh sách features (`features_list_uni.pkl`) ra file. Điều này cực kỳ quan trọng để sau này có thể tải lại mô hình và dự đoán cho chất mới mà không cần train lại từ đầu.\n",
    "\n",
    "3. Cấu hình Mô hình LightGBM\n",
    "\n",
    "Chiến thuật được sử dụng ở đây là **\"Học chậm và chắc\"**:\n",
    "\n",
    "* **`learning_rate`: 0.01** (Rất nhỏ): Giúp mô hình học từ từ, chi tiết, tránh bị vọt qua điểm tối ưu.\n",
    "* **`n_estimators`: 8000** (Rất lớn): Vì học chậm nên cần số lượng cây (trees) lớn để đủ thời gian hội tụ.\n",
    "* **`num_leaves`: 40** & **`max_depth`: 15**: Giới hạn độ phức tạp của cây để tránh Overfitting (học vẹt), vì tập features Union vốn đã có nhiều nhiễu.\n",
    "* **`colsample_bytree`: 0.6** & **`subsample`: 0.7**: Kỹ thuật Bagging. Chỉ cho phép mỗi cây nhìn thấy 60% số features và 70% số dữ liệu ngẫu nhiên. Điều này làm tăng tính đa dạng và giảm phương sai của mô hình.\n",
    "* **`objective`: 'regression'**: Tại bước này, mô hình vẫn đang dùng hàm mất mát chuẩn (L2/MSE) để tối ưu hóa.\n",
    "\n",
    "**Tóm lại:** Phần code này thực hiện hóa chiến lược **\"Lấy số lượng bù chất lượng\"** (Union), sử dụng một cấu hình LightGBM được tinh chỉnh để cố gắng kiểm soát lượng thông tin khổng lồ và hỗn tạp từ tập features đã hợp nhất."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d0e4d6",
   "metadata": {},
   "source": [
    "#### **SCORING ĐÁNH GIÁ ĐIỂM TỪ PHƯƠNG PHÁP UNION**\n",
    "\n",
    "Phương pháp Kiểm thử (Evaluation Methodology)\n",
    "\n",
    "Quá trình đánh giá được thực hiện trên tập kiểm thử độc lập ($N_{test} = 1713$ mẫu) để đảm bảo tính khách quan. Quy trình bao gồm ba bước xử lý hậu kỳ (post-processing) quan trọng:\n",
    "\n",
    "1. **Khôi phục Mô hình (Model Restoration):** Tải trọng số mô hình LightGBM và danh sách đặc trưng hợp nhất ($S_{Union}$) từ bộ nhớ.\n",
    "2. **Biến đổi ngược thang đo (Inverse Transformation):**\n",
    "Do mô hình được huấn luyện trên không gian Logarit ($y_{log} = \\ln(1 + T_m)$) để giảm thiểu tác động của phân phối lệch, các giá trị dự đoán ($\\hat{y}_{log}$) cần được chuyển đổi ngược về thang đo nhiệt độ thực (Kelvin) để có ý nghĩa vật lý:\n",
    "\n",
    "$$\\hat{y}_{real} = \\exp(\\hat{y}_{log}) - 1$$\n",
    "\n",
    "Đoạn mã tích hợp cơ chế kiểm tra tự động (`threshold > 15`) để xác định xem dữ liệu đầu vào đang ở dạng Log hay dạng Thực, đảm bảo tính nhất quán khi tính toán sai số.\n",
    "\n",
    "3. **Tính toán Chỉ số (Metric Calculation):** Sử dụng ba chỉ số tiêu chuẩn: MAE (Sai số tuyệt đối trung bình), RMSE (Căn bậc hai sai số trung bình bình phương) và  (Hệ số xác định).\n",
    "\n",
    "#### Kết quả Thực nghiệm (Experimental Results)\n",
    "\n",
    "Kết quả định lượng của mô hình sử dụng tập đặc trưng Union (Hợp) như sau:\n",
    "\n",
    "| Chỉ số (Metric) | Giá trị | Ý nghĩa |\n",
    "| --- | --- | --- |\n",
    "| ** Score** | **0.6313** | Mô hình giải thích được khoảng **63.13%** phương sai của dữ liệu nhiệt độ nóng chảy. |\n",
    "| **RMSE** | **95.97 K** | Độ lệch chuẩn của sai số dự đoán là xấp xỉ 96 Kelvin. |\n",
    "| **MAE** | **63.29 K** | Trung bình mỗi dự đoán lệch khoảng 63.3 Kelvin so với thực tế. |\n",
    "\n",
    "Độ chính xác giải thích ($R^2$): 0.6313. Điều này cho thấy mô hình có khả năng nắm bắt được khoảng 63% quy luật biến thiên của nhiệt độ nóng chảy dựa trên cấu trúc 2D.\n",
    "\n",
    "Sai số dự báo (Error Metrics): Với MAE $\\approx$ 63.3 K và RMSE $\\approx$ 96.0 K, sai số của mô hình nằm trong ngưỡng chấp nhận được đối với bài toán QSPR phức tạp này.\n",
    "\n",
    "#### **Kết luận (Discussion)**\n",
    "\n",
    "Chiến lược Union hoạt động dựa trên nguyên lý không bỏ sót, chấp nhận giữ lại tất cả các đặc trưng tiềm năng mà bất kỳ thuật toán thành phần nào (RFE hoặc GA) đề xuất.\n",
    "\n",
    "* Ưu điểm: Kết quả $R^2 > 0.63$ chứng minh rằng việc tổng hợp \"trí tuệ\" của nhiều thuật toán chọn lọc giúp tạo ra một tập dữ liệu đầu vào giàu thông tin, đảm bảo mô hình có đủ dữ kiện để học các mối quan hệ phức tạp.\n",
    "\n",
    "* Hạn chế tồn tại: Mặc dù sở hữu số lượng đặc trưng lớn nhất, hiệu suất mô hình không tăng trưởng tuyến tính tương ứng. Điều này chỉ ra hiện tượng bão hòa thông tin (information saturation) - tức là việc thêm quá nhiều biến số không còn giúp giảm sai số đáng kể, mà ngược lại có thể làm tăng độ phức tạp tính toán và nguy cơ nhiễu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660baded",
   "metadata": {},
   "source": [
    "#### 2.4.2. Features Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec589a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_features_int = set(rfe_features) & set(ga_features)\n",
    "\n",
    "Tổng features sau khi giao (Intersection): 283"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d78a2bc",
   "metadata": {},
   "source": [
    "Đọc kết quả từ rfe_features.pkl và ga_features.pkl.\n",
    "\n",
    "Toán tử Giao (&): Dòng `set(rfe_features) & set(ga_features)` thực hiện phép giao tập hợp.\n",
    "\n",
    "Kết quả: Biến common_features_int chỉ chứa 283 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b767cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_params = {\n",
    "    'n_estimators': 8000,\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 40,\n",
    "    'max_depth': 15,\n",
    "    'min_child_samples': 20,\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'reg_alpha': 0.2,\n",
    "    'reg_lambda': 0.2,\n",
    "    'random_state': 2601,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "model_int = LGBMRegressor(**manual_params)\n",
    "\n",
    "model_int.fit(X_train[valid_features_int], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e94730e",
   "metadata": {},
   "source": [
    "#### HUẤN LUYỆN MÔ HÌNH VỚI CHIẾN LƯỢC INTERSECTION (GIAO)\n",
    "\n",
    "Về mặt kỹ thuật, quy trình này **hoàn toàn tương tự** như phần Union ở trên (cùng các bước chuẩn bị, train và lưu model). Tuy nhiên, có **2 điểm cốt lõi** mình cần làm rõ:\n",
    "\n",
    "1. Sự thay đổi về Dữ liệu (Input Data) - \"Đãi cát tìm vàng\"\n",
    "\n",
    "Điểm khác biệt duy nhất và quan trọng nhất nằm ở dòng này:\n",
    "`best_features_int = list(common_features_int)`\n",
    "\n",
    "* **Thay đổi:** Thay vì nạp vào 727 features hỗn tạp (Union), mình chỉ nạp vào **283 features** nằm trong tập giao thoa.\n",
    "* **Mục tiêu mới:** Chuyển từ chiến thuật \"Lấy số lượng bù chất lượng\" sang chiến thuật **\"Tinh gọn và Chính xác\"**. Mình muốn kiểm chứng giả thuyết: *Liệu một mô hình nhỏ gọn, sạch nhiễu có thể đánh bại một mô hình khổng lồ nhưng chứa nhiều \"rác\" hay không?*\n",
    "\n",
    "2. Tại sao giữ nguyên Tham số (Model Config)?\n",
    "\n",
    "Bạn sẽ thấy `manual_params` y hệt như trên:\n",
    "\n",
    "* **Lý do:** Để đảm bảo **tính công bằng (Fair Comparison)**.\n",
    "* Nếu mình thay đổi tham số lúc này, khi kết quả tốt lên, ta sẽ không biết là do \"chọn features tốt\" hay do \"chỉnh model khéo\".\n",
    "* Việc giữ nguyên cấu hình \"Học chậm (`lr=0.01`) - Cây sâu (`num_leaves=40`)\" giúp chứng minh rằng sự cải thiện hiệu suất (nếu có) hoàn toàn đến từ chất lượng của bộ dữ liệu đầu vào.\n",
    "\n",
    "**Tóm lại:**\n",
    "\n",
    "Phần code này là bước thực nghiệm đối chứng. Nó vẫn dùng Model LightGBM cũ, nhưng thay vì chạy bằng \"nhiên liệu thô\" (Union), nó chạy bằng \"nhiên liệu tinh chế\" (Intersection) để tối ưu hiệu suất."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0464a9ed",
   "metadata": {},
   "source": [
    "#### **SCORING ĐÁNH GIÁ ĐIỂM TỪ PHƯƠNG PHÁP INTERSECTION**\n",
    "\n",
    "* Phương pháp Kiểm thử (Evaluation Methodology)\n",
    "\n",
    "Quá trình đánh giá được thực hiện trên tập kiểm thử độc lập ( mẫu), tập trung vào việc kiểm chứng giả thuyết về **tính tối giản (Model Parsimony)**:\n",
    "\n",
    "1. **Dự báo (Prediction):** Mô hình LightGBM sử dụng tập đặc trưng giao thoa ( features) để dự báo nhiệt độ trên không gian Logarit.\n",
    "2. **Hậu xử lý (Post-processing):** Kết quả dự báo () được chuyển đổi ngược về thang đo nhiệt độ thực (Kelvin) thông qua hàm mũ: .\n",
    "3. **Đánh giá (Metrics):** Các chỉ số hiệu năng được tính toán dựa trên độ lệch giữa giá trị dự báo sau chuyển đổi và giá trị thực nghiệm.\n",
    "\n",
    "* Kết quả Thực nghiệm (Experimental Results)\n",
    "\n",
    "Kết quả định lượng của mô hình Intersection trên tập Test như sau:\n",
    "\n",
    "| Chỉ số (Metric) | Giá trị | Ý nghĩa |\n",
    "| --- | --- | --- |\n",
    "| ** Score** | **0.6298** | Mô hình giải thích được xấp xỉ **63%** phương sai của dữ liệu thực tế. |\n",
    "| **RMSE** | **96.17 K** | Sai số chuẩn (Standard Error) của dự báo là khoảng 96.2 Kelvin. |\n",
    "| **MAE** | **63.58 K** | Sai số tuyệt đối trung bình là khoảng 63.6 Kelvin. |\n",
    "\n",
    "Độ chính xác giải thích ($R^2$): 0.6298.Điều này cho thấy mô hình có khả năng nắm bắt được xấp xỉ 63% quy luật biến thiên của nhiệt độ nóng chảy dựa trên cấu trúc 2D. Đáng chú ý là dù số lượng đặc trưng giảm đi đáng kể (chỉ còn ~39% so với Union), khả năng giải thích dữ liệu của mô hình vẫn được bảo toàn gần như nguyên vẹn.\n",
    "\n",
    "Sai số dự báo (Error Metrics):Với MAE $\\approx$ 63.6 K và RMSE $\\approx$ 96.2 K, sai số của mô hình nằm trong ngưỡng chấp nhận được đối với bài toán QSPR phức tạp này. Việc sai số chỉ tăng rất nhẹ (không đáng kể) so với mô hình Union chứng tỏ rằng các đặc trưng bị loại bỏ chủ yếu là nhiễu, và mô hình Intersection đã thành công trong việc tối ưu hóa giữa độ chính xác và sự tinh gọn.\n",
    "\n",
    "**Kết luận (Discussion)**\n",
    "\n",
    "Khi đối chiếu với mô hình Union, kết quả của Intersection cho thấy một sự đánh đổi thú vị giữa **độ chính xác** và **độ phức tạp**:\n",
    "\n",
    "* **Hiệu năng tương đương (Comparable Performance):** Mức giảm của  là cực kỳ nhỏ (chỉ ~0.0015) và mức tăng sai số MAE không đáng kể (chỉ ~0.3 K). Điều này cho thấy mô hình Intersection vẫn duy trì được sức mạnh dự báo gần như nguyên vẹn.\n",
    "* **Tối ưu hóa tài nguyên (Resource Optimization):** Quan trọng hơn, mô hình này đạt được kết quả trên với số lượng đặc trưng chỉ bằng **~39%** so với mô hình Union (283 vs 727 features).\n",
    "* **Kết luận:** Chiến lược Intersection đã chứng minh được tính hiệu quả cao trong việc **loại bỏ dư thừa (Redundancy Reduction)**. Việc loại bỏ hơn 400 features mà không làm sụt giảm đáng kể độ chính xác chứng tỏ những features đó chủ yếu là nhiễu hoặc đa cộng tuyến. Mô hình Intersection do đó được đánh giá là gọn nhẹ, ổn định và ít rủi ro Overfitting hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c952a124",
   "metadata": {},
   "source": [
    "### 2.5. SCORE EACH METHOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05ad887",
   "metadata": {},
   "source": [
    "=====================================================================================\n",
    "             BẢNG SO SÁNH HIỆU QUẢ CÁC PHƯƠNG PHÁP CHỌN FEATURES\n",
    "=====================================================================================\n",
    "            Method  Features     R2     MAE    RMSE  Diff_R2  Diff_MAE  Diff_RMSE\n",
    "          Original       937 0.6494 62.1447 93.5861   0.0000    0.0000     0.0000\n",
    "               RFE       537 0.6471 62.5783 93.8959  -0.0023    0.4336     0.3098\n",
    "                GA       473 0.6522 62.5876 93.2244   0.0027    0.4430    -0.3617\n",
    "    Union (RFE|GA)       727 0.6500 61.9453 93.5073   0.0006   -0.1994    -0.0788\n",
    "Intersect (RFE&GA)       283 0.6546 62.5282 92.8978   0.0051    0.3835    -0.6883\n",
    "====================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e1b0b8",
   "metadata": {},
   "source": [
    "#### 1. Phân tích Hiện tượng: Sự thay đổi của RFE và GA\n",
    "\n",
    "* **Chuẩn hóa tham số:** Trước đó RFE chạy mô hình nhẹ để lọc, GA chạy mô hình quần thể để tiến hóa. Còn ở đây, tất cả đều được đưa vào một mô hình LightGBM \"hạng nặng\" (`n_estimators=8000`, `learning_rate=0.01`, `objective='regression_l1'`).\n",
    "\n",
    "* **Hệ quả:** Đây là kết quả **chính xác nhất** để so sánh, vì nó loại bỏ yếu tố may mắn do cấu hình model khác nhau.\n",
    "\n",
    "#### 2. Phân tích Chi tiết Kết quả (Detailed Analysis)\n",
    "\n",
    "Dựa trên bảng số liệu, ta có thể chia thành 3 nhóm chiến lược:\n",
    "\n",
    "**A. Nhóm Hiệu quả Tối đa (The Efficiency Champion): *INTERSECTION***\n",
    "\n",
    "* **Chỉ số:** $R^2 = 0.6546$ **(Cao nhất)** | **RMSE = 92.89 K (Thấp nhất)**.\n",
    "* **Số lượng Features:** **283** (Chỉ bằng 30% so với Original).\n",
    "* **Nhận định:**\n",
    "* Đây là phương pháp chiến thắng thuyết phục nhất. Dù dùng ít \"nguyên liệu\" nhất, nó lại nấu ra món ăn \"ngon\" nhất.\n",
    "* Việc RMSE giảm mạnh nhất (-0.6883) chứng tỏ việc loại bỏ các features nhiễu đã giúp mô hình hạn chế được các sai số lớn (outliers).\n",
    "* **Kết luận:** Nếu ưu tiên sự cân bằng giữa độ chính xác và tốc độ, đây là lựa chọn số 1.\n",
    "\n",
    "\n",
    "\n",
    "**B. Nhóm An toàn thông tin (The Safety Player): *UNION***\n",
    "\n",
    "* **Chỉ số:** **MAE = 61.9453 K (Thấp nhất)**.\n",
    "* **Số lượng Features:** 727.\n",
    "* **Nhận định:**\n",
    "* Union đạt MAE tốt nhất, nghĩa là sai số trung bình trên toàn tập dữ liệu là nhỏ nhất.\n",
    "* Tuy nhiên, $R^2$ và RMSE lại thua Intersection. Điều này gợi ý rằng Union hoạt động tốt với các chất \"phổ thông\", nhưng việc ôm đồm quá nhiều features khiến nó bị nhiễu khi gặp các chất \"khó\" (làm tăng RMSE).\n",
    "* **Kết luận:** Phù hợp nếu bạn sợ bỏ sót thông tin và chấp nhận mô hình nặng nề.\n",
    "\n",
    "\n",
    "\n",
    "**C. Nhóm Đơn lẻ (Stand-alone Methods): *GA vs. RFE***\n",
    "\n",
    "* **GA (473 feats):** $R^2 = 0.6522$ -> Tốt hơn RFE và Original.\n",
    "* **RFE (537 feats):** $R^2 = 0.6471$ -> Tệ hơn Original.\n",
    "* **Phân tích:**\n",
    "* **GA thắng RFE:** Điều này chứng minh giả thuyết rằng dữ liệu hóa học chứa nhiều mối quan hệ phi tuyến tính mà thuật toán tham lam như RFE không nhìn thấy, nhưng thuật toán tiến hóa (GA) lại tìm ra được.\n",
    "* **RFE bị tụt hạng:** Việc RFE có điểm thấp hơn cả Original (Diff_R2 = -0.0023) cho thấy nó đã lỡ tay cắt bỏ một số thông tin quan trọng hoặc giữ lại nhầm các feature gây nhiễu.\n",
    "\n",
    "#### 3. Ma trận Ra quyết định (Decision Matrix)\n",
    "\n",
    "Tùy thuộc vào bài toán thực tế của bạn ưu tiên điều gì, hãy chọn phương pháp tương ứng:\n",
    "\n",
    "| Ưu tiên (Priority) | Phương pháp Khuyên dùng | Lý do |\n",
    "| --- | --- | --- |\n",
    "| **Độ chính xác tổng thể (, RMSE)** | **Intersect (RFE & GA)** | Loại bỏ triệt để nhiễu, mô hình tập trung vào quy luật cốt lõi. |\n",
    "| **Sai số trung bình thấp (MAE)** | **Union (RFE | GA)** |\n",
    "| **Tốc độ dự báo (Inference Speed)** | **Intersect (RFE & GA)** | Chỉ cần tính toán 283 features, nhanh gấp 3 lần so với Original. |\n",
    "| **Khả năng giải thích (Explainability)** | **Intersect (RFE & GA)** | Ít biến số hơn giúp các nhà hóa học dễ phân tích nguyên nhân hơn. |\n",
    "| **Tài nguyên phần cứng thấp** | **Intersect (RFE & GA)** | Tốn ít RAM và CPU hơn khi chạy thực tế. |\n",
    "\n",
    "#### 4. Conclusion\n",
    "\n",
    "Nghiên cứu này khẳng định nguyên lý **\"Parsimony\" (Tiết kiệm)** trong Hóa học tính toán:\n",
    "\n",
    "> *\"Việc sử dụng 283 đặc trưng được chọn lọc kỹ càng bởi sự đồng thuận của RFE và GA (Intersection) mang lại hiệu quả dự đoán vượt trội hơn so với việc sử dụng toàn bộ 937 đặc trưng ban đầu.\"*\n",
    "\n",
    "Mô hình **Intersection** không chỉ giảm được **69.8%** số chiều dữ liệu mà còn cải thiện độ chính xác dự báo ( tăng từ 0.649 lên 0.655), chứng minh tính đúng đắn của quy trình xử lý dữ liệu mà chúng ta đã xây dựng."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6390924",
   "metadata": {},
   "source": [
    "### 2.6. ELBOW PLOT TRAIN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c06f6ff",
   "metadata": {},
   "source": [
    "***Mục tiêu Nghiên cứu***\n",
    "\n",
    "Sau khi áp dụng các thuật toán chọn lọc (RFE/GA), nghiên cứu tiếp tục thực hiện bước **sàng lọc hậu kỳ (Post-hoc Pruning)** nhằm tìm kiếm điểm cân bằng tối ưu (Trade-off) giữa độ phức tạp của mô hình và hiệu suất dự báo. Mục tiêu là xác định **\"Điểm khuỷu tay\" (Elbow Point)** - nơi mà việc giảm bớt số lượng đặc trưng không làm suy giảm đáng kể độ chính xác, tuân thủ nguyên lý tiết kiệm (Occam's Razor).\n",
    "\n",
    "***Quy trình Thực nghiệm***\n",
    "\n",
    "Mình áp dụng chiến lược **Loại bỏ lùi dựa trên độ quan trọng (Importance-based Backward Elimination)** với quy trình như sau:\n",
    "\n",
    "1. **Xếp hạng (Ranking):** Sử dụng mô hình LightGBM để tính toán và xếp hạng toàn bộ các đặc trưng dựa trên mức độ đóng góp thông tin (Feature Importance).\n",
    "2. **Cắt tỉa đệ quy (Recursive Pruning):** Thiết lập vòng lặp giảm dần số lượng đặc trưng () từ tập hợp đầy đủ xuống 5 đặc trưng. Tại mỗi bước , mô hình được huấn luyện lại và đánh giá trên tập kiểm thử độc lập.\n",
    "3. **Xây dựng quỹ đạo hiệu suất (Performance Trajectory):** Ghi nhận biến thiên của  và RMSE theo số lượng đặc trưng để xác định điểm bão hòa thông tin (Information Saturation).\n",
    "\n",
    "***Ý nghĩa***\n",
    "\n",
    "Phương pháp này giúp minh họa trực quan quy luật **\"Lợi ích cận biên giảm dần\" (Diminishing Returns)** trong dữ liệu: xác định ngưỡng tối thiểu số lượng descriptors cần thiết để duy trì độ chính xác cao, từ đó loại bỏ triệt để các nhiễu còn sót lại mà các bước trước chưa lọc hết."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ee0e1",
   "metadata": {},
   "source": [
    "#### 2.6.1. Elbow GA Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678e0f63",
   "metadata": {},
   "source": [
    "***1. Mục tiêu Phân tích***\n",
    "\n",
    "Sau khi Genetic Algorithm **(GA)** đề xuất tập đặc trưng tiềm năng, nghiên cứu tiến hành phân tích độ nhạy (**Sensitivity Analysis**) bằng phương pháp Loại bỏ lùi **(Backward Elimination)**. Mục tiêu là xác định \"*điểm bão hòa thông tin*\" - nơi việc giảm số lượng biến không làm suy giảm đáng kể độ chính xác của mô hình.\n",
    "\n",
    "***Kết quả thực nghiệm (Dựa trên biểu đồ Elbow & Bảng xếp hạng)***\n",
    "\n",
    "Dựa trên biểu đồ và bảng số liệu biến thiên hiệu suất ($R^2$, RMSE) theo số lượng đặc trưng giảm dần, mình ghi nhận các mốc quan trọng sau:\n",
    "\n",
    "* **Điểm Cực đại (Global Maximum)**: Hiệu suất dự báo đạt đỉnh tại mốc 431 đặc trưng với $R^2 \\approx 0.6313$ và $RMSE \\approx 95.98 K$. Đây là cấu hình mang lại độ chính xác toán học cao nhất.\n",
    "\n",
    "* **Điểm Khuỷu tay (The Elbow Point)**: Tại mốc 171 đặc trưng, mô hình ghi nhận $R^2 \\approx 0.6294$ và $RMSE \\approx 96.23 K$.\n",
    "\n",
    "* **Nhận định quan trọng**: Khi giảm gần 60% lượng đặc trưng (từ 431 xuống 171), độ chính xác giải thích ($R^2$) chỉ sụt giảm không đáng kể ($\\Delta R^2 \\approx 0.0019$). Điều này chứng minh khoảng 260 đặc trưng bị loại bỏ chủ yếu là nhiễu hoặc thông tin dư thừa.\n",
    "\n",
    "* **Ngưỡng Phá vỡ (Breaking Point):**\n",
    "Khi số lượng đặc trưng giảm xuống dưới **50**, hiệu suất mô hình sụt giảm nghiêm trọng ($R^2$ tụt xuống dưới 0.60). Điều này khẳng định bản chất phức tạp của bài toán dự đoán nhiệt độ nóng chảy, đòi hỏi một lượng thông tin cấu trúc tối thiểu (khoảng 50-70 descriptors) để nắm bắt được các quy luật hóa lý cơ bản.\n",
    "\n",
    "***Kết luận:***\n",
    "\n",
    "Kết quả phân tích Elbow cho phép chúng ta tự tin đề xuất mô hình rút gọn với **171 đặc trưng** cho các ứng dụng thực tế. Mô hình này đảm bảo tính tinh gọn (parsimony), giảm thiểu rủi ro quá khớp (overfitting) và tăng tốc độ xử lý mà vẫn duy trì độ chính xác ngang bằng với các mô hình phức tạp hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c8ae66",
   "metadata": {},
   "source": [
    "#### 2.6.2. Elbow RFECV Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c909e85",
   "metadata": {},
   "source": [
    "\n",
    "***Mục tiêu và Phương pháp***\n",
    "\n",
    "Mình sử dụng thuật toán **RFECV (Recursive Feature Elimination with Cross-Validation)** được triển khai với chiến lược \"cắt tỉa mạnh\" (step=20) kết hợp kiểm định chéo (CV=3) trên mô hình LightGBM. Khác với phân tích GA (tập trung vào tinh chỉnh), RFECV tập trung vào việc tìm kiếm tập con tối ưu từ toàn bộ không gian 937 đặc trưng ban đầu.\n",
    "\n",
    "***Phân tích Hiệu suất Quá trình Sàng lọc***\n",
    "\n",
    "* **Hành trình tối ưu hóa:**\n",
    "Quá trình chạy mất khoảng 24 phút (1437s) để duyệt qua 937 features ban đầu. Biểu đồ hiệu suất cho thấy sự ổn định đáng kinh ngạc của mô hình trong khoảng từ 350 đến 890 features, với điểm số  dao động rất nhỏ quanh mức **0.6600**.\n",
    "\n",
    "* **Số lượng Đặc trưng Tối ưu (Best Number of Features):**\n",
    "Thuật toán xác định số lượng đặc trưng tốt nhất là 617 features với điểm kiểm định chéo trung bình cao nhất ($CV\\ R^2 \\approx 0.6600$).\n",
    "\n",
    "* **Vùng hiệu suất (Performance Plateau)**:\n",
    "Bảng Leaderboard cho thấy sự chênh lệch điểm số giữa các tập con hàng đầu (630, 570, 470 features) là cực kỳ nhỏ ($< 0.0001$).\n",
    "\n",
    "    Điều này chỉ ra một vùng \"cao nguyên hiệu suất\" rộng lớn, nơi mô hình hoạt động ổn định với số lượng đặc trưng dao động từ 400 đến 600.\n",
    "\n",
    "\n",
    "* **Nhận xét:** So với GA (chỉ cần ~473 features), RFECV có xu hướng giữ lại nhiều đặc trưng hơn. Điều này phản ánh bản chất \"thận trọng\" của thuật toán: nó ngần ngại loại bỏ các biến có đóng góp nhỏ vì sợ mất mát thông tin, dẫn đến một tập kết quả khá cồng kềnh.\n",
    "\n",
    "\n",
    "***Đánh giá trên Tập Kiểm thử Độc lập (Final Test Evaluation)***\n",
    "\n",
    "Khi áp dụng tập 617 đặc trưng này vào mô hình LightGBM để dự báo trên tập Test độc lập, kết quả thu được là:\n",
    "\n",
    "* **Score:** 0.6521\n",
    "* **RMSE:** 117.76 K\n",
    "* **MAE:** 68.64 K\n",
    "\n",
    "**So sánh và Đánh giá:**\n",
    "Mặc dù điểm  (0.6521) khá cao và tương đương với các phương pháp khác, nhưng chỉ số **RMSE (117.76 K)** lại cao bất thường (so với mức ~96 K của phương pháp Intersection hay Elbow GA). Điều này gợi ý rằng RFECV có xu hướng giữ lại nhiều đặc trưng \"an toàn\" nhưng gây nhiễu nhẹ, làm tăng phương sai của các dự báo ngoại lai (outliers), dẫn đến RMSE bị kéo lên cao.\n",
    "\n",
    "* **Nguyên nhân:** Việc giữ lại quá nhiều đặc trưng (617 features) đã khiến mô hình bị **nhiễu** bởi các giá trị ngoại lai (outliers) hoặc các biến không thực sự quan trọng. Điều này làm tăng sai số bình phương (RMSE) dù độ chính xác tổng thể ($R^2$) vẫn tốt.\n",
    "\n",
    "\n",
    "***Kết luận***\n",
    "\n",
    "RFECV đã hoàn thành tốt vai trò sàng lọc sơ cấp, xác định được một tập hợp lớn các đặc trưng có ý nghĩa thống kê. Tuy nhiên, kết quả thực nghiệm cho thấy chiến lược \"giữ lại nhiều\" của nó chưa thực sự tối ưu về mặt sai số (RMSE). Điều này củng cố thêm tầm quan trọng của việc kết hợp RFECV với các phương pháp khác (như GA hoặc Intersection) để tinh lọc kỹ hơn, loại bỏ bớt các đặc trưng gây nhiễu mà RFECV đã bỏ sót."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da821d6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. KẾT LUẬN (CONCLUSION)\n",
    "\n",
    "### 4.1. Tổng kết Nghiên cứu\n",
    "\n",
    "Dự án đã thiết lập thành công quy trình QSPR (Quantitative Structure-Property Relationship) để dự đoán nhiệt độ nóng chảy () của hợp chất hữu cơ dựa trên các đặc trưng cấu trúc 2D. Từ tập dữ liệu thô ban đầu, mình đã áp dụng chiến lược sàng lọc đa tầng để giải quyết thách thức về nhiễu dữ liệu và sự bùng nổ số chiều (curse of dimensionality).\n",
    "\n",
    "Các kết quả thực nghiệm chính bao gồm:\n",
    "\n",
    "1. **Hiệu quả của Tiền xử lý:** Việc giới hạn phạm vi nghiên cứu () và biến đổi logarit mục tiêu đã giúp thiết lập một mô hình cơ sở tin cậy (), loại bỏ các sai số hệ thống do các giá trị ngoại lai cực đoan gây ra.\n",
    "2. **Sức mạnh của Đồng thuận Thuật toán (Intersection Strategy):** Chiến lược giao thoa giữa RFECV (thống kê) và Genetic Algorithm (tiến hóa) đã chứng minh tính ưu việt vượt trội. Mô hình **Intersection** đạt độ chính xác cao nhất () với số lượng đặc trưng tối ưu (283 features), giảm **70%** số chiều dữ liệu so với ban đầu mà vẫn cải thiện hiệu suất dự báo.\n",
    "3. **Nguyên lý Tiết kiệm (Parsimony):** Phân tích độ nhạy (Elbow Analysis) chỉ ra rằng chỉ cần khoảng **171 đặc trưng cốt lõi** là đủ để mô hình đạt độ chính xác tiệm cận mức tối ưu (), gợi mở hướng đi cho việc xây dựng các ứng dụng dự báo nhanh (rapid screening).\n",
    "\n",
    "### 4.2. Hạn chế\n",
    "\n",
    "Mặc dù đạt kết quả khả quan, mô hình vẫn gặp khó khăn trong việc dự báo chính xác các hợp chất có nhiệt độ nóng chảy cao (). Điều này phù hợp với các nghiên cứu trước đây (Tetko et al., 2001), cho thấy giới hạn của descriptors 2D trong việc mô tả các tương tác mạng tinh thể 3D phức tạp.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c0e48",
   "metadata": {},
   "source": [
    "\n",
    "## 5. TÀI LIỆU THAM KHẢO (REFERENCES)\n",
    "\n",
    "Các phương pháp và ngưỡng xử lý trong báo cáo này được tham chiếu dựa trên các công bố khoa học tiêu chuẩn trong lĩnh vực Hóa tin (Cheminformatics):\n",
    "\n",
    "1. **Về 2D Descriptors:**\n",
    "* *Tetko, I. V., Tanchuk, V. Y., Kasheva, T. N., & Villa, A. E. (2001).* Estimation of aqueous solubility of chemical compounds using E-state indices. Journal of chemical information and computer sciences, 41(6), 1488–1493. https://doi.org/10.1021/ci000392t\n",
    "*(Công trình này chỉ ra rằng các mô tả 2D hoạt động tốt cho các tính chất phân tử độc lập, nhưng gặp hạn chế với các tính chất phụ thuộc vào mạng tinh thể như nhiệt độ nóng chảy).*\n",
    "\n",
    "* *Dearden, J. C. (1999)*. Quantitative structure-property relationships for prediction of boiling point, vapor pressure, and melting point. Environmental Toxicology and Chemistry, 22(8), 1696–1709. https://doi.org/10.1897/01-363\n",
    " (Paper kinh điển giải thích tại sao MP khó dự đoán và vai trò của 2D)\n",
    "\n",
    "* *Karthikeyan, M., Glen, R. C., & Bender, A. (2005)*. General melting point prediction based on a diverse compound data set and artificial neural networks. Journal of chemical information and modeling, 45(3), 581–590. https://doi.org/10.1021/ci0500132 (Paper này sử dụng chủ yếu 2D descriptors cho mô hình MP lớn, chứng minh 2D là đủ)\n",
    "\n",
    "\n",
    "2. **Về xử lý dữ liệu Nhiệt độ nóng chảy:**\n",
    "* *Hughes, L. D., Palmer, D. S., Nigsch, F., & Mitchell, J. B. (2008).* Why are some properties more difficult to predict than others? A study of QSPR models of solubility, melting point, and Log P. Journal of chemical information and modeling, 48(1), 220–232. https://doi.org/10.1021/ci700307p\n",
    "* *(Tham chiếu cho việc loại bỏ các muối/chất vô cơ và giới hạn ngưỡng nhiệt độ < 1000K để tránh nhiễu).*\n",
    "\n",
    "\n",
    "3. **Về phương pháp Recursive Feature Elimination (RFE):**\n",
    "* *Guyon, I., Weston, J., Barnhill, S. et al (2002)*. Gene Selection for Cancer Classification using Support Vector Machines. Machine Learning 46, 389–422 (2002). https://doi.org/10.1023/A:1012487302797\n",
    "*(Cơ sở lý thuyết cho việc sử dụng RFE để loại bỏ lùi các đặc trưng yếu).*\n",
    "\n",
    "* *Granitto, P.M., Furlanello, C., Biasioli, F., & Gasperi, F. (2006).* Recursive feature elimination with random forest for PTR-MS analysis of agroindustrial products. Chemometrics and Intelligent Laboratory Systems. https://doi.org/10.1016/J.CHEMOLAB.2006.01.007 (Chứng minh hiệu quả của RFE trong dữ liệu hóa học thực nghiệm)\n",
    "\n",
    "4. **Về ngưỡng đánh giá mô hình QSPR ():**\n",
    "* *Tropsha, A., Gramatica, P. and Gombar, V. (2003)*, The Importance of Being Earnest: Validation is the Absolute Essential for Successful Application and Interpretation of QSPR Models. *QSAR & Combinatorial Science*, 22(1), 69-77. https://doi.org/10.1002/qsar.200390007\n",
    "*(Tiêu chuẩn vàng để đánh giá độ tin cậy của một mô hình dự báo hóa học).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
