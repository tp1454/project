{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba85359e",
   "metadata": {},
   "source": [
    "# IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a22807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "df = pd.read_csv('result/data/melting_point_features.csv')\n",
    "\n",
    "y = df['Tm']\n",
    "X = df.drop(columns=['Tm'])\n",
    "\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_clean = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clean, y, test_size=0.2, random_state=2601)\n",
    "\n",
    "base_model = LGBMRegressor(random_state=2601, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad834c6e",
   "metadata": {},
   "source": [
    "# REMOVE OUTLIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d500dc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train Size: 8416\n",
      "Cleaned Train Size: 8115\n",
      "Removed: 301 Samp\n"
     ]
    }
   ],
   "source": [
    "def remove_outliers(df, col):\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    return df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "\n",
    "train_data = X_train.copy()\n",
    "train_data['Tm'] = y_train\n",
    "\n",
    "print(f\"Original Train Size: {len(train_data)}\")\n",
    "\n",
    "train_data_clean = remove_outliers(train_data, 'Tm')\n",
    "\n",
    "print(f\"Cleaned Train Size: {len(train_data_clean)}\")\n",
    "print(f\"Removed: {len(train_data) - len(train_data_clean)} Samp\")\n",
    "\n",
    "X_train_clean = train_data_clean.drop(columns=['Tm'])\n",
    "y_train_clean = train_data_clean['Tm']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e176a307",
   "metadata": {},
   "source": [
    "# RFECV Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "915a736d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---START RFE ---\n",
      "Fitting estimator with 937 features.\n",
      "Fitting estimator with 917 features.\n",
      "Fitting estimator with 897 features.\n",
      "Fitting estimator with 877 features.\n",
      "Fitting estimator with 857 features.\n",
      "Fitting estimator with 837 features.\n",
      "Fitting estimator with 817 features.\n",
      "Fitting estimator with 797 features.\n",
      "Fitting estimator with 777 features.\n",
      "Fitting estimator with 757 features.\n",
      "Fitting estimator with 737 features.\n",
      "Fitting estimator with 717 features.\n",
      "Fitting estimator with 697 features.\n",
      "Fitting estimator with 677 features.\n",
      "Fitting estimator with 657 features.\n",
      "Fitting estimator with 637 features.\n",
      "Fitting estimator with 617 features.\n",
      "Fitting estimator with 597 features.\n",
      "Fitting estimator with 577 features.\n",
      "Fitting estimator with 557 features.\n",
      "Fitting estimator with 537 features.\n",
      "Fitting estimator with 517 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 397 features.\n",
      "Fitting estimator with 377 features.\n",
      "Fitting estimator with 357 features.\n",
      "Fitting estimator with 337 features.\n",
      "Fitting estimator with 317 features.\n",
      "Fitting estimator with 297 features.\n",
      "Fitting estimator with 277 features.\n",
      "Fitting estimator with 257 features.\n",
      "Fitting estimator with 237 features.\n",
      "Fitting estimator with 217 features.\n",
      "Fitting estimator with 197 features.\n",
      "Fitting estimator with 177 features.\n",
      "Fitting estimator with 157 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 937 features.\n",
      "Fitting estimator with 917 features.\n",
      "Fitting estimator with 897 features.\n",
      "Fitting estimator with 877 features.\n",
      "Fitting estimator with 857 features.\n",
      "Fitting estimator with 837 features.\n",
      "Fitting estimator with 817 features.\n",
      "Fitting estimator with 797 features.\n",
      "Fitting estimator with 777 features.\n",
      "Fitting estimator with 757 features.\n",
      "Fitting estimator with 737 features.\n",
      "Fitting estimator with 717 features.\n",
      "Fitting estimator with 697 features.\n",
      "Fitting estimator with 677 features.\n",
      "Fitting estimator with 657 features.\n",
      "Fitting estimator with 637 features.\n",
      "Fitting estimator with 617 features.\n",
      "Fitting estimator with 597 features.\n",
      "Fitting estimator with 577 features.\n",
      "Fitting estimator with 557 features.\n",
      "Fitting estimator with 537 features.\n",
      "Fitting estimator with 517 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 397 features.\n",
      "Fitting estimator with 377 features.\n",
      "Fitting estimator with 357 features.\n",
      "Fitting estimator with 337 features.\n",
      "Fitting estimator with 317 features.\n",
      "Fitting estimator with 297 features.\n",
      "Fitting estimator with 277 features.\n",
      "Fitting estimator with 257 features.\n",
      "Fitting estimator with 237 features.\n",
      "Fitting estimator with 217 features.\n",
      "Fitting estimator with 197 features.\n",
      "Fitting estimator with 177 features.\n",
      "Fitting estimator with 157 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 937 features.\n",
      "Fitting estimator with 917 features.\n",
      "Fitting estimator with 897 features.\n",
      "Fitting estimator with 877 features.\n",
      "Fitting estimator with 857 features.\n",
      "Fitting estimator with 837 features.\n",
      "Fitting estimator with 817 features.\n",
      "Fitting estimator with 797 features.\n",
      "Fitting estimator with 777 features.\n",
      "Fitting estimator with 757 features.\n",
      "Fitting estimator with 737 features.\n",
      "Fitting estimator with 717 features.\n",
      "Fitting estimator with 697 features.\n",
      "Fitting estimator with 677 features.\n",
      "Fitting estimator with 657 features.\n",
      "Fitting estimator with 637 features.\n",
      "Fitting estimator with 617 features.\n",
      "Fitting estimator with 597 features.\n",
      "Fitting estimator with 577 features.\n",
      "Fitting estimator with 557 features.\n",
      "Fitting estimator with 537 features.\n",
      "Fitting estimator with 517 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 397 features.\n",
      "Fitting estimator with 377 features.\n",
      "Fitting estimator with 357 features.\n",
      "Fitting estimator with 337 features.\n",
      "Fitting estimator with 317 features.\n",
      "Fitting estimator with 297 features.\n",
      "Fitting estimator with 277 features.\n",
      "Fitting estimator with 257 features.\n",
      "Fitting estimator with 237 features.\n",
      "Fitting estimator with 217 features.\n",
      "Fitting estimator with 197 features.\n",
      "Fitting estimator with 177 features.\n",
      "Fitting estimator with 157 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 937 features.\n",
      "Fitting estimator with 917 features.\n",
      "Fitting estimator with 897 features.\n",
      "Fitting estimator with 877 features.\n",
      "Fitting estimator with 857 features.\n",
      "Fitting estimator with 837 features.\n",
      "Fitting estimator with 817 features.\n",
      "Fitting estimator with 797 features.\n",
      "Fitting estimator with 777 features.\n",
      "Fitting estimator with 757 features.\n",
      "Fitting estimator with 737 features.\n",
      "Fitting estimator with 717 features.\n",
      "Fitting estimator with 697 features.\n",
      "Fitting estimator with 677 features.\n",
      "Fitting estimator with 657 features.\n",
      "Fitting estimator with 637 features.\n",
      "Fitting estimator with 617 features.\n",
      "Fitting estimator with 597 features.\n",
      "Fitting estimator with 577 features.\n",
      "Fitting estimator with 557 features.\n",
      "Fitting estimator with 537 features.\n",
      "Fitting estimator with 517 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 397 features.\n",
      "Fitting estimator with 377 features.\n",
      "Fitting estimator with 357 features.\n",
      "Fitting estimator with 337 features.\n",
      "Fitting estimator with 317 features.\n",
      "â±ï¸ Time Run: 2265.25 s\n",
      "âœ… RFECV Choosen 297 features:\n",
      "['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA8', 'SlogP_VSA1', 'SlogP_VSA11', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA7', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'NHOHCount', 'NOCount', 'NumAliphaticHeterocycles', 'NumAmideBonds', 'NumAtomStereoCenters', 'NumBridgeheadAtoms', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumHeterocycles', 'NumRotatableBonds', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'NumUnspecifiedAtomStereoCenters', 'Phi', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_COO', 'fr_COO2', 'fr_C_O_noCOO', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_allylic_oxid', 'fr_aryl_methyl', 'fr_bicyclic', 'fr_ester', 'fr_ether', 'fr_halogen', 'fr_imidazole', 'fr_imide', 'fr_ketone', 'fr_ketone_Topliss', 'fr_para_hydroxylation', 'fr_piperdine', 'fr_quatN', 'fr_thiazole', 'fr_unbrch_alkane', 'LargestRingSize', 'Count_C', 'Count_N', 'Count_O', 'Count_F', 'Count_P', 'Count_H', 'Morgan_1', 'Morgan_2', 'Morgan_3', 'Morgan_6', 'Morgan_27', 'Morgan_38', 'Morgan_39', 'Morgan_45', 'Morgan_50', 'Morgan_57', 'Morgan_64', 'Morgan_72', 'Morgan_73', 'Morgan_76', 'Morgan_77', 'Morgan_78', 'Morgan_79', 'Morgan_80', 'Morgan_90', 'Morgan_102', 'Morgan_104', 'Morgan_121', 'Morgan_132', 'Morgan_133', 'Morgan_134', 'Morgan_137', 'Morgan_138', 'Morgan_145', 'Morgan_149', 'Morgan_152', 'Morgan_154', 'Morgan_156', 'Morgan_157', 'Morgan_161', 'Morgan_166', 'Morgan_173', 'Morgan_175', 'Morgan_183', 'Morgan_184', 'Morgan_204', 'Morgan_205', 'Morgan_214', 'Morgan_233', 'Morgan_235', 'Morgan_238', 'Morgan_242', 'Morgan_280', 'Morgan_283', 'Morgan_294', 'Morgan_310', 'Morgan_311', 'Morgan_313', 'Morgan_314', 'Morgan_319', 'Morgan_332', 'Morgan_342', 'Morgan_344', 'Morgan_345', 'Morgan_354', 'Morgan_361', 'Morgan_364', 'Morgan_368', 'Morgan_378', 'Morgan_400', 'Morgan_401', 'Morgan_408', 'Morgan_432', 'Morgan_444', 'Morgan_448', 'Morgan_453', 'Morgan_466', 'Morgan_473', 'Morgan_479', 'Morgan_489', 'Morgan_491', 'Morgan_494', 'Morgan_502', 'MACCS_9', 'MACCS_24', 'MACCS_39', 'MACCS_43', 'MACCS_44', 'MACCS_47', 'MACCS_72', 'MACCS_74', 'MACCS_79', 'MACCS_83', 'MACCS_89', 'MACCS_90', 'MACCS_94', 'MACCS_95', 'MACCS_98', 'MACCS_101', 'MACCS_102', 'MACCS_105', 'MACCS_106', 'MACCS_107', 'MACCS_109', 'MACCS_114', 'MACCS_120', 'MACCS_121', 'MACCS_126', 'MACCS_127', 'MACCS_131', 'MACCS_132', 'MACCS_133', 'MACCS_144', 'MACCS_149', 'MACCS_151', 'MACCS_153', 'MACCS_155', 'MACCS_156', 'SlogP_VSA0', 'SlogP_VSA_sum', 'SMR_VSA0', 'Gasteiger_q_sum', 'Gasteiger_q_abs_sum', 'Gasteiger_q_min', 'Gasteiger_q_max', 'Gasteiger_q_std', 'HBondCapacity', 'HBondDensity_perHeavyAtom', 'RingDensity_perHeavyAtom', 'HeteroAtomFrac', 'AromRingFrac', 'HBond_Product', 'LogP_div_TPSA', 'LogP_x_TPSA', 'Flexibility_Score', 'MolWt_x_AromaticRings', 'Complexity_per_MW', 'Rigidity_Score', 'FracSingle', 'FracDouble', 'FracTriple', 'FracAromatic', 'MeanBondOrder', 'UnsatBondCount', 'Rings6', 'Rings56_frac', 'MurckoAtoms', 'SideChainAtoms', 'FormalCharge', 'SMI_len', 'SMI_branches', 'SMI_ringDigits', 'SMI_stereoAt', 'SMI_ezSlashes']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['result/rfe_features.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from lightgbm import LGBMRegressor\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\n---START RFE ---\")\n",
    "start = time.time()\n",
    "\n",
    "model = LGBMRegressor(\n",
    "    objective='regression_l1',\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=2601,\n",
    "    n_jobs=1,\n",
    "    verbose=-1)\n",
    "\n",
    "rfe = RFECV(\n",
    "    estimator=model, \n",
    "    min_features_to_select=50,\n",
    "    step=20,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "selected_rfe = X_train.columns[rfe.support_]\n",
    "print(f\"â±ï¸ Time Run: {time.time() - start:.2f} s\")\n",
    "print(f\"âœ… RFECV Choosen {len(selected_rfe)} features:\")\n",
    "print(list(selected_rfe))\n",
    "\n",
    "import joblib\n",
    "joblib.dump(list(selected_rfe), 'result/rfe_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcada578",
   "metadata": {},
   "source": [
    "# GENETIC ALGORITHM (GA) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1addb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ðŸ§¬ START RUN GENETIC ALGORITHM ---\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import time\n",
    "from sklearn_genetic import GAFeatureSelectionCV\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\n--- ðŸ§¬ START RUN GENETIC ALGORITHM ---\")\n",
    "start = time.time()\n",
    "model = LGBMRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,\n",
    "    random_state=2601,\n",
    "    n_jobs=1, verbose=-1\n",
    "    )\n",
    "\n",
    "ga = GAFeatureSelectionCV(\n",
    "    estimator=model,\n",
    "    cv=3,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    population_size=60,    \n",
    "    generations=20,\n",
    "    mutation_probability=0.1,\n",
    "    crossover_probability=0.8,\n",
    "    keep_top_k=2,\n",
    "    elitism=True,\n",
    "    n_jobs=-1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "ga.fit(X_train, y_train)\n",
    "\n",
    "selected_ga = X_train.columns[ga.support_]\n",
    "\n",
    "print(f\"â±ï¸ Time Run: {time.time() - start:.2f} s\")\n",
    "print(f\"\\nâœ… GA Choosen {len(selected_ga)} features:\")\n",
    "print(\"\\List Features:\")\n",
    "print(list(selected_ga))\n",
    "\n",
    "import joblib\n",
    "joblib.dump(list(selected_ga), 'result/ga_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586459f5",
   "metadata": {},
   "source": [
    "# UNION 2 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "rfe_features = joblib.load('result/rfe_features.pkl')\n",
    "ga_features = joblib.load('result/ga_features.pkl')\n",
    "\n",
    "common_features = set(rfe_features) | set(ga_features)\n",
    "\n",
    "print(f\"\\nðŸ’Ž Tá»•ng features sau khi gá»™p (Union): {len(common_features)}\")\n",
    "print(list(common_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878057cb",
   "metadata": {},
   "source": [
    "# CHOICE BEST FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "best_features = list(common_features)\n",
    "\n",
    "print(f\"âœ… Äang train model cuá»‘i cÃ¹ng vá»›i {len(best_features)} features...\")\n",
    "\n",
    "manual_params = {\n",
    "    'n_estimators': 3000,\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 60,\n",
    "    'max_depth': 15,\n",
    "    'objective': 'regression_l1',\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'random_state': 2601,\n",
    "    'n_jobs': 1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "final_model = LGBMRegressor(**manual_params)\n",
    "\n",
    "final_model.fit(X_clean[best_features], y)\n",
    "\n",
    "# LÆ°u model vÃ  danh sÃ¡ch features\n",
    "joblib.dump(final_model, 'result/final_melting_point_model.pkl')\n",
    "joblib.dump(best_features, 'result/final_features_list.pkl')\n",
    "\n",
    "print(\"ðŸ’¾ ÄÃ£ lÆ°u model vÃ  features thÃ nh cÃ´ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dc7df6",
   "metadata": {},
   "source": [
    "# SCORING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c831c1d",
   "metadata": {},
   "source": [
    "## Score Combine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7549c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import gc\n",
    "model = joblib.load('result/final_melting_point_model.pkl')\n",
    "features = joblib.load('result/final_features_list.pkl')\n",
    "\n",
    "df = pd.read_csv('result/data/melting_point_features.csv')\n",
    "\n",
    "needed_cols = list(features) + ['Tm']\n",
    "\n",
    "existing_cols = [c for c in needed_cols if c in df.columns]\n",
    "\n",
    "df_reduced = df[existing_cols].copy()\n",
    "\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "y = df_reduced['Tm']\n",
    "X = df_reduced.drop(columns=['Tm'])\n",
    "\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X = X.mask(X > 1e308, np.nan)\n",
    "\n",
    "print(\"âš™ï¸(Imputing)...\")\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_clean = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "_, X_test, _, y_test = train_test_split(X_clean, y, test_size=0.2, random_state=2601)\n",
    "\n",
    "y_pred = model.predict(X_test[features])\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n--- ðŸ RESULT ---\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b8109b",
   "metadata": {},
   "source": [
    "## Score each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c2314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('result/data/melting_point_features.csv')\n",
    "y = df['Tm']\n",
    "X = df.drop(columns=['Tm']).select_dtypes(include=[np.number])\n",
    "\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X = X.mask(X > 1e308, np.nan)\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_clean = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clean, y, test_size=0.2)\n",
    "\n",
    "def get_metrics(name, feature_list):\n",
    "    valid_feats = [f for f in feature_list if f in X_train.columns]\n",
    "    \n",
    "    if not valid_feats: return {\"Method\": name, \"Features\": 0, \"RMSE\": 0, \"R2\": 0}\n",
    "\n",
    "    model = LGBMRegressor(      \n",
    "        n_jobs=1,\n",
    "        verbose=-1,\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.01,\n",
    "        num_leaves=50,\n",
    "        max_depth=-1)\n",
    "    model.fit(X_train[valid_feats], y_train)\n",
    "    y_pred = model.predict(X_test[valid_feats])\n",
    "    \n",
    "    return {\n",
    "        \"Method\": name,\n",
    "        \"Features\": len(valid_feats),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        \"R2\": r2_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "feats_all = list(X_train.columns)\n",
    "feats_rfe = list(selected_rfe) if 'selected_rfe' in globals() else []\n",
    "feats_ga = list(selected_ga) if 'selected_ga' in globals() else []\n",
    "\n",
    "results = []\n",
    "results.append(get_metrics(\"Original\", feats_all))\n",
    "results.append(get_metrics(\"RFE\", feats_rfe))\n",
    "results.append(get_metrics(\"GA\", feats_ga))\n",
    "\n",
    "df_res = pd.DataFrame(results)\n",
    "base_rmse = df_res.loc[0, 'RMSE']\n",
    "base_r2 = df_res.loc[0, 'R2']\n",
    "\n",
    "df_res['Diff_RMSE'] = df_res['RMSE'] - base_rmse\n",
    "df_res['Diff_R2'] = df_res['R2'] - base_r2\n",
    "\n",
    "print(df_res.round(4))\n",
    "\n",
    "common = set(feats_rfe) & set(feats_ga)\n",
    "print(f\"\\nCommon Features ({len(common)}):\", list(common))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736270fd",
   "metadata": {},
   "source": [
    "# GridSearch Find Best Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60922e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"... GridSearch ...\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [500, 1000, 2000],\n",
    "    'learning_rate': [0.01, 0.03, 0.05],\n",
    "    'num_leaves': [31, 50],\n",
    "    'max_depth': [-1, 10, 20]\n",
    "}\n",
    "\n",
    "base_model = LGBMRegressor(n_jobs=1, verbose=-1)\n",
    "\n",
    "valid_ga_feats = [f for f in list(selected_ga) if f in X_train.columns]\n",
    "\n",
    "grid = GridSearchCV(base_model, param_grid, cv=3, scoring='r2', n_jobs=4, verbose=1)\n",
    "grid.fit(X_train[valid_ga_feats], y_train)\n",
    "\n",
    "print(\"\\n--- Best Params ---\")\n",
    "print(f\"Best Params: {grid.best_params_}\")\n",
    "print(f\"Best R2 Score (Train CV): {grid.best_score_:.4f}\")\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test[valid_ga_feats])\n",
    "print(f\"Test R2 Score: {r2_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59d25ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "model_final = LGBMRegressor(\n",
    "    learning_rate=0.01, \n",
    "    n_estimators=500, \n",
    "    num_leaves=50, \n",
    "    max_depth=-1, \n",
    "    random_state=2601, \n",
    "    n_jobs=1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2601)\n",
    "\n",
    "valid_ga_feats = [f for f in list(selected_ga) if f in X_train.columns]\n",
    "X_ga = X_clean[valid_ga_feats]\n",
    "\n",
    "scores = cross_val_score(model_final, X_ga, y, cv=kf, scoring='r2', n_jobs=4)\n",
    "\n",
    "print(f\"Scores: {scores}\")\n",
    "print(f\"Mean R2: {scores.mean():.4f} (+/- {scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709c9b7",
   "metadata": {},
   "source": [
    "# Elbow GA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098e2a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "best_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 2000,\n",
    "    'num_leaves': 50,\n",
    "    'max_depth': -1,\n",
    "    'random_state': 2601,\n",
    "    'n_jobs': 1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "valid_ga_feats = [f for f in list(selected_ga) if f in X_train.columns]\n",
    "\n",
    "print(\"Äang xáº¿p háº¡ng features...\")\n",
    "ranker = LGBMRegressor(**best_params)\n",
    "ranker.fit(X_train[valid_ga_feats], y_train)\n",
    "\n",
    "imp_df = pd.DataFrame({\n",
    "    'Feature': valid_ga_feats,\n",
    "    'Importance': ranker.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "sorted_feats = imp_df['Feature'].tolist()\n",
    "\n",
    "steps = list(range(len(sorted_feats), 99, -50)) + list(range(90, 0, -10))\n",
    "results = []\n",
    "\n",
    "print(f\"\\nBáº¯t Ä‘áº§u vÃ²ng láº·p cáº¯t giáº£m features ({len(steps)} vÃ²ng)...\")\n",
    "\n",
    "for k in steps:\n",
    "    current_feats = sorted_feats[:k]\n",
    "    \n",
    "    model = LGBMRegressor(**best_params)\n",
    "    model.fit(X_train[current_feats], y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test[current_feats])\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    print(f\"   -> DÃ¹ng {k:3d} features: R2 = {r2:.4f} | RMSE = {rmse:.2f}\")\n",
    "    results.append({'Num_Features': k, 'R2': r2, 'RMSE': rmse})\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values(by='Num_Features')\n",
    "df_leaderboard = df_results.sort_values(by='R2', ascending=False).reset_index(drop=True)\n",
    "csv_filename = 'result/feature_selection_results.csv'\n",
    "df_results.to_csv(csv_filename, index=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_results['Num_Features'], df_results['R2'], marker='o', linewidth=2, color='blue')\n",
    "\n",
    "best_row = df_results.loc[df_results['R2'].idxmax()]\n",
    "plt.scatter(best_row['Num_Features'], best_row['R2'], color='red', s=150, zorder=5)\n",
    "plt.annotate(f\"Äá»‰nh: {best_row['R2']:.4f}\\n({int(best_row['Num_Features'])} feats)\", \n",
    "             (best_row['Num_Features'], best_row['R2']), \n",
    "             xytext=(best_row['Num_Features']+20, best_row['R2']-0.01),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "plt.title('Biá»ƒu Ä‘á»“ Elbow: Hiá»‡u quáº£ khi giáº£m dáº§n sá»‘ lÆ°á»£ng Features', fontsize=14)\n",
    "plt.xlabel('Sá»‘ lÆ°á»£ng Features', fontsize=12)\n",
    "plt.ylabel('Äá»™ chÃ­nh xÃ¡c (R2)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.gca().invert_xaxis()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBáº¢NG Xáº¾P Háº NG (SCORE GIáº¢M Dáº¦N):\")\n",
    "print(df_leaderboard[['R2', 'Num_Features', 'RMSE']].head(10))\n",
    "\n",
    "print(\"\\nBáº¢NG THEO THá»¨ Tá»° FEATURE (ÃT -> NHIá»€U):\")\n",
    "print(df_results[['Num_Features', 'R2', 'RMSE']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13a8c2",
   "metadata": {},
   "source": [
    "# Elbow RFECV Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ae099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from lightgbm import LGBMRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- START RFE WITH LEADERBOARD ---\")\n",
    "start = time.time()\n",
    "\n",
    "model = LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=50,\n",
    "    max_depth=-1,\n",
    "    random_state=2601,\n",
    "    n_jobs=1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=model,\n",
    "    step=20,\n",
    "    cv=3,\n",
    "    scoring='r2', \n",
    "    min_features_to_select=50,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "r2_scores = rfecv.cv_results_['mean_test_score']\n",
    "n_scores = len(r2_scores)\n",
    "\n",
    "feature_counts = [50 + i * 20 for i in range(n_scores)]\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "    'Num_Features': feature_counts,\n",
    "    'Score_R2': r2_scores\n",
    "})\n",
    "\n",
    "df_leaderboard = df_results.sort_values(by='Score_R2', ascending=False).reset_index(drop=True)\n",
    "\n",
    "selected_rfecv = X_train.columns[rfecv.support_]\n",
    "print(f\"\\nTime Run: {time.time() - start:.2f} s\")\n",
    "print(f\"Best Number of Features: {rfecv.n_features_}\")\n",
    "print(f\"Best R2 Score: {df_leaderboard.iloc[0]['Score_R2']:.4f}\")\n",
    "\n",
    "print(\"\\nLEADERBOARD (DESCENDING SCORE):\")\n",
    "print(df_leaderboard.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nPROGRESS (BY FEATURE COUNT):\")\n",
    "print(df_results.head(10).to_string(index=False))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_results['Num_Features'], df_results['Score_R2'], marker='o', color='green', linewidth=2)\n",
    "\n",
    "best_row = df_leaderboard.iloc[0]\n",
    "plt.scatter(best_row['Num_Features'], best_row['Score_R2'], color='red', s=150, zorder=5)\n",
    "plt.annotate(f\"Best: {best_row['Score_R2']:.4f}\\n({int(best_row['Num_Features'])} feats)\", \n",
    "             (best_row['Num_Features'], best_row['Score_R2']), \n",
    "             xytext=(best_row['Num_Features']+20, best_row['Score_R2']-0.005),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "plt.title('RFECV Performance Curve', fontsize=14)\n",
    "plt.xlabel('Number of Features Selected', fontsize=12)\n",
    "plt.ylabel('Cross Validation Score (R2)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSelected Features List:\")\n",
    "print(list(selected_rfecv))\n",
    "\n",
    "df_results.to_csv('result/rfecv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5728a31e",
   "metadata": {},
   "source": [
    "# Compare before and after using partial correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257cc2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "ULTRA_HIGH_CORR = 0.999 \n",
    "\n",
    "corr_matrix = X_train_clean.corr().abs()\n",
    "\n",
    "ranker_temp = LGBMRegressor(n_estimators=100, verbose=-1, random_state=2601)\n",
    "ranker_temp.fit(X_train_clean, y_train_clean)\n",
    "importances = pd.Series(ranker_temp.feature_importances_, index=X_train_clean.columns)\n",
    "\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = []\n",
    "\n",
    "for column in upper.columns:\n",
    "    correlated_cols = upper.index[upper[column] > ULTRA_HIGH_CORR].tolist()\n",
    "    if correlated_cols:\n",
    "        for other_col in correlated_cols:\n",
    "            if other_col in to_drop: continue\n",
    "            imp_col = importances.get(column, 0)\n",
    "            imp_other = importances.get(other_col, 0)\n",
    "            if imp_col < imp_other:\n",
    "                to_drop.append(column)\n",
    "                break \n",
    "            else:\n",
    "                to_drop.append(other_col)\n",
    "\n",
    "to_drop = list(set(to_drop))\n",
    "\n",
    "print(f\"Phat hien {len(to_drop)} features tuong quan > {ULTRA_HIGH_CORR}\")\n",
    "print(f\"Vi du: {to_drop[:5]}\")\n",
    "\n",
    "voting_model = LGBMRegressor(\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=2000,\n",
    "    num_leaves=50,\n",
    "    max_depth=-1,\n",
    "    random_state=2601,\n",
    "    n_jobs=1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "if len(to_drop) > 0:\n",
    "    X_train_ultra = X_train_clean.drop(columns=to_drop)\n",
    "    X_test_ultra = X_test.drop(columns=to_drop)\n",
    "\n",
    "    print(f\"Train thu tren {X_train_ultra.shape[1]} features...\")\n",
    "    voting_model.fit(X_train_ultra, y_train_clean)\n",
    "    \n",
    "    y_pred = voting_model.predict(X_test_ultra)\n",
    "    new_r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    old_r2 = 0.6313 \n",
    "    \n",
    "    print(f\"Ket qua R2 moi: {new_r2:.4f}\")\n",
    "    \n",
    "    methods = ['Original Clean', 'Filtered (>0.995)']\n",
    "    scores = [old_r2, new_r2]\n",
    "    colors = ['green', 'red']\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    bars = plt.bar(methods, scores, color=colors, width=0.5)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{height:.4f}',\n",
    "                 ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "    plt.title('Hieu qua giua giu nguyen va loc tuong quan', fontsize=14)\n",
    "    plt.ylabel('R2 Score', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.ylim(0, 0.8)\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Khong co features nao qua giong nhau de xoa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b8e1ac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a24c7",
   "metadata": {},
   "source": [
    "# EXTRA MODEL \n",
    "# MAE (Mean Absolute Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0985c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "import time\n",
    "\n",
    "best_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 2000,\n",
    "    'num_leaves': 50,\n",
    "    'max_depth': -1,\n",
    "    'random_state': 2601,\n",
    "    'n_jobs': 1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "fast_params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 100,\n",
    "    'num_leaves': 31,\n",
    "    'random_state': 2601,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "print(\"Ranking features (Fast Mode)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rfe_selector = RFE(estimator=LGBMRegressor(**fast_params), \n",
    "                   n_features_to_select=1, \n",
    "                   step=0.1, \n",
    "                   verbose=0)\n",
    "\n",
    "rfe_selector.fit(X_train, y_train) \n",
    "\n",
    "rfe_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Rank': rfe_selector.ranking_\n",
    "}).sort_values(by='Rank', ascending=True)\n",
    "\n",
    "sorted_feats = rfe_df['Feature'].tolist()\n",
    "print(f\"Ranking done in {time.time() - start_time:.1f}s\")\n",
    "\n",
    "steps = list(range(len(sorted_feats), 99, -50)) + list(range(90, 0, -10))\n",
    "results = []\n",
    "\n",
    "print(f\"\\nStarting MAE Loop ({len(steps)} steps)...\")\n",
    "\n",
    "for k in steps:\n",
    "    current_feats = sorted_feats[:k]\n",
    "    \n",
    "    model = LGBMRegressor(**best_params)\n",
    "    model.fit(X_train[current_feats], y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test[current_feats])\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"   -> Top {k:3d} features: MAE = {mae:.2f}\")\n",
    "    results.append({'Num_Features': k, 'MAE': mae})\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values(by='Num_Features')\n",
    "df_leaderboard = df_results.sort_values(by='MAE', ascending=True).reset_index(drop=True)\n",
    "\n",
    "csv_filename = 'result/rfe_mae_results.csv'\n",
    "df_results.to_csv(csv_filename, index=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_results['Num_Features'], df_results['MAE'], marker='o', linewidth=2, color='purple')\n",
    "\n",
    "best_row = df_results.loc[df_results['MAE'].idxmin()]\n",
    "plt.scatter(best_row['Num_Features'], best_row['MAE'], color='red', s=150, zorder=5)\n",
    "plt.annotate(f\"Best MAE: {best_row['MAE']:.2f}\\n({int(best_row['Num_Features'])} feats)\", \n",
    "             (best_row['Num_Features'], best_row['MAE']), \n",
    "             xytext=(best_row['Num_Features']+20, best_row['MAE']+5),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "plt.title('RFE Feature Selection (MAE Metric)', fontsize=14)\n",
    "plt.xlabel('Number of Features', fontsize=12)\n",
    "plt.ylabel('Mean Absolute Error (MAE)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.gca().invert_xaxis()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTOP 5 CONFIGURATIONS (LOWEST MAE):\")\n",
    "print(df_leaderboard.head(5))\n",
    "\n",
    "print(\"\\nLOOP PROGRESS:\")\n",
    "print(df_results.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
