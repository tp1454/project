{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba85359e",
   "metadata": {},
   "source": [
    "# IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a22807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "df = pd.read_csv('result/data/melting_point_features.csv')\n",
    "\n",
    "y = df['Tm']\n",
    "X = df.drop(columns=['Tm'])\n",
    "\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_clean = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clean, y, test_size=0.2, random_state=2601)\n",
    "\n",
    "base_model = LGBMRegressor(random_state=2601, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d500dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, col):\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "\n",
    "train_data = X_train.copy()\n",
    "train_data['Tm'] = y_train\n",
    "\n",
    "print(f\"Sá»‘ lÆ°á»£ng máº«u trÆ°á»›c khi lá»c: {len(train_data)}\")\n",
    "train_data_clean = remove_outliers(train_data, 'Tm')\n",
    "print(f\"Sá»‘ lÆ°á»£ng máº«u sau khi lá»c: {len(train_data_clean)}\")\n",
    "\n",
    "X_train_clean = train_data_clean.drop(columns=['Tm'])\n",
    "y_train_clean = train_data_clean['Tm']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e176a307",
   "metadata": {},
   "source": [
    "# RFECV Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from lightgbm import LGBMRegressor\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\n---START RFE ---\")\n",
    "start = time.time()\n",
    "\n",
    "model = LGBMRegressor(n_estimators=500, learning_rate=0.05,\n",
    "    n_jobs=1,\n",
    "    verbose=-1)\n",
    "\n",
    "rfe = RFECV(estimator=model, min_features_to_select=50, step=20, n_jobs=-1, verbose=1)\n",
    "\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "selected_rfe = X_train.columns[rfe.support_]\n",
    "print(f\"â±ï¸ Time Run: {time.time() - start:.2f} s\")\n",
    "print(f\"âœ… RFECV Choosen {len(selected_rfe)} features:\")\n",
    "print(list(selected_rfe))\n",
    "\n",
    "import joblib\n",
    "joblib.dump(list(selected_rfe), 'result/rfe_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcada578",
   "metadata": {},
   "source": [
    "# GENETIC ALGORITHM (GA) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "from sklearn_genetic import GAFeatureSelectionCV\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\n--- ðŸ§¬ START RUN GENETIC ALGORITHM ---\")\n",
    "start = time.time()\n",
    "model = LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,\n",
    "    random_state=2601,\n",
    "    n_jobs=1, verbose=-1\n",
    "    )\n",
    "\n",
    "ga = GAFeatureSelectionCV(\n",
    "    estimator=model,\n",
    "    cv=3,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    population_size=60,    \n",
    "    generations=30,\n",
    "    mutation_probability=0.1,\n",
    "    crossover_probability=0.8,\n",
    "    keep_top_k=2,\n",
    "    elitism=True,\n",
    "    max_features=400,\n",
    "    n_jobs=-1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "ga.fit(X_train, y_train)\n",
    "\n",
    "selected_ga = X_train.columns[ga.support_]\n",
    "\n",
    "print(f\"â±ï¸ Time Run: {time.time() - start:.2f} s\")\n",
    "print(f\"\\nâœ… GA Choosen {len(selected_ga)} features:\")\n",
    "print(f\"ðŸ† Best Score CV (MAE): {-ga.best_features_scores_:.4f}\")\n",
    "print(\"\\List Features:\")\n",
    "print(list(selected_ga))\n",
    "\n",
    "import joblib\n",
    "joblib.dump(list(selected_ga), 'result/ga_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586459f5",
   "metadata": {},
   "source": [
    "# COLLAB 2 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_features = set(selected_rfe) & set(selected_ga)\n",
    "print(f\"\\nðŸ’Ž CÃ¡c features quan trá»ng Ä‘Æ°á»£c cáº£ 2 thuáº­t toÃ¡n cÃ¹ng chá»n ({len(common_features)}):\")\n",
    "print(common_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878057cb",
   "metadata": {},
   "source": [
    "# CHOICE BEST FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "best_features = list(common_features) \n",
    "\n",
    "print(f\"âœ… Äang train model cuá»‘i cÃ¹ng vá»›i {len(best_features)} features...\")\n",
    "\n",
    "final_model = LGBMRegressor(random_state=2601, n_jobs=-1, verbose=-1)\n",
    "\n",
    "final_model.fit(X_clean[best_features], y)\n",
    "\n",
    "# LÆ°u model vÃ  danh sÃ¡ch features\n",
    "joblib.dump(final_model, 'result/final_melting_point_model.pkl')\n",
    "joblib.dump(best_features, 'result/final_features_list.pkl')\n",
    "\n",
    "print(\"ðŸ’¾ ÄÃ£ lÆ°u model vÃ  features thÃ nh cÃ´ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7607d1d1",
   "metadata": {},
   "source": [
    "# REDUCED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ffc008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "features_to_save = list(selected_ga)\n",
    "\n",
    "train_df = X_train[features_to_save].copy()\n",
    "train_df['Tm'] = y_train\n",
    "\n",
    "test_df = X_test[features_to_save].copy()\n",
    "test_df['Tm'] = y_test\n",
    "\n",
    "train_df.to_csv('result/train_data_reduced.csv', index=False)\n",
    "test_df.to_csv('result/test_data_reduced.csv', index=False)\n",
    "\n",
    "print(f\"âœ… Saved reduced Data:\")\n",
    "print(f\"   - Train: {train_df.shape} -> 'result/train_data_reduced.csv'\")\n",
    "print(f\"   - Test:  {test_df.shape}  -> 'result/test_data_reduced.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dc7df6",
   "metadata": {},
   "source": [
    "# SCORING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c831c1d",
   "metadata": {},
   "source": [
    "## Score Combine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7549c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import gc\n",
    "model = joblib.load('result/final_melting_point_model.pkl')\n",
    "features = joblib.load('result/final_features_list.pkl')\n",
    "\n",
    "df = pd.read_csv('result/data/melting_point_features.csv')\n",
    "\n",
    "needed_cols = list(features) + ['Tm']\n",
    "\n",
    "existing_cols = [c for c in needed_cols if c in df.columns]\n",
    "\n",
    "df_reduced = df[existing_cols].copy()\n",
    "\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "y = df_reduced['Tm']\n",
    "X = df_reduced.drop(columns=['Tm'])\n",
    "\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X = X.mask(X > 1e308, np.nan)\n",
    "\n",
    "print(\"âš™ï¸(Imputing)...\")\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_clean = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "_, X_test, _, y_test = train_test_split(X_clean, y, test_size=0.2, random_state=2601)\n",
    "\n",
    "y_pred = model.predict(X_test[features])\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n--- ðŸ RESULT ---\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b8109b",
   "metadata": {},
   "source": [
    "## Score each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c2314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('result/data/melting_point_features.csv')\n",
    "y = df['Tm']\n",
    "X = df.drop(columns=['Tm']).select_dtypes(include=[np.number])\n",
    "\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X = X.mask(X > 1e308, np.nan)\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_clean = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clean, y, test_size=0.2)\n",
    "\n",
    "def get_metrics(name, feature_list):\n",
    "    valid_feats = [f for f in feature_list if f in X_train.columns]\n",
    "    \n",
    "    if not valid_feats: return {\"Method\": name, \"Features\": 0, \"RMSE\": 0, \"R2\": 0}\n",
    "\n",
    "    model = LGBMRegressor(      \n",
    "        n_jobs=1,\n",
    "        verbose=-1,\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.01,\n",
    "        num_leaves=50,\n",
    "        max_depth=-1)\n",
    "    model.fit(X_train[valid_feats], y_train)\n",
    "    y_pred = model.predict(X_test[valid_feats])\n",
    "    \n",
    "    return {\n",
    "        \"Method\": name,\n",
    "        \"Features\": len(valid_feats),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        \"R2\": r2_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "feats_all = list(X_train.columns)\n",
    "feats_rfe = list(selected_rfe) if 'selected_rfe' in globals() else []\n",
    "feats_ga = list(selected_ga) if 'selected_ga' in globals() else []\n",
    "\n",
    "results = []\n",
    "results.append(get_metrics(\"Original\", feats_all))\n",
    "results.append(get_metrics(\"RFE\", feats_rfe))\n",
    "results.append(get_metrics(\"GA\", feats_ga))\n",
    "\n",
    "df_res = pd.DataFrame(results)\n",
    "base_rmse = df_res.loc[0, 'RMSE']\n",
    "base_r2 = df_res.loc[0, 'R2']\n",
    "\n",
    "df_res['Diff_RMSE'] = df_res['RMSE'] - base_rmse\n",
    "df_res['Diff_R2'] = df_res['R2'] - base_r2\n",
    "\n",
    "print(df_res.round(4))\n",
    "\n",
    "common = set(feats_rfe) & set(feats_ga)\n",
    "print(f\"\\nCommon Features ({len(common)}):\", list(common))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736270fd",
   "metadata": {},
   "source": [
    "# GridSearch Find Best Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60922e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"... GridSearch ...\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [500, 1000, 2000],\n",
    "    'learning_rate': [0.01, 0.03, 0.05],\n",
    "    'num_leaves': [31, 50],\n",
    "    'max_depth': [-1, 10, 20]\n",
    "}\n",
    "\n",
    "base_model = LGBMRegressor(n_jobs=1, verbose=-1)\n",
    "\n",
    "valid_ga_feats = [f for f in list(selected_ga) if f in X_train.columns]\n",
    "\n",
    "grid = GridSearchCV(base_model, param_grid, cv=3, scoring='r2', n_jobs=4, verbose=1)\n",
    "grid.fit(X_train[valid_ga_feats], y_train)\n",
    "\n",
    "print(\"\\n--- Best Params ---\")\n",
    "print(f\"Best Params: {grid.best_params_}\")\n",
    "print(f\"Best R2 Score (Train CV): {grid.best_score_:.4f}\")\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test[valid_ga_feats])\n",
    "print(f\"Test R2 Score: {r2_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59d25ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "model_final = LGBMRegressor(\n",
    "    learning_rate=0.01, \n",
    "    n_estimators=500, \n",
    "    num_leaves=50, \n",
    "    max_depth=-1, \n",
    "    random_state=2601, \n",
    "    n_jobs=1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2601)\n",
    "\n",
    "valid_ga_feats = [f for f in list(selected_ga) if f in X_train.columns]\n",
    "X_ga = X_clean[valid_ga_feats]\n",
    "\n",
    "scores = cross_val_score(model_final, X_ga, y, cv=kf, scoring='r2', n_jobs=4)\n",
    "\n",
    "print(f\"Scores: {scores}\")\n",
    "print(f\"Mean R2: {scores.mean():.4f} (+/- {scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709c9b7",
   "metadata": {},
   "source": [
    "# Elbow GA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098e2a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "best_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 2000,\n",
    "    'num_leaves': 50,\n",
    "    'max_depth': -1,\n",
    "    'random_state': 2601,\n",
    "    'n_jobs': 1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "valid_ga_feats = [f for f in list(selected_ga) if f in X_train.columns]\n",
    "\n",
    "print(\"Äang xáº¿p háº¡ng features...\")\n",
    "ranker = LGBMRegressor(**best_params)\n",
    "ranker.fit(X_train[valid_ga_feats], y_train)\n",
    "\n",
    "imp_df = pd.DataFrame({\n",
    "    'Feature': valid_ga_feats,\n",
    "    'Importance': ranker.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "sorted_feats = imp_df['Feature'].tolist()\n",
    "\n",
    "steps = list(range(len(sorted_feats), 99, -50)) + list(range(90, 0, -10))\n",
    "results = []\n",
    "\n",
    "print(f\"\\nBáº¯t Ä‘áº§u vÃ²ng láº·p cáº¯t giáº£m features ({len(steps)} vÃ²ng)...\")\n",
    "\n",
    "for k in steps:\n",
    "    current_feats = sorted_feats[:k]\n",
    "    \n",
    "    model = LGBMRegressor(**best_params)\n",
    "    model.fit(X_train[current_feats], y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test[current_feats])\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    print(f\"   -> DÃ¹ng {k:3d} features: R2 = {r2:.4f} | RMSE = {rmse:.2f}\")\n",
    "    results.append({'Num_Features': k, 'R2': r2, 'RMSE': rmse})\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values(by='Num_Features')\n",
    "df_leaderboard = df_results.sort_values(by='R2', ascending=False).reset_index(drop=True)\n",
    "csv_filename = 'result/feature_selection_results.csv'\n",
    "df_results.to_csv(csv_filename, index=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_results['Num_Features'], df_results['R2'], marker='o', linewidth=2, color='blue')\n",
    "\n",
    "best_row = df_results.loc[df_results['R2'].idxmax()]\n",
    "plt.scatter(best_row['Num_Features'], best_row['R2'], color='red', s=150, zorder=5)\n",
    "plt.annotate(f\"Äá»‰nh: {best_row['R2']:.4f}\\n({int(best_row['Num_Features'])} feats)\", \n",
    "             (best_row['Num_Features'], best_row['R2']), \n",
    "             xytext=(best_row['Num_Features']+20, best_row['R2']-0.01),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "plt.title('Biá»ƒu Ä‘á»“ Elbow: Hiá»‡u quáº£ khi giáº£m dáº§n sá»‘ lÆ°á»£ng Features', fontsize=14)\n",
    "plt.xlabel('Sá»‘ lÆ°á»£ng Features', fontsize=12)\n",
    "plt.ylabel('Äá»™ chÃ­nh xÃ¡c (R2)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.gca().invert_xaxis()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBáº¢NG Xáº¾P Háº NG (SCORE GIáº¢M Dáº¦N):\")\n",
    "print(df_leaderboard[['R2', 'Num_Features', 'RMSE']].head(10))\n",
    "\n",
    "print(\"\\nBáº¢NG THEO THá»¨ Tá»° FEATURE (ÃT -> NHIá»€U):\")\n",
    "print(df_results[['Num_Features', 'R2', 'RMSE']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13a8c2",
   "metadata": {},
   "source": [
    "# Elbow RFECV Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ae099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from lightgbm import LGBMRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- START RFE WITH LEADERBOARD ---\")\n",
    "start = time.time()\n",
    "\n",
    "model = LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=50,\n",
    "    max_depth=-1,\n",
    "    random_state=2601,\n",
    "    n_jobs=1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=model,\n",
    "    step=20,\n",
    "    cv=3,\n",
    "    scoring='r2', \n",
    "    min_features_to_select=50,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "r2_scores = rfecv.cv_results_['mean_test_score']\n",
    "n_scores = len(r2_scores)\n",
    "\n",
    "feature_counts = [50 + i * 20 for i in range(n_scores)]\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "    'Num_Features': feature_counts,\n",
    "    'Score_R2': r2_scores\n",
    "})\n",
    "\n",
    "df_leaderboard = df_results.sort_values(by='Score_R2', ascending=False).reset_index(drop=True)\n",
    "\n",
    "selected_rfecv = X_train.columns[rfecv.support_]\n",
    "print(f\"\\nTime Run: {time.time() - start:.2f} s\")\n",
    "print(f\"Best Number of Features: {rfecv.n_features_}\")\n",
    "print(f\"Best R2 Score: {df_leaderboard.iloc[0]['Score_R2']:.4f}\")\n",
    "\n",
    "print(\"\\nLEADERBOARD (DESCENDING SCORE):\")\n",
    "print(df_leaderboard.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nPROGRESS (BY FEATURE COUNT):\")\n",
    "print(df_results.head(10).to_string(index=False))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_results['Num_Features'], df_results['Score_R2'], marker='o', color='green', linewidth=2)\n",
    "\n",
    "best_row = df_leaderboard.iloc[0]\n",
    "plt.scatter(best_row['Num_Features'], best_row['Score_R2'], color='red', s=150, zorder=5)\n",
    "plt.annotate(f\"Best: {best_row['Score_R2']:.4f}\\n({int(best_row['Num_Features'])} feats)\", \n",
    "             (best_row['Num_Features'], best_row['Score_R2']), \n",
    "             xytext=(best_row['Num_Features']+20, best_row['Score_R2']-0.005),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "plt.title('RFECV Performance Curve', fontsize=14)\n",
    "plt.xlabel('Number of Features Selected', fontsize=12)\n",
    "plt.ylabel('Cross Validation Score (R2)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSelected Features List:\")\n",
    "print(list(selected_rfecv))\n",
    "\n",
    "df_results.to_csv('result/rfecv_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5728a31e",
   "metadata": {},
   "source": [
    "# Compare before and after using partial correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257cc2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "ULTRA_HIGH_CORR = 0.999 \n",
    "\n",
    "corr_matrix = X_train_clean.corr().abs()\n",
    "\n",
    "ranker_temp = LGBMRegressor(n_estimators=100, verbose=-1, random_state=2601)\n",
    "ranker_temp.fit(X_train_clean, y_train_clean)\n",
    "importances = pd.Series(ranker_temp.feature_importances_, index=X_train_clean.columns)\n",
    "\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = []\n",
    "\n",
    "for column in upper.columns:\n",
    "    correlated_cols = upper.index[upper[column] > ULTRA_HIGH_CORR].tolist()\n",
    "    if correlated_cols:\n",
    "        for other_col in correlated_cols:\n",
    "            if other_col in to_drop: continue\n",
    "            imp_col = importances.get(column, 0)\n",
    "            imp_other = importances.get(other_col, 0)\n",
    "            if imp_col < imp_other:\n",
    "                to_drop.append(column)\n",
    "                break \n",
    "            else:\n",
    "                to_drop.append(other_col)\n",
    "\n",
    "to_drop = list(set(to_drop))\n",
    "\n",
    "print(f\"Phat hien {len(to_drop)} features tuong quan > {ULTRA_HIGH_CORR}\")\n",
    "print(f\"Vi du: {to_drop[:5]}\")\n",
    "\n",
    "voting_model = LGBMRegressor(\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=2000,\n",
    "    num_leaves=50,\n",
    "    max_depth=-1,\n",
    "    random_state=2601,\n",
    "    n_jobs=1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "if len(to_drop) > 0:\n",
    "    X_train_ultra = X_train_clean.drop(columns=to_drop)\n",
    "    X_test_ultra = X_test.drop(columns=to_drop)\n",
    "\n",
    "    print(f\"Train thu tren {X_train_ultra.shape[1]} features...\")\n",
    "    voting_model.fit(X_train_ultra, y_train_clean)\n",
    "    \n",
    "    y_pred = voting_model.predict(X_test_ultra)\n",
    "    new_r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    old_r2 = 0.6313 \n",
    "    \n",
    "    print(f\"Ket qua R2 moi: {new_r2:.4f}\")\n",
    "    \n",
    "    methods = ['Original Clean', 'Filtered (>0.995)']\n",
    "    scores = [old_r2, new_r2]\n",
    "    colors = ['green', 'red']\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    bars = plt.bar(methods, scores, color=colors, width=0.5)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{height:.4f}',\n",
    "                 ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "    plt.title('Hieu qua giua giu nguyen va loc tuong quan', fontsize=14)\n",
    "    plt.ylabel('R2 Score', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.ylim(0, 0.8)\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Khong co features nao qua giong nhau de xoa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b8e1ac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a24c7",
   "metadata": {},
   "source": [
    "# EXTRA MODEL \n",
    "# MAE (Mean Absolute Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0985c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "import time\n",
    "\n",
    "best_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 2000,\n",
    "    'num_leaves': 50,\n",
    "    'max_depth': -1,\n",
    "    'random_state': 2601,\n",
    "    'n_jobs': 1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "fast_params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 100,\n",
    "    'num_leaves': 31,\n",
    "    'random_state': 2601,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "print(\"Ranking features (Fast Mode)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rfe_selector = RFE(estimator=LGBMRegressor(**fast_params), \n",
    "                   n_features_to_select=1, \n",
    "                   step=0.1, \n",
    "                   verbose=0)\n",
    "\n",
    "rfe_selector.fit(X_train, y_train) \n",
    "\n",
    "rfe_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Rank': rfe_selector.ranking_\n",
    "}).sort_values(by='Rank', ascending=True)\n",
    "\n",
    "sorted_feats = rfe_df['Feature'].tolist()\n",
    "print(f\"Ranking done in {time.time() - start_time:.1f}s\")\n",
    "\n",
    "steps = list(range(len(sorted_feats), 99, -50)) + list(range(90, 0, -10))\n",
    "results = []\n",
    "\n",
    "print(f\"\\nStarting MAE Loop ({len(steps)} steps)...\")\n",
    "\n",
    "for k in steps:\n",
    "    current_feats = sorted_feats[:k]\n",
    "    \n",
    "    model = LGBMRegressor(**best_params)\n",
    "    model.fit(X_train[current_feats], y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test[current_feats])\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"   -> Top {k:3d} features: MAE = {mae:.2f}\")\n",
    "    results.append({'Num_Features': k, 'MAE': mae})\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values(by='Num_Features')\n",
    "df_leaderboard = df_results.sort_values(by='MAE', ascending=True).reset_index(drop=True)\n",
    "\n",
    "csv_filename = 'result/rfe_mae_results.csv'\n",
    "df_results.to_csv(csv_filename, index=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_results['Num_Features'], df_results['MAE'], marker='o', linewidth=2, color='purple')\n",
    "\n",
    "best_row = df_results.loc[df_results['MAE'].idxmin()]\n",
    "plt.scatter(best_row['Num_Features'], best_row['MAE'], color='red', s=150, zorder=5)\n",
    "plt.annotate(f\"Best MAE: {best_row['MAE']:.2f}\\n({int(best_row['Num_Features'])} feats)\", \n",
    "             (best_row['Num_Features'], best_row['MAE']), \n",
    "             xytext=(best_row['Num_Features']+20, best_row['MAE']+5),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "plt.title('RFE Feature Selection (MAE Metric)', fontsize=14)\n",
    "plt.xlabel('Number of Features', fontsize=12)\n",
    "plt.ylabel('Mean Absolute Error (MAE)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.gca().invert_xaxis()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTOP 5 CONFIGURATIONS (LOWEST MAE):\")\n",
    "print(df_leaderboard.head(5))\n",
    "\n",
    "print(\"\\nLOOP PROGRESS:\")\n",
    "print(df_results.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
